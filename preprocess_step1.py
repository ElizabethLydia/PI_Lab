#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆPI-Labæ•°æ®é¢„å¤„ç†è„šæœ¬
æ™ºèƒ½å¤„ç†ï¼šBiopacé™é‡‡æ · + HUBæ’å€¼å»é‡ + æ’å€¼å¯¹é½ + åŒæ ¼å¼ä¿å­˜
"""

import os
import numpy as np
import pandas as pd
import pickle
from tqdm import tqdm
from scipy.interpolate import interp1d
import warnings
import time
import multiprocessing
import warnings
from multiprocessing import Manager
warnings.filterwarnings('ignore')

def interpolate_duplicate_timestamps(df, time_col='timestamp'):
    """
    æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³ - ç”¨äºHUBæ•°æ®ä¿æŒç²¾åº¦
    """
    if df.empty or time_col not in df.columns:
        return df
    
    df = df.copy()
    unique_times = df[time_col].unique()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
    if len(unique_times) == len(df):
        return df
    
    print(f"      æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³...")
    
    new_timestamps = []
    for t in unique_times:
        indices = df[df[time_col] == t].index
        n_points = len(indices)

        if n_points > 1:
            current_idx = np.where(unique_times == t)[0][0]
            if current_idx == len(unique_times) - 1:
                delta = 0.001  # æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ä½¿ç”¨é»˜è®¤å°é—´éš”
            else:
                next_t = unique_times[current_idx + 1]
                delta = (next_t - t) / n_points

            for i, idx in enumerate(indices):
                new_timestamps.append((t + i * delta, idx))
        else:
            new_timestamps.append((t, indices[0]))

    # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—æ—¶é—´æˆ³
    new_timestamps.sort(key=lambda x: x[1])
    df[time_col] = [t for t, _ in new_timestamps]
    
    duplicates_fixed = len(df) - len(unique_times)
    if duplicates_fixed > 0:
        print(f"      ä¿®å¤äº† {duplicates_fixed} ä¸ªé‡å¤æ—¶é—´æˆ³")
    
    return df

def quick_downsample_biopac(df, time_col='timestamp', target_freq=100):
    """å¯¹Biopacé«˜é¢‘æ•°æ®é™é‡‡æ ·"""
    if df.empty or time_col not in df.columns:
        return df
    
    original_len = len(df)
    
    if len(df) > 1:
        time_range = df[time_col].max() - df[time_col].min()
        current_freq = len(df) / time_range if time_range > 0 else 1
        
        print(f"      ä¼°ç®—é¢‘ç‡: {current_freq:.1f}Hz -> ç›®æ ‡: {target_freq}Hz")
        
        if current_freq > target_freq * 1.5:  # åªæœ‰æ˜æ˜¾é«˜é¢‘æ‰é™é‡‡æ ·
            step = max(1, int(current_freq / target_freq))
            result = df.iloc[::step].copy()
            print(f"      é™é‡‡æ ·: {original_len:,} -> {len(result):,} è¡Œ (æ­¥é•¿: {step})")
            return result
        else:
            print(f"      é¢‘ç‡é€‚ä¸­ï¼Œè·³è¿‡é™é‡‡æ ·")
    
    return df

def interpolate_with_reftime(time, data, reftime):
    """
    ä½¿ç”¨æ’å€¼å¯¹é½åˆ°å‚è€ƒæ—¶é—´æˆ³
    """
    if len(time) < 2 or len(data) < 2:
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])
    
    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    time = np.asarray(time, dtype=float)
    reftime = np.asarray(reftime, dtype=float)
    
    # é™åˆ¶æ’å€¼èŒƒå›´åˆ°æ•°æ®å®é™…èŒƒå›´å†…
    min_time, max_time = time.min(), time.max()
    valid_reftime_mask = (reftime >= min_time) & (reftime <= max_time)
    valid_reftime = reftime[valid_reftime_mask]
    
    if len(valid_reftime) == 0:
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])
    
    try:
        # ä½¿ç”¨çº¿æ€§æ’å€¼
        interp_func = interp1d(time, data, axis=0, kind='linear', bounds_error=False, fill_value=np.nan)
        interpolated_data = interp_func(valid_reftime)
        
        # åˆ›å»ºå®Œæ•´ç»“æœDataFrame
        if interpolated_data.ndim == 1:
            interpolated_data = interpolated_data.reshape(-1, 1)
        
        # åˆ›å»ºä¸å‚è€ƒæ—¶é—´é•¿åº¦ç›¸åŒçš„ç»“æœ
        full_result = np.full((len(reftime), interpolated_data.shape[1]), np.nan)
        full_result[valid_reftime_mask] = interpolated_data
        
        result_df = pd.DataFrame(full_result, columns=data.columns if hasattr(data, 'columns') else [f'col_{i}' for i in range(full_result.shape[1])])
        result_df['timestamp'] = reftime
        
        return result_df
    except Exception as e:
        print(f"        æ’å€¼é”™è¯¯: {e}")
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])

def process_biopac_file(file_path, target_freq=100):
    """å¤„ç†Biopacæ–‡ä»¶ - å…ˆé™é‡‡æ ·å†æ’å€¼å¤„ç†é‡å¤"""
    try:
        file_name = os.path.basename(file_path)
        try:
            data = pd.read_csv(file_path)
        except UnicodeDecodeError:
            data = pd.read_csv(file_path, encoding='latin1')
        original_rows = len(data)
        
        if data.empty:
            return pd.DataFrame()
        
        print(f"    Biopac {file_name} ({original_rows:,} è¡Œ)")
        
        # 1. å…ˆé™é‡‡æ ·åˆ°ç›®æ ‡é¢‘ç‡ï¼ˆä¸å»é‡ï¼‰
        data = quick_downsample_biopac(data, target_freq=target_freq)
        
        # 2. å†ç”¨æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³
        data = interpolate_duplicate_timestamps(data)
        
        # 3. æ’åº
        if data is not None and not data.empty and 'timestamp' in data.columns:
            data = data.sort_values('timestamp').reset_index(drop=True)
        
        final_rows = len(data) if data is not None else 0
        compression_ratio = original_rows / final_rows if final_rows > 0 else 1
        print(f"      æœ€ç»ˆ: {final_rows:,} è¡Œ (å‹ç¼©æ¯”: {compression_ratio:.1f}:1)")
        
        return data if data is not None else pd.DataFrame()
        
    except Exception as e:
        print(f"      é”™è¯¯: {e}")
        return pd.DataFrame()

def process_hub_file(file_path):
    """å¤„ç†HUBæ–‡ä»¶ - æ’å€¼ç­–ç•¥"""
    try:
        file_name = os.path.basename(file_path)
        try:
            data = pd.read_csv(file_path)
        except UnicodeDecodeError:
            data = pd.read_csv(file_path, encoding='latin1')
        original_rows = len(data)
        
        if data.empty:
            return pd.DataFrame()
        
        print(f"    HUB {file_name} ({original_rows:,} è¡Œ)")
        
        # 1. æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³ï¼ˆä¿æŒHUBæ•°æ®ç²¾åº¦ï¼‰
        processed_data = interpolate_duplicate_timestamps(data)
        
        # 2. æ’åº
        if processed_data is not None and not processed_data.empty and 'timestamp' in processed_data.columns:
            processed_data = processed_data.sort_values('timestamp').reset_index(drop=True)
        
        final_rows = len(processed_data) if processed_data is not None else 0
        print(f"      æœ€ç»ˆ: {final_rows:,} è¡Œ")
        
        return processed_data if processed_data is not None else pd.DataFrame()
        
    except Exception as e:
        print(f"      é”™è¯¯: {e}")
        return pd.DataFrame()

def load_experiment_smart(experiment_path, target_freq=100):
    """æ™ºèƒ½åŠ è½½å•ä¸ªå®éªŒ"""
    experiment_name = os.path.basename(experiment_path)
    print(f"\n{'='*50}")
    print(f"å¤„ç†å®éªŒ {experiment_name}")
    print(f"{'='*50}")
    
    result = {'biopac': {}, 'hub': {}}
    
    # å¤„ç†Biopacæ•°æ® (é«˜é¢‘ï¼Œå…ˆé™é‡‡æ ·å†æ’å€¼å¤„ç†é‡å¤)
    biopac_path = os.path.join(experiment_path, 'Biopac')
    if os.path.isdir(biopac_path):
        biopac_files = [f for f in os.listdir(biopac_path) if f.endswith('.csv')]
        print(f"\nBiopacæ–‡ä»¶ ({len(biopac_files)} ä¸ª) - é™é‡‡æ ·+æ’å€¼ç­–ç•¥:")
        
        for file in biopac_files:
            file_path = os.path.join(biopac_path, file)
            data = process_biopac_file(file_path, target_freq)
            if data is not None and not data.empty:
                key = file.split('-')[0] if '-' in file else file.replace('.csv', '')
                result['biopac'][key] = data
    
    # å¤„ç†HUBæ•°æ® (ä½é¢‘ï¼Œç”¨æ’å€¼)
    hub_path = os.path.join(experiment_path, 'HUB')
    if os.path.isdir(hub_path):
        hub_files = [f for f in os.listdir(hub_path) if f.endswith('.csv')]
        print(f"\nHUBæ–‡ä»¶ ({len(hub_files)} ä¸ª) - ä½¿ç”¨æ’å€¼ç­–ç•¥:")
        
        for file in hub_files:
            file_path = os.path.join(hub_path, file)
            data = process_hub_file(file_path)
            if not data.empty:
                key = file.replace('.csv', '')
                result['hub'][key] = data
    
    # ç»Ÿè®¡ç»“æœ
    biopac_count = len(result['biopac'])
    hub_count = len(result['hub'])
    print(f"\nå®éªŒ {experiment_name} å®Œæˆ: {biopac_count} ä¸ªBiopacæ–‡ä»¶, {hub_count} ä¸ªHUBæ–‡ä»¶")
    
    return result

def align_data_with_interpolation(data_dict, output_dir, csv_dir, subject):
    """ä½¿ç”¨æ’å€¼è¿›è¡Œç²¾ç¡®æ•°æ®å¯¹é½"""
    aligned_data = {}
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\n{'='*50}")
    print("æ’å€¼å¯¹é½é˜¶æ®µ")
    print(f"{'='*50}")
    
    for experiment_name, experiment_data in data_dict.items():
        try:
            print(f"\nå¯¹é½å®éªŒ {experiment_name}...")
            
            # æŸ¥æ‰¾å‚è€ƒæ—¶é—´åºåˆ—ï¼ˆä¼˜å…ˆä½¿ç”¨sensor2ï¼‰
            ref_data = None
            ref_name = ""
            
            try:
                hub_sensor2 = experiment_data['hub'].get('sensor2', pd.DataFrame())
                if not hub_sensor2.empty and 'timestamp' in hub_sensor2.columns:
                    ref_data = hub_sensor2
                    ref_name = "sensor2"
            except KeyError:
                print(f"  è­¦å‘Š: sensor2 ç¼ºå°‘ timestamp åˆ—")
            
            if ref_data is None:
                min_len = float('inf')
                for data_type in ['hub', 'biopac']:
                    for key, data in experiment_data[data_type].items():
                        if isinstance(data, pd.DataFrame) and not data.empty and 'timestamp' in data.columns:
                            if len(data) < min_len:
                                min_len = len(data)
                                ref_data = data
                                ref_name = f"{data_type}_{key}"
            
            if ref_data is None or ref_data.empty or 'timestamp' not in ref_data.columns:
                print(f"  è­¦å‘Š: experiment {experiment_name} æ— æœ‰æ•ˆå‚è€ƒæ•°æ®æˆ–ç¼ºå°‘ timestamp åˆ—ï¼Œè·³è¿‡")
                continue
            
            try:
                ref_timestamps = ref_data['timestamp'].values
            except KeyError:
                print(f"  é”™è¯¯: å‚è€ƒæ•°æ®ç¼ºå°‘ timestamp åˆ—ï¼Œè·³è¿‡ experiment {experiment_name}")
                continue
            
            print(f"  ä½¿ç”¨ {ref_name} ä½œä¸ºå‚è€ƒ ({len(ref_data):,} è¡Œ)")
            aligned_experiment = {'biopac': {}, 'hub': {}}
            
            # æ’å€¼å¯¹é½æ‰€æœ‰æ•°æ®
            for data_type in ['biopac', 'hub']:
                for key, data in experiment_data[data_type].items():
                    if isinstance(data, pd.DataFrame) and not data.empty and 'timestamp' in data.columns:
                        print(f"    å¯¹é½ {data_type}_{key}...")
                        
                        # æå–éœ€è¦æ’å€¼çš„åˆ—ï¼ˆé™¤äº†timestampï¼‰
                        data_columns = [col for col in data.columns if col != 'timestamp']
                        if data_columns:
                            # ä½¿ç”¨æ’å€¼å¯¹é½
                            interpolated_data = interpolate_with_reftime(
                                data['timestamp'].values,
                                data[data_columns].values,
                                ref_timestamps
                            )
                            
                            if not interpolated_data.empty:
                                # é‡æ–°è®¾ç½®åˆ—å
                                interpolated_data.columns = data_columns + ['timestamp']
                                aligned_experiment[data_type][key] = interpolated_data
                                print(f"      å¯¹é½å®Œæˆ: {len(data):,} -> {len(interpolated_data):,} è¡Œ")
                            else:
                                print(f"      å¯¹é½å¤±è´¥ï¼Œè·³è¿‡")
            
            aligned_data[experiment_name] = aligned_experiment
            
            # ä¿å­˜pklæ ¼å¼
            pkl_path = os.path.join(output_dir, f'experiment_{experiment_name}_aligned.pkl')
            with open(pkl_path, 'wb') as f:
                pickle.dump({experiment_name: aligned_experiment}, f)
            print(f"  ä¿å­˜PKL: {pkl_path}")
            
            # ä¿å­˜npyæ ¼å¼
            npy_path = os.path.join(output_dir, f'experiment_{experiment_name}_aligned.npy')
            npy_data = {
                experiment_name: {
                    data_type: {
                        key: df.to_dict() if isinstance(df, pd.DataFrame) else df
                        for key, df in type_data.items()
                    }
                    for data_type, type_data in aligned_experiment.items()
                }
            }
            np.save(npy_path, npy_data, allow_pickle=True)
            file_size = os.path.getsize(npy_path) / (1024 * 1024)  # MB
            print(f"  ä¿å­˜NPY (å•æ–‡ä»¶): {npy_path}, å¤§å°: {file_size:.2f} MB")
        except Exception as e:
            print(f'å¯¹é½ experiment {experiment_name} å¤±è´¥: {e}')
            continue
    
    # CSV ç”Ÿæˆå¾ªç¯ - ä¸ºæ¯ä¸ª experiment ç”Ÿæˆ
    for exp_name, exp_data in aligned_data.items():
        # æ•´åˆBiopacæ•°æ®ä¸ºå•æ–‡ä»¶CSV
        biopac_data = exp_data['biopac']
        if biopac_data:
            ref_timestamps = exp_data['hub'].get('sensor2', pd.DataFrame())['timestamp'].values
            if len(ref_timestamps) == 0:
                ref_timestamps = biopac_data[next(iter(biopac_data))]['timestamp'].values
            
            merged_biopac = pd.DataFrame({'timestamp': ref_timestamps})
            
            for key, df in biopac_data.items():
                if isinstance(df, pd.DataFrame) and not df.empty:
                    merged_biopac = merged_biopac.merge(df[['timestamp', key]], on='timestamp', how='left')
            
            merged_biopac = merged_biopac.fillna(method='ffill').fillna(method='bfill')
            biopac_csv_path = os.path.join(csv_dir, f'{subject}_{exp_name}_biopac_aligned.csv')
            merged_biopac.to_csv(biopac_csv_path, index=False)
            print(f'  ä¿å­˜æ•´åˆBiopac CSV: {biopac_csv_path}')
        
        # ä¿å­˜HUBæ•°æ®ä¸ºç‹¬ç«‹CSVæ–‡ä»¶
        for key, df in exp_data['hub'].items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                columns = ['timestamp'] + [col for col in df.columns if col != 'timestamp']
                df_reordered = df[columns]
                hub_csv_path = os.path.join(csv_dir, f'{subject}_{exp_name}_hub_{key}_aligned.csv')
                df_reordered.to_csv(hub_csv_path, index=False)
                print(f'  ä¿å­˜HUB CSV: {hub_csv_path}')
    
    return aligned_data

def process_subject(date_folder, subject):
    dataset_root = '/root/shared/PhysioNet2025/'  # æ·»åŠ å¸¸é‡å®šä¹‰
    MAX_EXPERIMENTS = None  # æ·»åŠ å¸¸é‡å®šä¹‰
    TARGET_FREQ = 100  # æ·»åŠ å¸¸é‡å®šä¹‰
    date_path = os.path.join(dataset_root, date_folder)
    subject_path = os.path.join(date_path, subject)
    output_dir = os.path.join('/root/autodl-tmp/', subject, 'output')
    csv_dir = os.path.join('/root/autodl-tmp/', subject, 'csv_output')
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(csv_dir, exist_ok=True)
    
    if os.listdir(output_dir):
        print(f"\nsubject {subject} å·²å¤„ç†ï¼ˆoutputç›®å½•éç©ºï¼‰ï¼Œè·³è¿‡")
        return None  # æˆåŠŸè¿”å› None
    
    print(f"\nå¤„ç†subject: {subject} åœ¨ {date_folder}")
    
    all_folders = os.listdir(subject_path)
    experiment_folders = [f for f in all_folders if f.isdigit() and os.path.isdir(os.path.join(subject_path, f))]
    experiment_folders.sort(key=lambda x: int(x))
    
    if MAX_EXPERIMENTS:
        experiment_folders = experiment_folders[:MAX_EXPERIMENTS]
    
    print(f"å‘ç°å®éªŒ: {experiment_folders}")
    
    all_data = {}
    load_start = time.time()
    for experiment in experiment_folders:
        experiment_path = os.path.join(subject_path, experiment)
        experiment_data = load_experiment_smart(experiment_path, TARGET_FREQ)
        all_data[experiment] = experiment_data
    load_time = time.time() - load_start
    
    align_start = time.time()
    aligned_data = align_data_with_interpolation(all_data, output_dir, csv_dir, subject)
    align_time = time.time() - align_start
    
    # ç»Ÿè®¡
    total_time = time.time() - load_start  # ä½¿ç”¨ load_start ä½œä¸ºèµ·ç‚¹
    print(f"\n{'='*60}")
    print("ï¿½ï¿½ å¤„ç†å®Œæˆç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æˆåŠŸå¤„ç†å®éªŒ: {len(aligned_data)}")
    print(f"æ•°æ®åŠ è½½è€—æ—¶: {load_time:.1f} ç§’")
    print(f"æ•°æ®å¯¹é½è€—æ—¶: {align_time:.1f} ç§’")
    print(f"æ€»å¤„ç†è€—æ—¶: {total_time:.1f} ç§’")
    if len(experiment_folders) > 0:
        print(f"å¹³å‡æ¯ä¸ªå®éªŒ: {total_time/len(experiment_folders):.1f} ç§’")
    
    for exp_name, exp_data in aligned_data.items():
        total_biopac = len(exp_data['biopac'])
        total_hub = len(exp_data['hub'])
        print(f"å®éªŒ {exp_name}: {total_biopac} ä¸ªBiopac + {total_hub} ä¸ªHUBæ–‡ä»¶")
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"PKLæ ¼å¼: {output_dir}/experiment_*_aligned.pkl")
    print(f"NPYæ ¼å¼: {output_dir}/experiment_*_aligned.npy")
    return None  # æˆåŠŸè¿”å› None

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    manager = Manager()
    failed_subjects = manager.list()
    
    dataset_root = '/root/shared/PhysioNet2025/'
    
    # è®¾ç½®å‚æ•°
    TARGET_FREQ = 100  # ç›®æ ‡é¢‘ç‡
    MAX_EXPERIMENTS = None  # Noneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å®éªŒ
    
    # è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹
    date_folders = [f for f in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, f)) and f.startswith('20')]
    date_folders.sort()
    
    for date_folder in date_folders:
        date_path = os.path.join(dataset_root, date_folder)
        subject_folders = [f for f in os.listdir(date_path) if os.path.isdir(os.path.join(date_path, f)) and f.startswith('00')]
        subject_folders.sort()
        
        with multiprocessing.Pool() as p:
            results = p.starmap(process_subject, [(date_folder, subject) for subject in subject_folders])
    
    failed_subjects = [res for res in results if res is not None]
    
    if failed_subjects:
        with open('/root/autodl-tmp/failed_subjects.txt', 'w') as f:
            f.write("å¤±è´¥çš„ subject:\n")
            for fs in failed_subjects:
                f.write(f"{fs}\n")
        print(f"å¤±è´¥ subject æ•°é‡: {len(failed_subjects)}, è¯¦æƒ…è§ /root/autodl-tmp/failed_subjects.txt")
    else:
        print("æ‰€æœ‰ subject å¤„ç†æˆåŠŸï¼")
    
    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    print(f"\n{'='*60}")
    print("ğŸ“Š æ€»å¤„ç†å®Œæˆç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æ€»å¤„ç†è€—æ—¶: {total_time:.1f} ç§’")

    # with open('failed_subjects.txt', 'w') as f: # This line is removed as per the new_code, as failed_subjects is now a Manager list
    #     for date, subj in failed_subjects:
    #         f.write(f'{date}/{subj}\n')
    # if failed_subjects: # This line is removed as per the new_code, as failed_subjects is now a Manager list
    #     print(f"å¤±è´¥çš„ subjects å·²å†™å…¥ failed_subjects.txt") # This line is removed as per the new_code, as failed_subjects is now a Manager list
    # else: # This line is removed as per the new_code, as failed_subjects is now a Manager list
    #     print("æ‰€æœ‰ subjects å¤„ç†æˆåŠŸ") # This line is removed as per the new_code, as failed_subjects is now a Manager list

if __name__ == "__main__":
    main()