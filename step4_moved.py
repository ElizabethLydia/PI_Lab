#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 4: Integrated PTT-BP Analysis (Moved Version)
Uses results from step3_moved for further analysis: bar charts, integrated views, etc.
åŸºäºæä¾›çš„ç›®å½•ç»“æ„ï¼ˆæ¯ä¸ªå—è¯•è€…å¦‚ /root/autodl-tmp/00003 æœ‰ ptt_bp_analysis_moved/ ç­‰å­ç›®å½•ï¼‰ï¼Œ
è„šæœ¬ä»æ¯ä¸ªå—è¯•è€…çš„ ptt_bp_analysis_moved/ åŠ è½½ step3 æ–‡ä»¶ï¼Œè¿›è¡Œè·¨å—è¯•è€…åˆ†æã€‚
è¾“å‡ºä¿å­˜åˆ° /root/autodl-tmp/integrated_analysis_moved_filtered ï¼ˆå…¨å±€ç›®å½•ï¼‰ã€‚
ä½¿ç”¨movedæ–‡ä»¶å¤¹ä¸­çš„æ ¡å‡†åè¡€å‹æ•°æ®ä½œä¸ºçœŸå®å€¼ï¼Œå¹¶è¿‡æ»¤æ‰å¼‚å¸¸çš„è¡€å‹æ•°æ®ã€‚
"""

import os
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.ioff()

class IntegratedPTTBloodPressureAnalyzerMoved:
    def __init__(self, root_path="/root/autodl-tmp/", output_dir="integrated_analysis_calibrated_filtered"):
        self.root_path = root_path
        self.output_dir = os.path.join(root_path, output_dir)
        self.step3_dir = "ptt_bp_analysis_calibrated"  # step3_calibrated è¾“å‡ºç›®å½•ï¼ˆæ¯ä¸ªå—è¯•è€…ä¸‹ï¼‰
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¡€å‹è¿‡æ»¤é˜ˆå€¼
        self.bp_filter_thresholds = {
            'systolic_bp': {'min': 80, 'max': 200},    # æ”¶ç¼©å‹æ­£å¸¸èŒƒå›´ï¼š80-200 mmHg
            'diastolic_bp': {'min': 50, 'max': 120},   # èˆ’å¼ å‹æ­£å¸¸èŒƒå›´ï¼š50-120 mmHg
            'mean_bp': {'min': 60, 'max': 150}         # å¹³å‡è¡€å‹æ­£å¸¸èŒƒå›´ï¼š60-150 mmHg
        }
        
        self.physiological_indicators = {
            'systolic_bp': 'Systolic BP (mmHg)',
            'diastolic_bp': 'Diastolic BP (mmHg)', 
            'mean_bp': 'Mean Arterial Pressure (mmHg)',
        }
        
        self.ptt_combinations_en = {
            'sensor2-sensor3': 'Noseâ†’Finger',
            'sensor2-sensor4': 'Noseâ†’Wrist', 
            'sensor2-sensor5': 'Noseâ†’Ear',
            'sensor3-sensor4': 'Fingerâ†’Wrist',
            'sensor3-sensor5': 'Fingerâ†’Ear',
            'sensor4-sensor5': 'Wristâ†’Ear'
        }
        
        print("ğŸ”¬ Integrated PTT-Cardiovascular Parameters Correlation Analyzer (Calibrated Version + Filtered)")
        print(f"ğŸ“ Results will be saved to: {self.output_dir}")
        print(f"ğŸ“‚ Loading from each subject's {self.step3_dir}/")
        print(f"ğŸ¯ Using calibrated BP data from PhysioNet2025_Calibrated as ground truth")
        print(f"ğŸ” BP filtering thresholds:")
        for bp_type, thresholds in self.bp_filter_thresholds.items():
            print(f"   {bp_type}: {thresholds['min']}-{thresholds['max']} mmHg")
    
    def filter_abnormal_bp_data(self, sync_df, subject):
        """è¿‡æ»¤å¼‚å¸¸çš„è¡€å‹æ•°æ®"""
        if sync_df.empty:
            return sync_df, {'subject': subject, 'filtered': False, 'reason': 'Empty data'}
        
        original_count = len(sync_df)
        filtered_df = sync_df.copy()
        
        # æ£€æŸ¥è¡€å‹åˆ—æ˜¯å¦å­˜åœ¨
        bp_columns = []
        for bp_type in self.bp_filter_thresholds.keys():
            col = f'{bp_type}_mean'
            if col in filtered_df.columns:
                bp_columns.append(col)
        
        if not bp_columns:
            return filtered_df, {'subject': subject, 'filtered': False, 'reason': 'No BP columns found'}
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        mask = pd.Series([True] * len(filtered_df), index=filtered_df.index)
        
        for bp_col in bp_columns:
            bp_type = bp_col.replace('_mean', '')
            if bp_type in self.bp_filter_thresholds:
                thresholds = self.bp_filter_thresholds[bp_type]
                col_mask = (filtered_df[bp_col] >= thresholds['min']) & (filtered_df[bp_col] <= thresholds['max'])
                mask = mask & col_mask
        
        # ç»Ÿè®¡è¿‡æ»¤ç»“æœ
        filtered_count = mask.sum()
        removed_count = original_count - filtered_count
        
        # åº”ç”¨è¿‡æ»¤
        filtered_df = filtered_df[mask].reset_index(drop=True)
        
        # è®¡ç®—è¿‡æ»¤ç»Ÿè®¡
        filter_stats = {
            'subject': subject,
            'filtered': True,
            'original_count': original_count,
            'filtered_count': filtered_count,
            'removed_count': removed_count,
            'removal_rate': (removed_count / original_count * 100) if original_count > 0 else 0,
            'bp_columns_checked': bp_columns
        }
        
        # å¦‚æœè¿‡æ»¤åæ•°æ®å¤ªå°‘ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ
        if filtered_count < 10:
            filter_stats['filtered'] = False
            filter_stats['reason'] = f'Insufficient data after filtering ({filtered_count} < 10)'
        
        return filtered_df, filter_stats
    
    def check_bp_data_quality(self, sync_df, subject):
        """æ£€æŸ¥è¡€å‹æ•°æ®è´¨é‡ï¼Œè¿”å›å¼‚å¸¸å€¼ç»Ÿè®¡"""
        if sync_df.empty:
            return {'subject': subject, 'quality_check': False, 'reason': 'Empty data'}
        
        quality_stats = {'subject': subject, 'quality_check': True}
        
        for bp_type, thresholds in self.bp_filter_thresholds.items():
            col = f'{bp_type}_mean'
            if col in sync_df.columns:
                values = sync_df[col].dropna()
                if len(values) > 0:
                    below_min = (values < thresholds['min']).sum()
                    above_max = (values > thresholds['max']).sum()
                    total = len(values)
                    
                    quality_stats[f'{bp_type}_below_min'] = below_min
                    quality_stats[f'{bp_type}_above_max'] = above_max
                    quality_stats[f'{bp_type}_total'] = total
                    quality_stats[f'{bp_type}_below_min_pct'] = (below_min / total * 100) if total > 0 else 0
                    quality_stats[f'{bp_type}_above_max_pct'] = (above_max / total * 100) if total > 0 else 0
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡å¼‚å¸¸å€¼
                    if below_min > 0 or above_max > 0:
                        quality_stats[f'{bp_type}_has_abnormal'] = True
                        quality_stats[f'{bp_type}_min_val'] = values.min()
                        quality_stats[f'{bp_type}_max_val'] = values.max()
                    else:
                        quality_stats[f'{bp_type}_has_abnormal'] = False
                else:
                    quality_stats[f'{bp_type}_has_abnormal'] = False
                    quality_stats[f'{bp_type}_total'] = 0
            else:
                quality_stats[f'{bp_type}_has_abnormal'] = False
                quality_stats[f'{bp_type}_total'] = 0
        
        return quality_stats
    
    def load_subjects(self):
        """åŠ è½½æ‰€æœ‰å—è¯•è€…ï¼ˆç›®å½•ä»¥ '00' å¼€å¤´ï¼‰"""
        return sorted([d for d in os.listdir(self.root_path) 
                       if os.path.isdir(os.path.join(self.root_path, d)) and d.startswith('00')])
    
    def check_subject_status(self, subject):
        """æ£€æŸ¥å—è¯•è€…çš„å¤„ç†çŠ¶æ€"""
        status_file = os.path.join(self.root_path, subject, self.step3_dir, 'step3_calibrated_succ.txt')
        if not os.path.exists(status_file):
            return {'subject': subject, 'status': 'NO_FILE', 'available': False}
        
        try:
            with open(status_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if 'Status: SUCCESS' in content:
                return {'subject': subject, 'status': 'SUCCESS', 'available': True}
            elif 'Status: FAILED' in content:
                return {'subject': subject, 'status': 'FAILED', 'available': False}
            elif 'Status: ERROR' in content:
                return {'subject': subject, 'status': 'ERROR', 'available': False}
            else:
                return {'subject': subject, 'status': 'UNKNOWN', 'available': False}
        except Exception as e:
            return {'subject': subject, 'status': 'READ_ERROR', 'available': False, 'error': str(e)}
    
    def get_available_subjects(self):
        """è·å–å¯ç”¨çš„å—è¯•è€…åˆ—è¡¨ï¼ˆåªåŒ…å«æˆåŠŸå¤„ç†çš„ä¸”è¡€å‹æ•°æ®æ­£å¸¸çš„ï¼‰"""
        all_subjects = self.load_subjects()
        available_subjects = []
        filtered_subjects = []
        status_summary = []
        bp_quality_summary = []
        filter_summary = []
        
        print("ğŸ” æ£€æŸ¥å—è¯•è€…å¤„ç†çŠ¶æ€...")
        for subject in all_subjects:
            status = self.check_subject_status(subject)
            status_summary.append(status)
            
            if status['available']:
                print(f"âœ… {subject}: å¤„ç†æˆåŠŸï¼Œæ£€æŸ¥è¡€å‹æ•°æ®è´¨é‡...")
                
                # åŠ è½½åŒæ­¥æ•°æ®æ£€æŸ¥è¡€å‹è´¨é‡
                sync_df = self.load_step3_sync_data(subject)
                if sync_df is not None:
                    # æ£€æŸ¥è¡€å‹æ•°æ®è´¨é‡
                    quality_stats = self.check_bp_data_quality(sync_df, subject)
                    bp_quality_summary.append(quality_stats)
                    
                    # è¿‡æ»¤å¼‚å¸¸è¡€å‹æ•°æ®
                    filtered_df, filter_stats = self.filter_abnormal_bp_data(sync_df, subject)
                    filter_summary.append(filter_stats)
                    
                    if filter_stats['filtered']:
                        available_subjects.append(subject)
                        print(f"   âœ… è¡€å‹æ•°æ®æ­£å¸¸ï¼Œå¯ç”¨ ({filter_stats['filtered_count']}/{filter_stats['original_count']} æ•°æ®ç‚¹)")
                    else:
                        filtered_subjects.append(subject)
                        print(f"   âŒ è¡€å‹æ•°æ®å¼‚å¸¸ï¼Œå·²è¿‡æ»¤: {filter_stats['reason']}")
                else:
                    filtered_subjects.append(subject)
                    print(f"   âŒ æ— æ³•åŠ è½½åŒæ­¥æ•°æ®")
            else:
                print(f"âŒ {subject}: {status['status']}")
        
        # ç»Ÿè®¡ç»“æœ
        total_count = len(all_subjects)
        success_count = len([s for s in status_summary if s['status'] == 'SUCCESS'])
        failed_count = len([s for s in status_summary if s['status'] == 'FAILED'])
        error_count = len([s for s in status_summary if s['status'] == 'ERROR'])
        no_file_count = len([s for s in status_summary if s['status'] == 'NO_FILE'])
        bp_filtered_count = len(filtered_subjects)
        
        print(f"\nğŸ“Š å—è¯•è€…çŠ¶æ€ç»Ÿè®¡:")
        print(f"   ğŸ“‹ æ€»å—è¯•è€…æ•°: {total_count}")
        print(f"   âœ… æˆåŠŸå¤„ç†: {success_count} ({success_count/total_count*100:.1f}%)")
        print(f"   âŒ å¤„ç†å¤±è´¥: {failed_count} ({failed_count/total_count*100:.1f}%)")
        print(f"   âš ï¸  å¤„ç†å‡ºé”™: {error_count} ({error_count/total_count*100:.1f}%)")
        print(f"   ğŸ“ æ— çŠ¶æ€æ–‡ä»¶: {no_file_count} ({no_file_count/total_count*100:.1f}%)")
        print(f"   ğŸ” è¡€å‹æ•°æ®å¼‚å¸¸è¢«è¿‡æ»¤: {bp_filtered_count}")
        print(f"   ğŸ¯ æœ€ç»ˆå¯ç”¨å—è¯•è€…: {len(available_subjects)}")
        
        # ä¿å­˜çŠ¶æ€ç»Ÿè®¡
        status_df = pd.DataFrame(status_summary)
        status_file = os.path.join(self.output_dir, 'subject_processing_status.csv')
        status_df.to_csv(status_file, index=False)
        print(f"ğŸ’¾ ä¿å­˜å—è¯•è€…å¤„ç†çŠ¶æ€: {status_file}")
        
        # ä¿å­˜è¡€å‹è´¨é‡ç»Ÿè®¡
        if bp_quality_summary:
            quality_df = pd.DataFrame(bp_quality_summary)
            quality_file = os.path.join(self.output_dir, 'bp_data_quality_summary.csv')
            quality_df.to_csv(quality_file, index=False)
            print(f"ğŸ’¾ ä¿å­˜è¡€å‹æ•°æ®è´¨é‡ç»Ÿè®¡: {quality_file}")
        
        # ä¿å­˜è¿‡æ»¤ç»Ÿè®¡
        if filter_summary:
            filter_df = pd.DataFrame(filter_summary)
            filter_file = os.path.join(self.output_dir, 'bp_filter_summary.csv')
            filter_df.to_csv(filter_file, index=False)
            print(f"ğŸ’¾ ä¿å­˜è¡€å‹è¿‡æ»¤ç»Ÿè®¡: {filter_file}")
        
        # è¾“å‡ºè¢«è¿‡æ»¤çš„å—è¯•è€…ä¿¡æ¯
        if filtered_subjects:
            print(f"\nğŸ” è¢«è¡€å‹è¿‡æ»¤ç­›æ‰çš„å—è¯•è€… ({len(filtered_subjects)}ä¸ª):")
            for subject in filtered_subjects:
                filter_info = next((f for f in filter_summary if f['subject'] == subject), None)
                if filter_info:
                    print(f"   âŒ {subject}: {filter_info.get('reason', 'Unknown reason')}")
                    if 'removal_rate' in filter_info:
                        print(f"       æ•°æ®ç§»é™¤ç‡: {filter_info['removal_rate']:.1f}%")
        
        return available_subjects, status_summary, filtered_subjects, filter_summary
    
    def load_step3_correlations(self, subject, exp_id=None):
        """ä»å—è¯•è€…çš„ ptt_bp_analysis_moved/ åŠ è½½ correlations CSV"""
        print(f"ğŸ“‚ åŠ è½½ {subject} çš„ correlations CSV")
        subject_dir = os.path.join(self.root_path, subject, self.step3_dir)
        if exp_id is not None:
            # æ–‡ä»¶åœ¨ exp_X å­æ–‡ä»¶å¤¹ä¸­
            exp_subdir = os.path.join(subject_dir, f'exp_{exp_id}')
            corr_file = os.path.join(exp_subdir, f'ptt_cardiovascular_correlations_exp_{exp_id}.csv')
        else:
            corr_file = os.path.join(subject_dir, 'ptt_cardiovascular_correlations.csv')
        if not os.path.exists(corr_file):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {corr_file}")
            return None
        try:
            df = pd.read_csv(corr_file)
            if df.empty:
                print(f"âš ï¸ ç©ºæ–‡ä»¶: {corr_file}")
                return None
            df['subject'] = subject
            return df
        except pd.errors.EmptyDataError:
            print(f"âš ï¸ ç©ºæ•°æ®é”™è¯¯: {corr_file}")
            return None
        except Exception as e:
            print(f"âŒ åŠ è½½é”™è¯¯ {subject} exp_{exp_id}: {e}")
            return None
    
    def load_step3_sync_data(self, subject, exp_id=None, apply_filter=True):
        """ä»å—è¯•è€…çš„ ptt_bp_analysis_moved/ åŠ è½½ sync æ•°æ® CSV (always load overall file)"""
        print(f"ğŸ“‚ åŠ è½½ {subject} çš„ sync æ•°æ®")
        subject_dir = os.path.join(self.root_path, subject, self.step3_dir)
        sync_file = os.path.join(subject_dir, 'synchronized_ptt_cardiovascular_data.csv')
        if not os.path.exists(sync_file):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {sync_file}")
            return None
        try:
            df = pd.read_csv(sync_file)
            if df.empty:
                print(f"âš ï¸ ç©ºæ–‡ä»¶: {sync_file}")
                return None
            df['subject'] = subject
            
            # å¦‚æœå¯ç”¨è¿‡æ»¤ï¼Œåº”ç”¨è¡€å‹æ•°æ®è¿‡æ»¤
            if apply_filter:
                filtered_df, filter_stats = self.filter_abnormal_bp_data(df, subject)
                if filter_stats['filtered']:
                    print(f"   ğŸ” è¡€å‹è¿‡æ»¤: {filter_stats['filtered_count']}/{filter_stats['original_count']} æ•°æ®ç‚¹ä¿ç•™")
                    return filtered_df
                else:
                    print(f"   âš ï¸ è¡€å‹è¿‡æ»¤åæ•°æ®ä¸è¶³: {filter_stats['reason']}")
                    return None
            else:
                return df
                
        except pd.errors.EmptyDataError:
            print(f"âš ï¸ ç©ºæ•°æ®é”™è¯¯: {sync_file}")
            return None
        except Exception as e:
            print(f"âŒ åŠ è½½é”™è¯¯ {subject}: {e}")
            return None
    
    def create_correlation_bar_chart(self, corr_df, title_suffix, subjects, subdir, exp_id=None):
        """åˆ›å»ºç›¸å…³æ€§æŸ±çŠ¶å›¾ï¼ˆä¸¤ç§ç‰ˆæœ¬ï¼šmulti-pair ä¸€å¼ å›¾ + per pair å•ç‹¬å›¾ï¼‰"""
        valid_sensors = list(self.ptt_combinations_en.values())
        for physio, physio_label in self.physiological_indicators.items():
            physio_col = f'{physio}_mean'
            data = []
            for subject in subjects:
                subj_df = corr_df[corr_df['subject'] == subject]
                for _, row in subj_df.iterrows():
                    sensor_label = row['sensor_combination']
                    if sensor_label in valid_sensors and row['physiological_parameter'] == physio_col:
                        data.append({
                            'subject': subject,
                            'sensor_pair': sensor_label,
                            'correlation': row['correlation_coefficient'],
                            'p_value': row.get('p_value', 1.0),  # Assume p_value in data
                            'significant': row.get('p_value', 1.0) < 0.05
                        })
            if not data:
                continue
            df = pd.DataFrame(data)
            n_subjects = len(df['subject'].unique())
            if n_subjects == 0:
                continue
            
            os.makedirs(subdir, exist_ok=True)
            
            # ç‰ˆæœ¬1: ä¸€å¼ å›¾æ‰€æœ‰6ä¸ª pair
            fig_width = max(12, n_subjects * 0.5)
            plt.figure(figsize=(fig_width, 8))
            ax = sns.barplot(data=df, x='subject', y='correlation', hue='sensor_pair', palette='tab10')
            lines = [(0.4, 'green'), (-0.4, 'green'), (0.5, 'blue'), (-0.5, 'blue'), (0.7, 'red'), (-0.7, 'red')]
            for val, color in lines:
                plt.axhline(val, color=color, linestyle='--')
            # Add significance asterisks
            for i, bar in enumerate(ax.patches):
                if i < len(df) and df.iloc[i]['significant']:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01 if bar.get_height() > 0 else bar.get_height() - 0.05,
                            '*', ha='center', va='bottom' if bar.get_height() > 0 else 'top')
            plt.title(f'Correlation Bar Chart for {physio_label} (All Pairs) {title_suffix}')
            plt.ylim(-1, 1)
            plt.xlabel('Subject')
            plt.ylabel('Pearson Correlation')
            plt.xticks(rotation=90, ha='right')
            plt.legend(title='Sensor Pair', bbox_to_anchor=(1.05, 1), loc='upper left')
            filename_multi = f'correlation_bar_{physio}_multi{("_exp" + str(exp_id) if exp_id else "")}.png'
            plt.savefig(os.path.join(subdir, filename_multi), bbox_inches='tight')
            plt.close()
            print(f"ğŸ’¾ ä¿å­˜ multi-pair æŸ±çŠ¶å›¾: {os.path.join(subdir, filename_multi)}")
            
            # ç‰ˆæœ¬2: æ¯ä¸ª pair å•ç‹¬ä¸€å¼ å›¾
            for pair_label in valid_sensors:
                pair_data = [d for d in data if d['sensor_pair'] == pair_label]
                if not pair_data:
                    continue
                pair_df = pd.DataFrame(pair_data)
                fig_width = max(12, len(pair_df) * 0.5)
                plt.figure(figsize=(fig_width, 8))
                ax = sns.barplot(data=pair_df, x='subject', y='correlation', color='skyblue')
                for val, color in lines:
                    plt.axhline(val, color=color, linestyle='--')
                # Add significance asterisks
                for i, bar in enumerate(ax.patches):
                    if pair_df.iloc[i]['significant']:
                        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01 if bar.get_height() > 0 else bar.get_height() - 0.05,
                                '*', ha='center', va='bottom' if bar.get_height() > 0 else 'top')
                plt.title(f'Correlation Bar Chart for {physio_label} - {pair_label} {title_suffix}')
                plt.ylim(-1, 1)
                plt.xlabel('Subject')
                plt.ylabel('Pearson Correlation')
                plt.xticks(rotation=90, ha='right')
                safe_pair = pair_label.replace('â†’', '-').replace(' ', '_')
                filename = f'correlation_bar_{physio}_{safe_pair}{("_exp" + str(exp_id) if exp_id else "")}.png'
                plt.savefig(os.path.join(subdir, filename), bbox_inches='tight')
                plt.close()
                print(f"ğŸ’¾ ä¿å­˜ per-pair æŸ±çŠ¶å›¾: {os.path.join(subdir, filename)}")
    
    def create_r_distribution_plot(self, corr_df, title_suffix, subjects, subdir, exp_id=None):
        """ç»˜åˆ¶ r çš„åˆ†å¸ƒå›¾ (violin plot with gradient)"""
        valid_sensors = list(self.ptt_combinations_en.values())
        for physio, physio_label in self.physiological_indicators.items():
            physio_col = f'{physio}_mean'
            data = []
            for subject in subjects:
                subj_df = corr_df[corr_df['subject'] == subject]
                for _, row in subj_df.iterrows():
                    sensor_label = row['sensor_combination']
                    if sensor_label in valid_sensors and row['physiological_parameter'] == physio_col:
                        data.append({
                            'sensor_pair': sensor_label,
                            'correlation': row['correlation_coefficient']
                        })
            if not data:
                continue
            df = pd.DataFrame(data)
            
            # æ£€æŸ¥æ¯ä¸ªä¼ æ„Ÿå™¨å¯¹çš„å—è¯•è€…æ•°é‡ï¼Œè¿‡æ»¤æ‰æ•°æ®ä¸è¶³çš„
            sensor_pair_counts = df.groupby('sensor_pair').size()
            valid_pairs = sensor_pair_counts[sensor_pair_counts >= 10].index.tolist()
            
            if not valid_pairs:
                print(f"âš ï¸ {physio_label}: æ‰€æœ‰ä¼ æ„Ÿå™¨å¯¹çš„å—è¯•è€…æ•°é‡éƒ½å°‘äº10ä¸ªï¼Œè·³è¿‡ç»˜åˆ¶")
                continue
            
            # åªä¿ç•™æ•°æ®å……è¶³çš„ä¼ æ„Ÿå™¨å¯¹
            df_filtered = df[df['sensor_pair'].isin(valid_pairs)]
            
            os.makedirs(subdir, exist_ok=True)
            plt.figure(figsize=(12, 8))
            sns.violinplot(data=df_filtered, x='sensor_pair', y='correlation', palette='viridis', inner='box')
            
            # æ·»åŠ æ°´å¹³å‚è€ƒçº¿
            lines = [(0, 'black'), (0.4, 'green'), (-0.4, 'green'), (0.7, 'red'), (-0.7, 'red')]
            for val, color in lines:
                plt.axhline(val, color=color, linestyle='--', linewidth=1)
            
            # è®¡ç®—å¹¶æ ‡æ³¨ Q1, median, Q3 å’Œæœ€å®½ç‚¹ (å³°å€¼)ï¼ŒåŒæ—¶æ ‡æ³¨å—è¯•è€…æ•°é‡
            from scipy.stats import gaussian_kde
            quantiles = df_filtered.groupby('sensor_pair')['correlation'].quantile([0.25, 0.5, 0.75]).unstack()
            
            for i, pair in enumerate(valid_pairs):
                pair_data = df_filtered[df_filtered['sensor_pair'] == pair]['correlation']
                if pair in quantiles.index and not pair_data.empty:
                    q1, median, q3 = quantiles.loc[pair, [0.25, 0.5, 0.75]]
                    n_subjects = len(pair_data)
                    
                    # åœ¨xè½´æ ‡ç­¾ä¸‹æ–¹æ ‡æ³¨å—è¯•è€…æ•°é‡
                    plt.text(i, -1.05, f'n={n_subjects}', ha='center', va='top', fontsize=10, 
                            color='black', fontweight='bold', bbox=dict(facecolor='yellow', alpha=0.7, edgecolor='black'))
                    
                    # æ ‡æ³¨ Q1 å’Œ Q3 (è°ƒæ•´ä½ç½®ä»¥æé«˜æ¸…æ™°åº¦)
                    plt.text(i + 0.2, q1 - 0.05, f'Q1: {q1:.2f}', ha='left', va='top', fontsize=8, color='blue', 
                            bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))
                    plt.text(i + 0.2, q3 + 0.05, f'Q3: {q3:.2f}', ha='left', va='bottom', fontsize=8, color='blue', 
                            bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))
                    
                    # ä¸­ä½æ•°ç”¨ç™½è‰²æ ‡æ³¨
                    plt.text(i, median + 0.05, f'Med: {median:.2f}', ha='center', va='bottom', fontsize=8, 
                            color='white', bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))
                    
                    # æ ‡æ³¨æœ€å®½ç‚¹ (å¯†åº¦å³°å€¼)
                    if len(pair_data) > 1:
                        pair_data = pair_data.dropna()  # ç§»é™¤ NaN ä»¥é¿å… KDE é”™è¯¯
                        if not pair_data.empty and np.isfinite(pair_data).all():
                            kde = gaussian_kde(pair_data)
                            y_vals = np.linspace(pair_data.min(), pair_data.max(), 100)
                            kde_vals = kde(y_vals)
                            peak_y = y_vals[np.argmax(kde_vals)]
                            plt.text(i - 0.2, peak_y, f'Peak: {peak_y:.2f}', ha='right', va='center', fontsize=8, 
                                    color='red', bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))
            
            # åœ¨æ ‡é¢˜ä¸­æ˜¾ç¤ºæœ‰æ•ˆä¼ æ„Ÿå™¨å¯¹çš„æ•°é‡
            title_with_count = f'Distribution of Correlations for {physio_label} {title_suffix}\n(Valid pairs: {len(valid_pairs)}/{len(valid_sensors)}, Min subjects: 10)'
            plt.title(title_with_count)
            plt.ylim(-1.2, 1.1)  # æ‰©å±• y è½´ä»¥å®¹çº³å—è¯•è€…æ•°é‡æ ‡æ³¨
            plt.xlabel('Sensor Pair')
            plt.ylabel('Pearson Correlation')
            plt.xticks(rotation=45, ha='right')
            
            # æ·»åŠ å›¾ä¾‹è¯´æ˜
            plt.figtext(0.02, 0.02, f'Note: Only sensor pairs with â‰¥10 subjects are shown. n=X shows subject count for each pair.', 
                       fontsize=9, style='italic', bbox=dict(facecolor='lightgray', alpha=0.8))
            
            filename = f'r_distribution_{physio}{("_exp" + str(exp_id) if exp_id else "")}.png'
            plt.savefig(os.path.join(subdir, filename), bbox_inches='tight')
            plt.close()
            print(f"ğŸ’¾ ä¿å­˜ r åˆ†å¸ƒå›¾: {os.path.join(subdir, filename)}")
            print(f"   ğŸ“Š æœ‰æ•ˆä¼ æ„Ÿå™¨å¯¹: {len(valid_pairs)}/{len(valid_sensors)} (å—è¯•è€…æ•°é‡â‰¥10)")
            if len(valid_pairs) < len(valid_sensors):
                skipped_pairs = set(valid_sensors) - set(valid_pairs)
                print(f"   âš ï¸ è·³è¿‡çš„ä¼ æ„Ÿå™¨å¯¹: {', '.join(skipped_pairs)} (æ•°æ®ä¸è¶³)")
    
    def remove_outliers_iqr(self, data_series):
        """ä½¿ç”¨IQRæ–¹æ³•å»é™¤æå€¼ (from step3)"""
        q1 = data_series.quantile(0.25)
        q3 = data_series.quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        return data_series[(data_series >= lower_bound) & (data_series <= upper_bound)]

    def create_bland_altman_plots(self, sync_df, exp_id=None, title_suffix=""):
        """åˆ›å»ºBland-Altmanå›¾ - å…­ä¸ªä¼ æ„Ÿå™¨å¯¹çš„SBPã€DBPå’ŒMean BP"""
        try:
            # è¡€å‹æŒ‡æ ‡ - åŒ…æ‹¬Mean BPï¼ˆåªå¤„ç†è¡€å‹ç›¸å…³æŒ‡æ ‡ï¼‰
            bp_indicators = ['systolic_bp_mean', 'diastolic_bp_mean', 'mean_bp_mean']
            bp_labels = ['Systolic BP', 'Diastolic BP', 'Mean BP']
            
            # ä¼ æ„Ÿå™¨å¯¹
            sensor_pairs = list(self.ptt_combinations_en.keys())
            
            # åˆ›å»º6x6çš„å­å›¾å¸ƒå±€ - 6è¡Œï¼ˆä¼ æ„Ÿå™¨å¯¹ï¼‰x 6åˆ—ï¼ˆæ¯ä¸ªç”Ÿç†æŒ‡æ ‡å·¦å³ä¸¤ä¸ªå›¾ï¼‰
            fig, axes = plt.subplots(6, 6, figsize=(24, 20))
            fig.suptitle(f'PTT vs Reference BP Analysis{title_suffix}', 
                        fontsize=16, fontweight='bold', y=1)
            
            # å­˜å‚¨è¯¯å·®å¸¦ç»Ÿè®¡ä¿¡æ¯
            error_band_stats = []
            
            for sensor_idx, sensor_pair in enumerate(sensor_pairs):
                for bp_idx, (bp_indicator, bp_label) in enumerate(zip(bp_indicators, bp_labels)):
                    # è®¡ç®—å­å›¾ä½ç½® - 6è¡Œx6åˆ—å¸ƒå±€
                    row = sensor_idx  # 0-5 for 6 sensor pairs
                    col_left = bp_idx * 2      # 0, 2, 4 for left plots (regression)
                    col_right = bp_idx * 2 + 1 # 1, 3, 5 for right plots (bland-altman)
                    
                    ax_left = axes[row, col_left]   # å·¦ä¾§å›å½’å›¾
                    ax_right = axes[row, col_right] # å³ä¾§Bland-Altmanå›¾
                    
                    # è·å–è¯¥ä¼ æ„Ÿå™¨å¯¹çš„æ•°æ®
                    pair_data = sync_df[sync_df['sensor_pair'] == sensor_pair].copy()
                    
                    if len(pair_data) < 10:
                        ax_left.text(0.5, 0.5, 'Insufficient Data', ha='center', va='center', 
                                   transform=ax_left.transAxes, fontsize=10)
                        ax_right.text(0.5, 0.5, 'Insufficient Data', ha='center', va='center', 
                                    transform=ax_right.transAxes, fontsize=10)
                        ax_left.set_title(f'{self.ptt_combinations_en[sensor_pair]}\n{bp_label}')
                        continue
                    
                    # å‡†å¤‡æ•°æ®
                    mask = ~(pair_data['ptt_ms'].isna() | pair_data[bp_indicator].isna())
                    if mask.sum() < 10:
                        ax_left.text(0.5, 0.5, 'Insufficient Data', ha='center', va='center', 
                                   transform=ax_left.transAxes, fontsize=10)
                        ax_right.text(0.5, 0.5, 'Insufficient Data', ha='center', va='center', 
                                    transform=ax_right.transAxes, fontsize=10)
                        ax_left.set_title(f'{self.ptt_combinations_en[sensor_pair]}\n{bp_label}')
                        continue
                    
                    ptt_vals = pair_data.loc[mask, 'ptt_ms'].values
                    bp_vals = pair_data.loc[mask, bp_indicator].values
                    
                    # æ„å»ºç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œé¢„æµ‹
                    try:
                        # æ ‡å‡†åŒ–æ•°æ®
                        scaler_ptt = StandardScaler()
                        scaler_bp = StandardScaler()
                        ptt_scaled = scaler_ptt.fit_transform(ptt_vals.reshape(-1, 1))
                        bp_scaled = scaler_bp.fit_transform(bp_vals.reshape(-1, 1))
                        
                        # è®­ç»ƒæ¨¡å‹
                        model = LinearRegression()
                        model.fit(ptt_scaled, bp_scaled.flatten())
                        
                        # é¢„æµ‹
                        bp_pred_scaled = model.predict(ptt_scaled)
                        bp_pred = scaler_bp.inverse_transform(bp_pred_scaled.reshape(-1, 1)).flatten()
                        
                    except Exception as e:
                        print(f"âš ï¸ æ¨¡å‹è®­ç»ƒå¤±è´¥ {sensor_pair}-{bp_indicator}: {e}")
                        ax_left.text(0.5, 0.5, 'Model Error', ha='center', va='center', 
                                   transform=ax_left.transAxes, fontsize=10)
                        ax_right.text(0.5, 0.5, 'Model Error', ha='center', va='center', 
                                    transform=ax_right.transAxes, fontsize=10)
                        ax_left.set_title(f'{self.ptt_combinations_en[sensor_pair]}\n{bp_label}')
                        continue
                    
                    # === å·¦ä¾§ï¼šå›å½’æ‹Ÿåˆå›¾ + è¯¯å·®å¸¦ ===
                    # ç»˜åˆ¶è¯¯å·®å¸¦ï¼ˆæ”¹è¿›çš„é¢œè‰²è®¾ç½®ï¼šä¸€å±‚ä¸€å±‚å åŠ ï¼‰
                    bp_range = [min(bp_vals.min(), bp_pred.min()), max(bp_vals.max(), bp_pred.max())]
                    # å…ˆç»˜åˆ¶æœ€å¤§çš„è¯¯å·®å¸¦ï¼ˆ15mmHgï¼‰
                    ax_left.fill_between(bp_range, 
                                       [b - 15 for b in bp_range], [b + 15 for b in bp_range],
                                       alpha=0.3, color='pink', label='Â±15 mmHg')
                    # å†ç»˜åˆ¶ä¸­ç­‰è¯¯å·®å¸¦ï¼ˆ10mmHgï¼‰
                    ax_left.fill_between(bp_range, 
                                       [b - 10 for b in bp_range], [b + 10 for b in bp_range],
                                       alpha=0.4, color=(1.0, 1.0, 0.6), label='Â±10 mmHg')
                    # æœ€åç»˜åˆ¶æœ€å°è¯¯å·®å¸¦ï¼ˆ5mmHgï¼‰
                    ax_left.fill_between(bp_range, 
                                       [b - 5 for b in bp_range], [b + 5 for b in bp_range],
                                       alpha=0.5, color=(0.7, 1.0, 0.7), label='Â±5 mmHg')
                    
                    # ç»˜åˆ¶ç†æƒ³çº¿ï¼ˆy=xï¼‰
                    ax_left.plot(bp_range, bp_range, 'k--', alpha=0.5, linewidth=1, label='Perfect Match')
                    
                    # æœ€åç»˜åˆ¶æ•°æ®ç‚¹ï¼ˆç¡®ä¿åœ¨æœ€ä¸Šå±‚ï¼‰
                    ax_left.scatter(bp_pred, bp_vals, alpha=0.6, s=20, color='blue')
                    
                    ax_left.set_xlabel('Predicted BP (mmHg)', fontsize=9)
                    ax_left.set_ylabel('Reference BP (mmHg)', fontsize=9)
                    ax_left.set_title(f'{self.ptt_combinations_en[sensor_pair]}\n{bp_label}', fontsize=10)
                    ax_left.grid(True, alpha=0.3)
                    ax_left.legend(fontsize=7, loc='upper left')
                    
                    # === å³ä¾§ï¼šBland-Altmanå›¾ ===
                    # Bland-Altmanè®¡ç®—
                    mean_bp = (bp_vals + bp_pred) / 2
                    diff_bp = bp_pred - bp_vals
                    
                    # è®¡ç®—ç»Ÿè®¡é‡
                    mean_diff = np.mean(diff_bp)
                    std_diff = np.std(diff_bp)
                    limits_of_agreement_upper = mean_diff + 1.96 * std_diff
                    limits_of_agreement_lower = mean_diff - 1.96 * std_diff
                    
                    # ç»˜åˆ¶Bland-Altmanå›¾
                    ax_right.scatter(mean_bp, diff_bp, alpha=0.6, s=20, color='blue')
                    
                    # ç»˜åˆ¶å‡å€¼çº¿å’Œä¸€è‡´æ€§ç•Œé™
                    ax_right.axhline(y=mean_diff, color='red', linestyle='-', linewidth=2, 
                                   label=f'Mean: {mean_diff:.2f}')
                    ax_right.axhline(y=limits_of_agreement_upper, color='red', linestyle='--', linewidth=1, 
                                   label=f'Upper LoA: {limits_of_agreement_upper:.2f}')
                    ax_right.axhline(y=limits_of_agreement_lower, color='red', linestyle='--', linewidth=1, 
                                   label=f'Lower LoA: {limits_of_agreement_lower:.2f}')
                    
                    ax_right.set_xlabel('Mean BP (mmHg)', fontsize=9)
                    ax_right.set_ylabel('Difference (Predicted - Reference) (mmHg)', fontsize=9)
                    ax_right.set_title(f'{self.ptt_combinations_en[sensor_pair]}\n{bp_label}', fontsize=10)
                    ax_right.grid(True, alpha=0.3)
                    ax_right.legend(fontsize=7, loc='upper right')
                    
                    # è®¡ç®—è¯¯å·®å¸¦ç»Ÿè®¡
                    abs_diff = np.abs(diff_bp)
                    within_5 = np.sum(abs_diff <= 5) / len(abs_diff) * 100
                    within_10 = np.sum(abs_diff <= 10) / len(abs_diff) * 100
                    within_15 = np.sum(abs_diff <= 15) / len(abs_diff) * 100
                    
                    error_band_stats.append({
                        'exp_id': exp_id,
                        'sensor_pair': sensor_pair,
                        'sensor_label': self.ptt_combinations_en[sensor_pair],
                        'bp_type': bp_label,
                        'n_samples': len(diff_bp),
                        'within_5_mmhg': within_5,
                        'within_10_mmhg': within_10,
                        'within_15_mmhg': within_15,
                        'mean_diff': mean_diff,
                        'std_diff': std_diff,
                        'loa_upper': limits_of_agreement_upper,
                        'loa_lower': limits_of_agreement_lower
                    })
                    
                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯åˆ°å›¾ä¸­
                    stats_text = f'n={len(diff_bp)}\nÂ±5mmHg: {within_5:.1f}%\nÂ±10mmHg: {within_10:.1f}%\nÂ±15mmHg: {within_15:.1f}%'
                    ax_right.text(0.02, 0.98, stats_text, transform=ax_right.transAxes, 
                               verticalalignment='top', fontsize=7, 
                               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            
            # è®¾ç½®åˆ—æ ‡é¢˜
            for i, bp_label in enumerate(bp_labels):
                # å¦‚æœæ˜¯Systolic BPï¼Œä½¿ç”¨æ›´å¤§çš„å­—ä½“å’ŒåŠ ç²—
                if bp_label == 'Systolic BP':
                    fontsize = 14
                    fontweight = 'bold'
                else:
                    fontsize = 12
                    fontweight = 'bold'
                fig.text(0.167 + i * 0.333, 0.97, bp_label, ha='center', va='center', 
                        fontsize=fontsize, fontweight=fontweight)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾åƒåˆ°å½“å‰å·¥ä½œç›®å½•
            if exp_id is not None:
                filename = f"bland_altman_bp_exp_{exp_id}{title_suffix.replace(' ', '_')}.png"
            else:
                filename = f"bland_altman_bp_overall{title_suffix.replace(' ', '_')}.png"
            
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"ğŸ’¾ ä¿å­˜Bland-Altmanå›¾: {filename}")
            
            # ä¿å­˜è¯¯å·®å¸¦ç»Ÿè®¡åˆ°CSV
            if error_band_stats:
                stats_df = pd.DataFrame(error_band_stats)
                if exp_id is not None:
                    stats_filename = f"error_band_stats_exp_{exp_id}{title_suffix.replace(' ', '_')}.csv"
                else:
                    stats_filename = f"error_band_stats_overall{title_suffix.replace(' ', '_')}.csv"
                stats_df.to_csv(stats_filename, index=False)
                print(f"ğŸ’¾ ä¿å­˜è¯¯å·®å¸¦ç»Ÿè®¡: {stats_filename}")
            
            return fig
            
        except Exception as e:
            print(f"âŒ Bland-Altmanå›¾åˆ›å»ºå¤±è´¥: {e}")
            return None

    def run_individual_experiment_analysis(self, subjects):
        print("\n=== å•å®éªŒåˆ†æ ===")
        individual_dir = os.path.join(self.output_dir, 'individual_experiments_correlations')
        os.makedirs(individual_dir, exist_ok=True)
        
        # å¤„ç†æ‰€æœ‰å¯ç”¨çš„å®éªŒï¼ˆä¸step3_calibratedä¿æŒä¸€è‡´ï¼‰
        target_experiments = list(range(1, 12))  # å®éªŒ1-11
        print(f"ğŸ¯ å¤„ç†å®éªŒ: {target_experiments}")
        
        for exp_id in target_experiments:
            exp_corrs = pd.DataFrame()
            for subject in subjects:
                df = self.load_step3_correlations(subject, exp_id)
                if df is not None:
                    exp_corrs = pd.concat([exp_corrs, df])
            if not exp_corrs.empty:
                print(f"ğŸ“Š ç”Ÿæˆå®éªŒ {exp_id} çš„æŸ±çŠ¶å›¾")
                subdir = os.path.join(individual_dir, f'exp_{exp_id}')
                os.makedirs(subdir, exist_ok=True)
                if 'subject' in exp_corrs.columns:
                    cols = ['subject'] + [col for col in exp_corrs.columns if col != 'subject']
                    exp_corrs = exp_corrs[cols]
                csv_path = os.path.join(subdir, f'exp_{exp_id}_correlations.csv')
                exp_corrs.to_csv(csv_path, index=False)
                print(f"ğŸ’¾ ä¿å­˜å®éªŒ {exp_id} çš„ correlations CSV: {csv_path}")
                self.create_correlation_bar_chart(exp_corrs, f'(Exp {exp_id})', subjects, subdir, exp_id)
                self.create_r_distribution_plot(exp_corrs, f'(Exp {exp_id})', subjects, subdir, exp_id)
            else:
                print(f"âš ï¸ å®éªŒ {exp_id} æ²¡æœ‰ç›¸å…³æ€§æ•°æ®")
    
    def run_subject_overall_analysis(self, subjects):
        print("\n=== æ¯ä¸ªå—è¯•è€…æ•´ä½“åˆ†æ ===")
        overall_corrs = pd.DataFrame()
        all_sync_df = pd.DataFrame()
        
        # åŠ è½½ç›¸å…³æ€§å’ŒåŒæ­¥æ•°æ®
        for subject in subjects:
            # åŠ è½½ç›¸å…³æ€§æ•°æ®
            df = self.load_step3_correlations(subject)
            if df is not None:
                overall_corrs = pd.concat([overall_corrs, df])
            
            # åŠ è½½åŒæ­¥æ•°æ®
            sync_df = self.load_step3_sync_data(subject)
            if sync_df is not None:
                all_sync_df = pd.concat([all_sync_df, sync_df], ignore_index=True)
        
        if not overall_corrs.empty:
            print("ğŸ“Š ç”Ÿæˆæ¯ä¸ªå—è¯•è€…æ•´ä½“æŸ±çŠ¶å›¾")
            subdir = os.path.join(self.output_dir, 'overall_correlations')
            os.makedirs(subdir, exist_ok=True)
            if 'subject' in overall_corrs.columns:
                cols = ['subject'] + [col for col in overall_corrs.columns if col != 'subject']
                overall_corrs = overall_corrs[cols]
            csv_path = os.path.join(subdir, 'overall_correlations.csv')
            overall_corrs.to_csv(csv_path, index=False)
            print(f"ğŸ’¾ ä¿å­˜æ•´ä½“ correlations CSV: {csv_path}")
            self.create_correlation_bar_chart(overall_corrs, '(Overall per Subject)', subjects, subdir)
            self.create_r_distribution_plot(overall_corrs, '(Overall per Subject)', subjects, subdir)
        
        # ä¸ºæ•´ä½“æ•°æ®åˆ›å»ºBland-Altmanå›¾
        if not all_sync_df.empty:
            print("ğŸ“Š ä¸ºæ•´ä½“æ•°æ®åˆ›å»ºBland-Altmanå›¾")
            # æ¸…ç†æ•°æ®
            cleaned_overall = pd.DataFrame()
            for pair in all_sync_df['sensor_pair'].unique():
                pair_df = all_sync_df[all_sync_df['sensor_pair'] == pair]
                for col in [c for c in pair_df.columns if c.endswith('_mean') or c == 'ptt_ms']:
                    cleaned_series = self.remove_outliers_iqr(pair_df[col])
                    mask = pair_df[col].isin(cleaned_series)
                    pair_df = pair_df[mask]
                cleaned_overall = pd.concat([cleaned_overall, pair_df])
            
            if not cleaned_overall.empty:
                # åˆ‡æ¢åˆ°æ­£ç¡®çš„ç›®å½•
                os.chdir(subdir)
                self.create_bland_altman_plots(cleaned_overall, None, " (Overall per Subject)")
                os.chdir(self.output_dir)  # å›åˆ°è¾“å‡ºç›®å½•
                
                # ä¿å­˜æ¸…ç†åçš„æ•°æ®
                cleaned_path = os.path.join(subdir, 'overall_cleaned.csv')
                cleaned_overall.to_csv(cleaned_path, index=False)
                print(f"ğŸ’¾ ä¿å­˜æ•´ä½“æ¸…ç†æ•°æ®: {cleaned_path}")
    
    def run_non_cross_experiment_fits(self, subjects):
        """ä¸è·¨å®éªŒçš„çº¿æ€§æ‹Ÿåˆï¼šæ¯ä¸ªå®éªŒå•ç‹¬æ‹Ÿåˆ"""
        print("\n=== ä¸è·¨å®éªŒçº¿æ€§æ‹Ÿåˆåˆ†æ ===")
        non_cross_dir = os.path.join(self.output_dir, 'per_experiment_fits')
        os.makedirs(non_cross_dir, exist_ok=True)
        
        all_sync_df = pd.DataFrame()
        for subject in subjects:
            df = self.load_step3_sync_data(subject)
            if df is not None:
                all_sync_df = pd.concat([all_sync_df, df], ignore_index=True)
        
        if all_sync_df.empty:
            print("âš ï¸ æ— å¯ç”¨ sync æ•°æ®")
            return
        
        if 'exp_id' not in all_sync_df.columns:
            print("âš ï¸ sync æ•°æ®ç¼ºå°‘ 'exp_id' åˆ—ï¼Œæ— æ³•è¿›è¡Œ per-exp åˆ†æ")
            return
        
        # å¤„ç†æ‰€æœ‰å¯ç”¨çš„å®éªŒ
        target_experiments = list(range(1, 12))  # å®éªŒ1-11
        for exp_id in target_experiments:
            if exp_id not in all_sync_df['exp_id'].unique():
                print(f"âš ï¸ å®éªŒ {exp_id} ä¸åœ¨æ•°æ®ä¸­ï¼Œè·³è¿‡")
                continue
                
            exp_sync = all_sync_df[all_sync_df['exp_id'] == exp_id].reset_index(drop=True)
            if exp_sync.empty:
                continue
            
            # Clean outliers per group
            cleaned_exp = pd.DataFrame()
            for pair in exp_sync['sensor_pair'].unique():
                pair_df = exp_sync[exp_sync['sensor_pair'] == pair]
                for col in [c for c in pair_df.columns if c.endswith('_mean') or c == 'ptt_ms']:
                    cleaned_series = self.remove_outliers_iqr(pair_df[col])
                    mask = pair_df[col].isin(cleaned_series)
                    pair_df = pair_df[mask]
                cleaned_exp = pd.concat([cleaned_exp, pair_df])
            
            cleaned_path = os.path.join(non_cross_dir, f'per_exp_{exp_id}_cleaned.csv')
            cleaned_exp.to_csv(cleaned_path, index=False)
            print(f"ğŸ’¾ ä¿å­˜ per-exp cleaned CSV: {cleaned_path}")
            
            # ç»˜åˆ¶æ•£ç‚¹å›¾ + çº¿æ€§æ‹Ÿåˆ
            for physio, label in self.physiological_indicators.items():
                col = f'{physio}_mean'
                if col in cleaned_exp.columns:
                    for pair in cleaned_exp['sensor_pair'].unique():
                        pair_df = cleaned_exp[cleaned_exp['sensor_pair'] == pair].reset_index(drop=True)
                        if len(pair_df) < 10:
                            continue
                        plt.figure(figsize=(10, 8))
                        # å•å®éªŒåˆ†æï¼šæŒ‰å®éªŒIDç€è‰² - ä¸ºå®éªŒ1å’Œ7åˆ†é…ä¸åŒçš„é¢œè‰²
                        if exp_id == 1:
                            exp_color = 'blue'
                            exp_label = 'Exp 1'
                        elif exp_id == 7:
                            exp_color = 'red'
                            exp_label = 'Exp 7'
                        else:
                            exp_color = 'green'
                            exp_label = f'Exp {exp_id}'
                        
                        # å•å®éªŒå†…ï¼Œä¸åŒå—è¯•è€…ç”¨ä¸åŒé¢œè‰²ï¼ˆä¿æŒåŸæœ‰çš„subjectåŒºåˆ†ï¼‰
                        sns.scatterplot(data=pair_df, x='ptt_ms', y=col, hue='subject', 
                                     palette='tab20', alpha=0.6, s=50)
                        
                        mask = ~(pair_df['ptt_ms'].isna() | pair_df[col].isna())
                        if mask.sum() >= 10:
                            X = pair_df.loc[mask, 'ptt_ms'].values.reshape(-1, 1)
                            y = pair_df.loc[mask, col].values
                            model = LinearRegression().fit(X, y)
                            pred = model.predict(X)
                            r, _ = stats.pearsonr(pair_df.loc[mask, 'ptt_ms'], y)
                            r2 = model.score(X, y)
                            mae = mean_absolute_error(y, pred)
                            std = np.std(y - pred)
                            x_sort = np.sort(X, axis=0)
                            # æ‹Ÿåˆçº¿ä½¿ç”¨ä¸å®éªŒç›¸åŒçš„é¢œè‰²
                            plt.plot(x_sort, model.predict(x_sort), color=exp_color, linewidth=2, 
                                   label=f'{exp_label} Fit', alpha=0.8)
                            
                            stats_text = f'r = {r:.2f}\nRÂ² = {r2:.2f}\nMAE = {mae:.2f}\nSTD = {std:.2f}'
                            plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes,
                                     bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray', boxstyle='round,pad=0.5'),
                                     verticalalignment='top')
                        
                        plt.title(f'{label} vs PTT ({self.ptt_combinations_en.get(pair, pair)}) - Per Exp {exp_id} (Cleaned)')
                        plt.xlabel('PTT (ms)')
                        plt.ylabel(label)
                        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
                        filename = f'scatter_fit_{physio}_{pair}_per_exp{exp_id}_cleaned.png'
                        plt.savefig(os.path.join(non_cross_dir, filename), bbox_inches='tight')
                        plt.close()
                        print(f"ğŸ’¾ ä¿å­˜ per-exp æ•£ç‚¹æ‹Ÿåˆå›¾: {os.path.join(non_cross_dir, filename)}")
            
            # ä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºBland-Altmanå›¾
            if not cleaned_exp.empty:
                print(f"ğŸ“Š ä¸ºå®éªŒ {exp_id} åˆ›å»ºBland-Altmanå›¾")
                # åˆ‡æ¢åˆ°æ­£ç¡®çš„ç›®å½•
                os.chdir(non_cross_dir)
                self.create_bland_altman_plots(cleaned_exp, exp_id, f" (Per Exp {exp_id})")
                os.chdir(self.output_dir)  # å›åˆ°è¾“å‡ºç›®å½•
    
    def run_cross_experiment_fits(self, subjects):
        """è·¨å®éªŒçš„çº¿æ€§æ‹Ÿåˆï¼šç»¼åˆæ‰€æœ‰å®éªŒ"""
        print("\n=== è·¨å®éªŒçº¿æ€§æ‹Ÿåˆåˆ†æ ===")
        cross_dir = os.path.join(self.output_dir, 'cross_experiment_fits')
        os.makedirs(cross_dir, exist_ok=True)
        
        all_sync_df = pd.DataFrame()
        for subject in subjects:
            df = self.load_step3_sync_data(subject)
            if df is not None:
                all_sync_df = pd.concat([all_sync_df, df], ignore_index=True)
        
        if all_sync_df.empty:
            print("âš ï¸ æ— å¯ç”¨ sync æ•°æ®")
            return
        
        # Clean comprehensive
        cleaned_comp = pd.DataFrame()
        for pair in all_sync_df['sensor_pair'].unique():
            pair_df = all_sync_df[all_sync_df['sensor_pair'] == pair]
            for col in [c for c in pair_df.columns if c.endswith('_mean') or c == 'ptt_ms']:
                cleaned_series = self.remove_outliers_iqr(pair_df[col])
                mask = pair_df[col].isin(cleaned_series)
                pair_df = pair_df[mask]
            cleaned_comp = pd.concat([cleaned_comp, pair_df])
        
        cleaned_comp_path = os.path.join(cross_dir, 'cross_experiments_cleaned.csv')
        cleaned_comp.to_csv(cleaned_comp_path, index=False)
        print(f"ğŸ’¾ ä¿å­˜ cross-exp cleaned CSV: {cleaned_comp_path}")
        
        # ç»¼åˆæ•£ç‚¹ + æ‹Ÿåˆ
        for physio, label in self.physiological_indicators.items():
            col = f'{physio}_mean'
            if col in cleaned_comp.columns:
                for pair in cleaned_comp['sensor_pair'].unique():
                    pair_df = cleaned_comp[cleaned_comp['sensor_pair'] == pair].reset_index(drop=True)
                    if len(pair_df) < 10:
                        continue
                    plt.figure(figsize=(10, 8))
                    # è·¨å®éªŒåˆ†æï¼šåŒä¸€å®éªŒå†…ï¼Œä¸åŒå—è¯•è€…ç”¨ä¸åŒé¢œè‰²
                    # ä½¿ç”¨seabornçš„hueå‚æ•°æ¥åŒºåˆ†ä¸åŒå—è¯•è€…
                    sns.scatterplot(data=pair_df, x='ptt_ms', y=col, hue='subject', 
                                 palette='tab20', alpha=0.6, s=50)
                    
                    mask = ~(pair_df['ptt_ms'].isna() | pair_df[col].isna())
                    if mask.sum() >= 10:
                        X = pair_df.loc[mask, 'ptt_ms'].values.reshape(-1, 1)
                        y = pair_df.loc[mask, col].values
                        model = LinearRegression().fit(X, y)
                        pred = model.predict(X)
                        r, _ = stats.pearsonr(pair_df.loc[mask, 'ptt_ms'], y)
                        r2 = model.score(X, y)
                        mae = mean_absolute_error(y, pred)
                        std = np.std(y - pred)
                        x_sort = np.sort(X, axis=0)
                        # æ‹Ÿåˆçº¿ä½¿ç”¨é»‘è‰²ï¼Œè¡¨ç¤ºæ•´ä½“æ‹Ÿåˆ
                        plt.plot(x_sort, model.predict(x_sort), color='black', linewidth=2, 
                               label='Overall Fit', alpha=0.8)
                        
                        stats_text = f'r = {r:.2f}\nRÂ² = {r2:.2f}\nMAE = {mae:.2f}\nSTD = {std:.2f}'
                        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes,
                                 bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray', boxstyle='round,pad=0.5'),
                                 verticalalignment='top')
                    
                    plt.title(f'{label} vs PTT ({self.ptt_combinations_en.get(pair, pair)}) - Cross Experiments (Cleaned)')
                    plt.xlabel('PTT (ms)')
                    plt.ylabel(label)
                    plt.legend(title='Subject ID', bbox_to_anchor=(1.05, 1), loc='upper left')
                    filename = f'scatter_fit_{physio}_{pair}_cross_experiments_cleaned.png'
                    plt.savefig(os.path.join(cross_dir, filename), bbox_inches='tight')
                    plt.close()
                    print(f"ğŸ’¾ ä¿å­˜ cross-exp æ•£ç‚¹æ‹Ÿåˆå›¾: {os.path.join(cross_dir, filename)}")
            
            # ä¸ºè·¨å®éªŒæ•°æ®åˆ›å»ºBland-Altmanå›¾
            if not cleaned_comp.empty:
                print(f"ğŸ“Š ä¸ºè·¨å®éªŒæ•°æ®åˆ›å»ºBland-Altmanå›¾")
                # åˆ‡æ¢åˆ°æ­£ç¡®çš„ç›®å½•
                os.chdir(cross_dir)
                self.create_bland_altman_plots(cleaned_comp, None, " (Cross Experiments)")
                os.chdir(self.output_dir)  # å›åˆ°è¾“å‡ºç›®å½•

    def run_integrated_analysis(self, subjects):
        """èåˆåˆ†æï¼šè·¨å—è¯•è€…æ•´åˆ (ç°åœ¨åªä¿å­˜CSVï¼Œä¸ç”Ÿæˆæ‹Ÿåˆå›¾)"""
        print("\n=== èåˆåˆ†æ ===")
        integrated_dir = os.path.join(self.output_dir, 'integrated_experiments')
        os.makedirs(integrated_dir, exist_ok=True)
        
        all_sync_df = pd.DataFrame()
        for subject in subjects:
            df = self.load_step3_sync_data(subject)
            if df is not None:
                all_sync_df = pd.concat([all_sync_df, df], ignore_index=True)
        
        if all_sync_df.empty:
            print("âš ï¸ æ— å¯ç”¨ sync æ•°æ®")
            return
        
        if 'subject' in all_sync_df.columns:
            cols = ['subject'] + [col for col in all_sync_df.columns if col != 'subject']
            all_sync_df = all_sync_df[cols]
        
        if 'exp_id' in all_sync_df.columns:
            # åªå¤„ç†å®éªŒ1å’Œ7
            target_experiments = [1, 7]
            for exp_id in target_experiments:
                if exp_id in all_sync_df['exp_id'].unique():
                    exp_sync = all_sync_df[all_sync_df['exp_id'] == exp_id].reset_index(drop=True)
                    if not exp_sync.empty:
                        csv_path = os.path.join(integrated_dir, f'integrated_exp_{exp_id}.csv')
                        exp_sync.to_csv(csv_path, index=False)
                        print(f"ğŸ’¾ ä¿å­˜æ•´åˆ CSV: {csv_path}")
                else:
                    print(f"âš ï¸ å®éªŒ {exp_id} ä¸åœ¨æ•°æ®ä¸­ï¼Œè·³è¿‡")
        else:
            print("âš ï¸ sync æ•°æ®ç¼ºå°‘ 'exp_id' åˆ—ï¼Œæ— æ³• per-exp æ‹†åˆ†")
        
        comprehensive_path = os.path.join(self.output_dir, 'comprehensive_integrated.csv')
        all_sync_df.to_csv(comprehensive_path, index=False)
        print(f"ğŸ’¾ ä¿å­˜ç»¼åˆ CSV: {comprehensive_path}")

    def run_static_experiments_analysis(self, subjects):
        """é™æ­¢ç›¸å…³å®éªŒåˆ†æï¼šæ‰€æœ‰å®éªŒ1-11çš„è·¨å®éªŒæ‹Ÿåˆå’Œç›¸å…³æ€§åˆ†æï¼Œä¿å­˜åˆ°æ¯ä¸ªäººçš„å•ç‹¬æ–‡ä»¶å¤¹"""
        print("\n=== é™æ­¢ç›¸å…³å®éªŒåˆ†æ (æ‰€æœ‰å®éªŒ1-11) ===")
        static_experiments = list(range(1, 12))  # ä¸step3_calibratedä¿æŒä¸€è‡´ï¼Œå¤„ç†æ‰€æœ‰å®éªŒ
        
        # ä¸ºæ¯ä¸ªå—è¯•è€…åˆ›å»ºå•ç‹¬çš„åˆ†æ
        for subject in subjects:
            print(f"\nğŸ“Š åˆ†æå—è¯•è€… {subject} çš„é™æ­¢ç›¸å…³å®éªŒ")
            subject_static_dir = os.path.join(self.root_path, subject, 'static_experiments_analysis_calibrated')
            os.makedirs(subject_static_dir, exist_ok=True)
            
            # åŠ è½½è¯¥å—è¯•è€…çš„æ‰€æœ‰é™æ­¢ç›¸å…³å®éªŒæ•°æ®
            subject_corrs = pd.DataFrame()
            subject_sync_data = pd.DataFrame()
            
            for exp_id in static_experiments:
                # åŠ è½½ç›¸å…³æ€§æ•°æ®
                corr_df = self.load_step3_correlations(subject, exp_id)
                if corr_df is not None:
                    corr_df['exp_id'] = exp_id
                    subject_corrs = pd.concat([subject_corrs, corr_df], ignore_index=True)
                
                # åŠ è½½åŒæ­¥æ•°æ®ï¼ˆä¸åº”ç”¨è¿‡æ»¤ï¼Œå› ä¸ºå·²ç»åœ¨get_available_subjectsä¸­è¿‡æ»¤è¿‡äº†ï¼‰
                sync_df = self.load_step3_sync_data(subject, exp_id, apply_filter=False)
                if sync_df is not None:
                    # å¦‚æœsyncæ•°æ®æ²¡æœ‰exp_idåˆ—ï¼Œæ·»åŠ å®ƒ
                    if 'exp_id' not in sync_df.columns:
                        sync_df['exp_id'] = exp_id
                    subject_sync_data = pd.concat([subject_sync_data, sync_df], ignore_index=True)
            
            if subject_corrs.empty and subject_sync_data.empty:
                print(f"âš ï¸ å—è¯•è€… {subject} æ²¡æœ‰é™æ­¢ç›¸å…³å®éªŒæ•°æ®")
                continue
            
            # ä¿å­˜ç›¸å…³æ€§æ•°æ®
            if not subject_corrs.empty:
                corr_path = os.path.join(subject_static_dir, f'{subject}_static_experiments_correlations.csv')
                subject_corrs.to_csv(corr_path, index=False)
                print(f"ğŸ’¾ ä¿å­˜ {subject} é™æ­¢å®éªŒç›¸å…³æ€§æ•°æ®: {corr_path}")
                
                # åˆ›å»ºç›¸å…³æ€§æŸ±çŠ¶å›¾
                self.create_correlation_bar_chart(subject_corrs, f'(Static Exp 1,7 - {subject})', 
                                               [subject], subject_static_dir, None)
                self.create_r_distribution_plot(subject_corrs, f'(Static Exp 1,7 - {subject})', 
                                             [subject], subject_static_dir, None)
            
            # ä¿å­˜åŒæ­¥æ•°æ®
            if not subject_sync_data.empty:
                sync_path = os.path.join(subject_static_dir, f'{subject}_static_experiments_sync.csv')
                subject_sync_data.to_csv(sync_path, index=False)
                print(f"ğŸ’¾ ä¿å­˜ {subject} é™æ­¢å®éªŒåŒæ­¥æ•°æ®: {sync_path}")
                
                # æ¸…ç†æ•°æ®
                cleaned_static = pd.DataFrame()
                for pair in subject_sync_data['sensor_pair'].unique():
                    pair_df = subject_sync_data[subject_sync_data['sensor_pair'] == pair]
                    for col in [c for c in pair_df.columns if c.endswith('_mean') or c == 'ptt_ms']:
                        if col in pair_df.columns:
                            cleaned_series = self.remove_outliers_iqr(pair_df[col])
                            mask = pair_df[col].isin(cleaned_series)
                            pair_df = pair_df[mask]
                    cleaned_static = pd.concat([cleaned_static, pair_df])
                
                if not cleaned_static.empty:
                    cleaned_path = os.path.join(subject_static_dir, f'{subject}_static_experiments_cleaned.csv')
                    cleaned_static.to_csv(cleaned_path, index=False)
                    print(f"ğŸ’¾ ä¿å­˜ {subject} é™æ­¢å®éªŒæ¸…ç†æ•°æ®: {cleaned_path}")
                    
                    # åˆ›å»ºæ•£ç‚¹æ‹Ÿåˆå›¾
                    for physio, label in self.physiological_indicators.items():
                        col = f'{physio}_mean'
                        if col in cleaned_static.columns:
                            for pair in cleaned_static['sensor_pair'].unique():
                                pair_df = cleaned_static[cleaned_static['sensor_pair'] == pair].reset_index(drop=True)
                                if len(pair_df) < 5:  # é™ä½é˜ˆå€¼ï¼Œå› ä¸ºåªåˆ†æ2ä¸ªå®éªŒ
                                    continue
                                
                                plt.figure(figsize=(10, 8))
                                # é™æ­¢å®éªŒåˆ†æï¼šåŒä¸€å®éªŒå†…ï¼Œä¸åŒå—è¯•è€…ç”¨ä¸åŒé¢œè‰²
                                sns.scatterplot(data=pair_df, x='ptt_ms', y=col, hue='subject', 
                                             palette='tab20', alpha=0.7, s=50)
                                
                                mask = ~(pair_df['ptt_ms'].isna() | pair_df[col].isna())
                                if mask.sum() >= 5:
                                    X = pair_df.loc[mask, 'ptt_ms'].values.reshape(-1, 1)
                                    y = pair_df.loc[mask, col].values
                                    model = LinearRegression().fit(X, y)
                                    pred = model.predict(X)
                                    r, _ = stats.pearsonr(pair_df.loc[mask, 'ptt_ms'], y)
                                    r2 = model.score(X, y)
                                    mae = mean_absolute_error(y, pred)
                                    std = np.std(y - pred)
                                    x_sort = np.sort(X, axis=0)
                                    # æ‹Ÿåˆçº¿ä½¿ç”¨é»‘è‰²ï¼Œè¡¨ç¤ºæ•´ä½“æ‹Ÿåˆ
                                    plt.plot(x_sort, model.predict(x_sort), color='black', linewidth=2, 
                                           label='Overall Fit', alpha=0.8)
                                    
                                    stats_text = f'r = {r:.3f}\nRÂ² = {r2:.3f}\nMAE = {mae:.2f}\nSTD = {std:.2f}'
                                    plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes,
                                             bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray', 
                                                      boxstyle='round,pad=0.5'), verticalalignment='top')
                                
                                plt.title(f'{label} vs PTT ({self.ptt_combinations_en.get(pair, pair)}) - {subject} Static Exp 1-11')
                                plt.xlabel('PTT (ms)')
                                plt.ylabel(label)
                                plt.legend(title='Subject ID', bbox_to_anchor=(1.05, 1), loc='upper left')
                                filename = f'{subject}_scatter_fit_{physio}_{pair}_static_exp_1_11.png'
                                plt.savefig(os.path.join(subject_static_dir, filename), bbox_inches='tight')
                                plt.close()
                                print(f"ğŸ’¾ ä¿å­˜ {subject} é™æ­¢å®éªŒæ•£ç‚¹æ‹Ÿåˆå›¾: {filename}")
                    
                    # åˆ›å»ºBland-Altmanå›¾
                    print(f"ğŸ“Š ä¸º {subject} çš„é™æ­¢å®éªŒåˆ›å»ºBland-Altmanå›¾")
                    os.chdir(subject_static_dir)
                    self.create_bland_altman_plots(cleaned_static, None, f" (Static Exp 1-11 - {subject})")
                    os.chdir(self.output_dir)  # å›åˆ°è¾“å‡ºç›®å½•
        
        print(f"\nâœ… é™æ­¢ç›¸å…³å®éªŒåˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨æ¯ä¸ªå—è¯•è€…çš„ static_experiments_analysis_calibrated/ æ–‡ä»¶å¤¹ä¸­")

    def create_analysis_summary(self, subjects, status_summary, filtered_subjects=None, filter_summary=None):
        """åˆ›å»ºåˆ†ææ€»ç»“æŠ¥å‘Š"""
        print("\n=== åˆ›å»ºåˆ†ææ€»ç»“æŠ¥å‘Š ===")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_subjects = len(status_summary)
        success_subjects = len(subjects)
        failed_count = len([s for s in status_summary if s['status'] == 'FAILED'])
        error_count = len([s for s in status_summary if s['status'] == 'ERROR'])
        no_file_count = len([s for s in status_summary if s['status'] == 'NO_FILE'])
        bp_filtered_count = len(filtered_subjects) if filtered_subjects else 0
        
        # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
        summary_file = os.path.join(self.output_dir, 'step4_calibrated_filtered_analysis_summary.txt')
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Step4 Calibrated Filtered åˆ†ææ€»ç»“æŠ¥å‘Š\n")
            f.write(f"="*50 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n\n")
            
            f.write(f"ğŸ” è¡€å‹æ•°æ®è¿‡æ»¤è®¾ç½®:\n")
            for bp_type, thresholds in self.bp_filter_thresholds.items():
                f.write(f"   {bp_type}: {thresholds['min']}-{thresholds['max']} mmHg\n")
            f.write("\n")
            
            f.write(f"ğŸ“Š å—è¯•è€…å¤„ç†çŠ¶æ€ç»Ÿè®¡:\n")
            f.write(f"   ğŸ“‹ æ€»å—è¯•è€…æ•°: {total_subjects}\n")
            f.write(f"   âœ… æˆåŠŸå¤„ç†: {success_subjects} ({success_subjects/total_subjects*100:.1f}%)\n")
            f.write(f"   âŒ å¤„ç†å¤±è´¥: {failed_count} ({failed_count/total_subjects*100:.1f}%)\n")
            f.write(f"   âš ï¸  å¤„ç†å‡ºé”™: {error_count} ({error_count/total_subjects*100:.1f}%)\n")
            f.write(f"   ğŸ“ æ— çŠ¶æ€æ–‡ä»¶: {no_file_count} ({no_file_count/total_subjects*100:.1f}%)\n")
            f.write(f"   ğŸ” è¡€å‹æ•°æ®å¼‚å¸¸è¢«è¿‡æ»¤: {bp_filtered_count}\n")
            f.write(f"   ğŸ¯ æœ€ç»ˆå¯ç”¨å—è¯•è€…: {len(subjects)}\n\n")
            
            f.write(f"ğŸ¯ å¯ç”¨å—è¯•è€…åˆ—è¡¨ ({len(subjects)}ä¸ª):\n")
            for i, subject in enumerate(subjects, 1):
                f.write(f"{subject}")
                if i % 10 == 0:
                    f.write("\n")
                elif i < len(subjects):
                    f.write(", ")
            f.write("\n\n")
            
            if filtered_subjects:
                f.write(f"ğŸ” è¢«è¡€å‹è¿‡æ»¤ç­›æ‰çš„å—è¯•è€… ({len(filtered_subjects)}ä¸ª):\n")
                for subject in filtered_subjects:
                    filter_info = next((f for f in filter_summary if f['subject'] == subject), None) if filter_summary else None
                    if filter_info:
                        f.write(f"   âŒ {subject}: {filter_info.get('reason', 'Unknown reason')}\n")
                        if 'removal_rate' in filter_info:
                            f.write(f"       æ•°æ®ç§»é™¤ç‡: {filter_info['removal_rate']:.1f}%\n")
                f.write("\n")
            
            f.write(f"ğŸ“ è¾“å‡ºç›®å½•ç»“æ„:\n")
            f.write(f"   {self.output_dir}/\n")
            f.write(f"   â”œâ”€â”€ subject_processing_status.csv          # å—è¯•è€…å¤„ç†çŠ¶æ€\n")
            f.write(f"   â”œâ”€â”€ bp_data_quality_summary.csv            # è¡€å‹æ•°æ®è´¨é‡ç»Ÿè®¡\n")
            f.write(f"   â”œâ”€â”€ bp_filter_summary.csv                  # è¡€å‹è¿‡æ»¤ç»Ÿè®¡\n")
            f.write(f"   â”œâ”€â”€ individual_experiments_correlations/   # å•å®éªŒç›¸å…³æ€§åˆ†æ\n")
            f.write(f"   â”œâ”€â”€ overall_correlations/                  # æ•´ä½“ç›¸å…³æ€§åˆ†æ\n")
            f.write(f"   â”œâ”€â”€ per_experiment_fits/                   # å•å®éªŒæ‹Ÿåˆåˆ†æ\n")
            f.write(f"   â”œâ”€â”€ cross_experiment_fits/                 # è·¨å®éªŒæ‹Ÿåˆåˆ†æ\n")
            f.write(f"   â”œâ”€â”€ integrated_experiments/                # èåˆåˆ†æ\n")
            f.write(f"   â””â”€â”€ step4_moved_filtered_analysis_summary.txt  # æœ¬æ€»ç»“æŠ¥å‘Š\n\n")
            
            f.write(f"ğŸ”¬ åˆ†æç‰¹ç‚¹:\n")
            f.write(f"   â€¢ ä½¿ç”¨æ ¡å‡†åçš„è¡€å‹æ•°æ®ï¼ˆPhysioNet2025_Calibratedï¼‰ä½œä¸ºçœŸå®å€¼\n")
            f.write(f"   â€¢ å¤„ç†æ‰€æœ‰å®éªŒ1-11ï¼ˆä¸step3_calibratedä¿æŒä¸€è‡´ï¼‰\n")
            f.write(f"   â€¢ è‡ªåŠ¨æ£€æŸ¥å—è¯•è€…å¤„ç†çŠ¶æ€ï¼Œåªåˆ†ææˆåŠŸçš„æ•°æ®\n")
            f.write(f"   â€¢ è‡ªåŠ¨è¿‡æ»¤å¼‚å¸¸çš„è¡€å‹æ•°æ®ï¼ˆæ”¶ç¼©å‹<80æˆ–>200ï¼Œèˆ’å¼ å‹<50æˆ–>120ï¼Œå¹³å‡è¡€å‹<60æˆ–>150ï¼‰\n")
            f.write(f"   â€¢ ç”Ÿæˆå¤šç§å¯è§†åŒ–å›¾è¡¨å’Œç»Ÿè®¡æŠ¥å‘Š\n")
            f.write(f"   â€¢ æ”¯æŒè·¨å—è¯•è€…å’Œè·¨å®éªŒçš„ç»¼åˆåˆ†æ\n\n")
            
            f.write(f"ğŸ’¡ å»ºè®®:\n")
            f.write(f"1. æˆåŠŸå¤„ç†çš„{len(subjects)}ä¸ªå—è¯•è€…æ•°æ®å·²å®Œæ•´åˆ†æ\n")
            if bp_filtered_count > 0:
                f.write(f"2. {bp_filtered_count}ä¸ªå—è¯•è€…å› è¡€å‹æ•°æ®å¼‚å¸¸è¢«è¿‡æ»¤ï¼Œå»ºè®®æ£€æŸ¥Biopacæ ¡å‡†\n")
            if error_count > 0:
                f.write(f"3. å‡ºé”™çš„{error_count}ä¸ªå—è¯•è€…éœ€è¦æ£€æŸ¥step3å¤„ç†çŠ¶æ€\n")
            if failed_count > 0:
                f.write(f"4. å¤±è´¥çš„{failed_count}ä¸ªå—è¯•è€…éœ€è¦é‡æ–°è¿è¡Œstep3\n")
            f.write(f"5. æ€»ä½“æˆåŠŸç‡{len(subjects)/total_subjects*100:.1f}%ï¼Œåˆ†ææ•ˆæœè‰¯å¥½\n")
        
        print(f"ğŸ’¾ ä¿å­˜åˆ†ææ€»ç»“æŠ¥å‘Š: {summary_file}")
        return summary_file

def main():
    analyzer = IntegratedPTTBloodPressureAnalyzerMoved()
    subjects, status_summary, filtered_subjects, filter_summary = analyzer.get_available_subjects()
    
    if not subjects:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„å—è¯•è€…æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œstep3_calibratedå®Œæˆæ•°æ®å¤„ç†")
        return
    
    print(f"\nğŸ“‹ å‘ç° {len(subjects)} ä¸ªå—è¯•è€…")
    print(f"   ğŸ¯ å®é™…å¯ç”¨çš„å—è¯•è€…: {len(subjects)}")
    if filtered_subjects:
        print(f"   ğŸ” è¢«è¡€å‹è¿‡æ»¤ç­›æ‰çš„å—è¯•è€…: {len(filtered_subjects)}")
    
    print("\nğŸ“‹ è¯·é€‰æ‹©åˆ†ææ–¹å¼:")
    print("1. ç»¼åˆåˆ†æ (æ‰€æœ‰)")
    print("2. å•å®éªŒç›¸å…³æ€§åˆ†æ (æ¯ä¸ªå®éªŒçš„æŸ±çŠ¶å›¾)")
    print("3. ç»¼åˆå®éªŒç›¸å…³æ€§åˆ†æ (æ‰€æœ‰å®éªŒçš„æŸ±çŠ¶å›¾)")
    print("4. ä¸è·¨å®éªŒçš„çº¿æ€§æ‹Ÿåˆ")
    print("5. è·¨å®éªŒçš„çº¿æ€§æ‹Ÿåˆ")
    print("6. é™æ­¢ç›¸å…³å®éªŒåˆ†æ (æ‰€æœ‰å®éªŒ1-11)")
    print("7. åªç”ŸæˆçŠ¶æ€ç»Ÿè®¡æŠ¥å‘Š")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3/4/5/6/7, é»˜è®¤1): ").strip()
    if not choice:
        choice = "1"
    
    if choice == "1":
        print("\nğŸš€ è¿è¡Œç»¼åˆåˆ†æ...")
        analyzer.run_individual_experiment_analysis(subjects)
        analyzer.run_subject_overall_analysis(subjects)
        analyzer.run_integrated_analysis(subjects)
        analyzer.run_non_cross_experiment_fits(subjects)
        analyzer.run_cross_experiment_fits(subjects)
        analyzer.run_static_experiments_analysis(subjects)
        analyzer.create_analysis_summary(subjects, status_summary, filtered_subjects, filter_summary)
    elif choice == "2":
        print("\nğŸš€ è¿è¡Œå•å®éªŒç›¸å…³æ€§åˆ†æ...")
        analyzer.run_individual_experiment_analysis(subjects)
        analyzer.create_analysis_summary(subjects, status_summary, filtered_subjects, filter_summary)
    elif choice == "3":
        print("\nğŸš€ è¿è¡Œç»¼åˆå®éªŒç›¸å…³æ€§åˆ†æ...")
        analyzer.run_subject_overall_analysis(subjects)
        analyzer.create_analysis_summary(subjects, status_summary, filtered_subjects, filter_summary)
    elif choice == "4":
        print("\nğŸš€ è¿è¡Œä¸è·¨å®éªŒçº¿æ€§æ‹Ÿåˆ...")
        analyzer.run_non_cross_experiment_fits(subjects)
        analyzer.create_analysis_summary(subjects, status_summary, filtered_subjects, filter_summary)
    elif choice == "5":
        print("\nğŸš€ è¿è¡Œè·¨å®éªŒçº¿æ€§æ‹Ÿåˆ...")
        analyzer.run_cross_experiment_fits(subjects)
        analyzer.create_analysis_summary(subjects, status_summary, filtered_subjects, filter_summary)
    elif choice == "6":
        print("\nğŸš€ è¿è¡Œé™æ­¢ç›¸å…³å®éªŒåˆ†æ...")
        analyzer.run_static_experiments_analysis(subjects)
        analyzer.create_analysis_summary(subjects, status_summary, filtered_subjects, filter_summary)
    elif choice == "7":
        print("\nğŸš€ åªç”ŸæˆçŠ¶æ€ç»Ÿè®¡æŠ¥å‘Š...")
        analyzer.create_analysis_summary(subjects, status_summary, filtered_subjects, filter_summary)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤è¿è¡Œç»¼åˆåˆ†æ")
        analyzer.run_individual_experiment_analysis(subjects)
        analyzer.run_subject_overall_analysis(subjects)
        analyzer.run_integrated_analysis(subjects)
        analyzer.run_non_cross_experiment_fits(subjects)
        analyzer.run_cross_experiment_fits(subjects)
        analyzer.run_static_experiments_analysis(subjects)
        analyzer.create_analysis_summary(subjects, status_summary, filtered_subjects, filter_summary)
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {analyzer.output_dir}")
    print(f"ğŸ“Š æˆåŠŸåˆ†æäº† {len(subjects)} ä¸ªå—è¯•è€…çš„æ•°æ®")
    print(f"ğŸ¯ ä½¿ç”¨æ ¡å‡†åçš„è¡€å‹æ•°æ®ï¼ˆPhysioNet2025_Calibratedï¼‰ä½œä¸ºçœŸå®å€¼")
    print(f"ğŸ”¬ å¤„ç†æ‰€æœ‰å®éªŒ1-11ï¼ˆä¸step3_calibratedä¿æŒä¸€è‡´ï¼‰")
    print(f"ğŸ” è‡ªåŠ¨è¿‡æ»¤å¼‚å¸¸çš„è¡€å‹æ•°æ®")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    total_count = len(status_summary)
    success_count = len(subjects)
    bp_filtered_count = len(filtered_subjects) if filtered_subjects else 0
    print(f"\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
    print(f"   ğŸ“‹ æ€»å—è¯•è€…æ•°: {total_count}")
    print(f"   âœ… æˆåŠŸåˆ†æ: {success_count} ({success_count/total_count*100:.1f}%)")
    print(f"   ğŸ” è¡€å‹è¿‡æ»¤ç­›æ‰: {bp_filtered_count} ({bp_filtered_count/total_count*100:.1f}%)")
    print(f"   ğŸ¯ åˆ†ææˆåŠŸç‡: {success_count/total_count*100:.1f}%")

if __name__ == "__main__":
    main()