#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆPI-Labæ•°æ®é¢„å¤„ç†è„šæœ¬ - ä½¿ç”¨PhysioNet2025_Calibratedæ–‡ä»¶å¤¹ä¸­çš„æ ¡å‡†è¡€å‹æ•°æ®
æ™ºèƒ½å¤„ç†ï¼šBiopacé™é‡‡æ · + HUBæ’å€¼å»é‡ + æ’å€¼å¯¹é½ + åŒæ ¼å¼ä¿å­˜ + æ ¡å‡†è¡€å‹æ•°æ®
"""

import os
import numpy as np
import pandas as pd
import pickle
from tqdm import tqdm
from scipy.interpolate import interp1d
from scipy.signal import find_peaks
import warnings
import time
import multiprocessing
import warnings
from multiprocessing import Manager
warnings.filterwarnings('ignore')

def extract_sbp_dbp_from_waveform(bp_waveform, window_size=2000, min_peak_distance=500):
    """
    ä»ABPæ³¢å½¢ä¸­æå–SBPå’ŒDBP
    ä½¿ç”¨ä¼˜åŒ–çš„å³°å€¼æ£€æµ‹æ–¹æ³•ï¼Œä¸“é—¨é’ˆå¯¹åŠ¨è„‰è¡€å‹æ³¢å½¢
    """
    if len(bp_waveform) < window_size:
        # æ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨æ›´å°çš„çª—å£
        window_size = min(1000, len(bp_waveform) // 2)
        if window_size < 100:
            print(f"        è­¦å‘Š: æ•°æ®ç‚¹å¤ªå°‘ ({len(bp_waveform)})ï¼Œæ— æ³•æå–å³°å€¼")
            return bp_waveform, bp_waveform
    
    print(f"        ä½¿ç”¨çª—å£å¤§å° {window_size} ä»ABPæ³¢å½¢ä¸­æå–SBPå’ŒDBP...")
    
    # åˆå§‹åŒ–SBPå’ŒDBPæ•°ç»„
    sbp_values = []
    dbp_values = []
    timestamps = []
    
    # ä½¿ç”¨è¾ƒå°çš„æ­¥é•¿æ¥è·å¾—æ›´ç²¾ç¡®çš„å³°å€¼
    step_size = max(100, window_size // 8)  # 12.5%é‡å ï¼Œæé«˜ç²¾åº¦
    
    # æ»‘åŠ¨çª—å£å¤„ç†
    for i in range(0, len(bp_waveform) - window_size + 1, step_size):
        window_data = bp_waveform.iloc[i:i+window_size]
        
        if len(window_data) < 200:  # çª—å£å¤ªå°ï¼Œè·³è¿‡
            continue
            
        # åœ¨çª—å£å†…å¯»æ‰¾å³°å€¼ï¼ˆSBPï¼‰å’Œè°·å€¼ï¼ˆDBPï¼‰
        window_values = window_data['bp_value'].values
        
        try:
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„å³°å€¼æ£€æµ‹
            # æ‰¾åˆ°çª—å£å†…çš„å±€éƒ¨æœ€å¤§å€¼å’Œæœ€å°å€¼
            from scipy.signal import find_peaks
            
            # å¯»æ‰¾å³°å€¼ï¼ˆSBPï¼‰- å¯»æ‰¾å±€éƒ¨æœ€å¤§å€¼
            peaks, _ = find_peaks(window_values, height=None, distance=min_peak_distance//2)
            if len(peaks) > 0:
                # é€‰æ‹©æœ€é«˜çš„å³°å€¼
                peak_heights = window_values[peaks]
                max_peak_idx = peaks[np.argmax(peak_heights)]
                max_bp = window_values[max_peak_idx]
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å³°å€¼ï¼Œä½¿ç”¨çª—å£å†…çš„æœ€å¤§å€¼
                max_bp = np.max(window_values)
            
            # å¯»æ‰¾è°·å€¼ï¼ˆDBPï¼‰- å¯»æ‰¾å±€éƒ¨æœ€å°å€¼
            valleys, _ = find_peaks(-window_values, height=None, distance=min_peak_distance//2)
            if len(valleys) > 0:
                # é€‰æ‹©æœ€ä½çš„è°·å€¼
                valley_heights = window_values[valleys]
                min_valley_idx = valleys[np.argmin(valley_heights)]
                min_bp = window_values[min_valley_idx]
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è°·å€¼ï¼Œä½¿ç”¨çª—å£å†…çš„æœ€å°å€¼
                min_bp = np.min(window_values)
            
            # è®¡ç®—çª—å£ä¸­å¿ƒæ—¶é—´æˆ³
            center_idx = i + window_size // 2
            if center_idx < len(bp_waveform):
                center_timestamp = bp_waveform.iloc[center_idx]['timestamp']
                
                sbp_values.append(max_bp)
                dbp_values.append(min_bp)
                timestamps.append(center_timestamp)
                
        except Exception as e:
            print(f"        çª—å£ {i} å¤„ç†é”™è¯¯: {e}")
            continue
    
    # åˆ›å»ºSBPå’ŒDBPçš„DataFrame
    if sbp_values:
        sbp_df = pd.DataFrame({
            'timestamp': timestamps,
            'sbp': sbp_values
        })
        dbp_df = pd.DataFrame({
            'timestamp': timestamps,
            'dbp': dbp_values
        })
        
        # éªŒè¯SBPå’ŒDBPçš„åˆç†æ€§
        sbp_mean = np.mean(sbp_values)
        dbp_mean = np.mean(dbp_values)
        if sbp_mean <= dbp_mean:
            print(f"        è­¦å‘Š: SBP ({sbp_mean:.1f}) <= DBP ({dbp_mean:.1f})ï¼Œå¯èƒ½æ£€æµ‹é”™è¯¯")
        
        print(f"        æ£€æµ‹å®Œæˆ: SBP {len(sbp_df):,} ç‚¹ (å‡å€¼: {sbp_mean:.1f}), DBP {len(dbp_df):,} ç‚¹ (å‡å€¼: {dbp_mean:.1f})")
        return sbp_df, dbp_df
    else:
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å³°å€¼ï¼Œè¿”å›åŸå§‹æ•°æ®
        print(f"        è­¦å‘Š: æœªæ£€æµ‹åˆ°å³°å€¼ï¼Œè¿”å›åŸå§‹æ•°æ®")
        return bp_waveform, bp_waveform

def load_calibrated_bp_data(subject_id, experiment_numbers=None):
    """
    åŠ è½½PhysioNet2025_Calibratedæ–‡ä»¶å¤¹ä¸­çš„æ ¡å‡†è¡€å‹æ•°æ®
    """
    calibrated_root = '/root/shared/PhysioNet2025_Calibrated/'
    subject_path = os.path.join(calibrated_root, subject_id)
    
    if not os.path.exists(subject_path):
        print(f"  è­¦å‘Š: PhysioNet2025_Calibratedæ–‡ä»¶å¤¹ä¸­ä¸å­˜åœ¨subject {subject_id}")
        return {}
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå®éªŒç¼–å·ï¼Œè·å–æ‰€æœ‰å¯ç”¨çš„å®éªŒ
    if experiment_numbers is None:
        available_experiments = [d for d in os.listdir(subject_path) 
                               if os.path.isdir(os.path.join(subject_path, d)) and d.isdigit()]
        experiment_numbers = [int(exp) for exp in available_experiments]
        experiment_numbers.sort()
        print(f"    å‘ç°å¯ç”¨å®éªŒ: {experiment_numbers}")
    
    bp_data = {}
    
    for exp_num in experiment_numbers:
        exp_dir = os.path.join(subject_path, str(exp_num))
        bp_file_path = os.path.join(exp_dir, 'Biopac', 'bp.csv')
        
        if os.path.exists(bp_file_path):
            try:
                # è¯»å–æ ¡å‡†åçš„è¡€å‹æ•°æ®
                data = pd.read_csv(bp_file_path)
                print(f"    åŠ è½½å®éªŒ {exp_num} æ ¡å‡†è¡€å‹æ•°æ®: {len(data):,} è¡Œ")
                
                # æ£€æŸ¥æ•°æ®åˆ—ç»“æ„ - æ–°æ•°æ®æ²¡æœ‰åˆ—åï¼Œéœ€è¦æ·»åŠ 
                if len(data.columns) == 2:
                    # æ•°æ®æœ‰ä¸¤åˆ—ä½†æ²¡æœ‰åˆ—åï¼Œæ·»åŠ åˆ—å
                    data.columns = ['timestamp', 'bp_value']
                    print(f"      æ·»åŠ åˆ—å: timestamp, bp_value")
                
                if 'timestamp' in data.columns and 'bp_value' in data.columns:
                    # æ•°æ®ç°åœ¨æ˜¯æ­£ç¡®çš„æ ¼å¼ï¼Œä½†éœ€è¦ä»ABPæ³¢å½¢ä¸­æå–SBPå’ŒDBP
                    print(f"      ä»ABPæ³¢å½¢ä¸­æå–SBPå’ŒDBP...")
                    sbp_df, dbp_df = extract_sbp_dbp_from_waveform(data)
                    
                    bp_data[str(exp_num)] = {
                        'waveform': data,
                        'sbp': sbp_df,
                        'dbp': dbp_df
                    }
                    print(f"      æå–å®Œæˆ: SBP {len(sbp_df):,} ç‚¹, DBP {len(dbp_df):,} ç‚¹")
                else:
                    # å°è¯•æå–SBPå’ŒDBP
                    print(f"      æå–SBPå’ŒDBP...")
                    sbp_df, dbp_df = extract_sbp_dbp_from_waveform(data)
                    
                    # å­˜å‚¨åŸå§‹æ³¢å½¢ã€SBPå’ŒDBPæ•°æ®
                    bp_data[str(exp_num)] = {
                        'waveform': data,
                        'sbp': sbp_df,
                        'dbp': dbp_df
                    }
                    
                    print(f"      æå–å®Œæˆ: SBP {len(sbp_df):,} ç‚¹, DBP {len(dbp_df):,} ç‚¹")
                
            except Exception as e:
                print(f"    é”™è¯¯: è¯»å–å®éªŒ {exp_num} æ ¡å‡†è¡€å‹æ•°æ®å¤±è´¥: {e}")
                continue
        else:
            print(f"    è­¦å‘Š: å®éªŒ {exp_num} æ ¡å‡†è¡€å‹æ–‡ä»¶ä¸å­˜åœ¨: {bp_file_path}")
    
    return bp_data

def interpolate_duplicate_timestamps(df, time_col='timestamp'):
    """
    æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³ - ç”¨äºHUBæ•°æ®ä¿æŒç²¾åº¦
    """
    if df.empty or time_col not in df.columns:
        return df
    
    df = df.copy()
    unique_times = df[time_col].unique()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
    if len(unique_times) == len(df):
        return df
    
    print(f"      æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³...")
    
    new_timestamps = []
    for t in unique_times:
        indices = df[df[time_col] == t].index
        n_points = len(indices)

        if n_points > 1:
            current_idx = np.where(unique_times == t)[0][0]
            if current_idx == len(unique_times) - 1:
                delta = 0.001  # æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ä½¿ç”¨é»˜è®¤å°é—´éš”
            else:
                next_t = unique_times[current_idx + 1]
                delta = (next_t - t) / n_points

            for i, idx in enumerate(indices):
                new_timestamps.append((t + i * delta, idx))
        else:
            new_timestamps.append((t, indices[0]))

    # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—æ—¶é—´æˆ³
    new_timestamps.sort(key=lambda x: x[1])
    df[time_col] = [t for t, _ in new_timestamps]
    
    duplicates_fixed = len(df) - len(unique_times)
    if duplicates_fixed > 0:
        print(f"      ä¿®å¤äº† {duplicates_fixed} ä¸ªé‡å¤æ—¶é—´æˆ³")
    
    return df

def quick_downsample_biopac(df, time_col='timestamp', target_freq=100):
    """å¯¹Biopacé«˜é¢‘æ•°æ®é™é‡‡æ ·"""
    if df.empty or time_col not in df.columns:
        return df
    
    original_len = len(df)
    
    if len(df) > 1:
        time_range = df[time_col].max() - df[time_col].min()
        current_freq = len(df) / time_range if time_range > 0 else 1
        
        print(f"      ä¼°ç®—é¢‘ç‡: {current_freq:.1f}Hz -> ç›®æ ‡: {target_freq}Hz")
        
        if current_freq > target_freq * 1.5:  # åªæœ‰æ˜æ˜¾é«˜é¢‘æ‰é™é‡‡æ ·
            step = max(1, int(current_freq / target_freq))
            result = df.iloc[::step].copy()
            print(f"      é™é‡‡æ ·: {original_len:,} -> {len(result):,} è¡Œ (æ­¥é•¿: {step})")
            return result
        else:
            print(f"      é¢‘ç‡é€‚ä¸­ï¼Œè·³è¿‡é™é‡‡æ ·")
    
    return df

def interpolate_with_reftime(time, data, reftime):
    """
    ä½¿ç”¨æ’å€¼å¯¹é½åˆ°å‚è€ƒæ—¶é—´æˆ³
    """
    if len(time) < 2 or len(data) < 2:
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])
    
    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    time = np.asarray(time, dtype=float)
    reftime = np.asarray(reftime, dtype=float)
    
    # é™åˆ¶æ’å€¼èŒƒå›´åˆ°æ•°æ®å®é™…èŒƒå›´å†…
    min_time, max_time = time.min(), time.max()
    valid_reftime_mask = (reftime >= min_time) & (reftime <= max_time)
    valid_reftime = reftime[valid_reftime_mask]
    
    if len(valid_reftime) == 0:
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])
    
    try:
        # ä½¿ç”¨çº¿æ€§æ’å€¼
        interp_func = interp1d(time, data, axis=0, kind='linear', bounds_error=False, fill_value=np.nan)
        interpolated_data = interp_func(valid_reftime)
        
        # åˆ›å»ºå®Œæ•´ç»“æœDataFrame
        if interpolated_data.ndim == 1:
            interpolated_data = interpolated_data.reshape(-1, 1)
        
        # åˆ›å»ºä¸å‚è€ƒæ—¶é—´é•¿åº¦ç›¸åŒçš„ç»“æœ
        full_result = np.full((len(reftime), interpolated_data.shape[1]), np.nan)
        full_result[valid_reftime_mask] = interpolated_data
        
        result_df = pd.DataFrame(full_result, columns=data.columns if hasattr(data, 'columns') else [f'col_{i}' for i in range(full_result.shape[1])])
        result_df['timestamp'] = reftime
        
        return result_df
    except Exception as e:
        print(f"        æ’å€¼é”™è¯¯: {e}")
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])

def process_biopac_file(file_path, target_freq=100):
    """å¤„ç†Biopacæ–‡ä»¶ - å…ˆé™é‡‡æ ·å†æ’å€¼å¤„ç†é‡å¤"""
    try:
        file_name = os.path.basename(file_path)
        try:
            data = pd.read_csv(file_path)
        except UnicodeDecodeError:
            data = pd.read_csv(file_path, encoding='latin1')
        original_rows = len(data)
        
        if data.empty:
            return pd.DataFrame()
        
        print(f"    Biopac {file_name} ({original_rows:,} è¡Œ)")
        
        # 1. å…ˆé™é‡‡æ ·åˆ°ç›®æ ‡é¢‘ç‡ï¼ˆä¸å»é‡ï¼‰
        data = quick_downsample_biopac(data, target_freq=target_freq)
        
        # 2. å†ç”¨æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³
        data = interpolate_duplicate_timestamps(data)
        
        # 3. æ’åº
        if data is not None and not data.empty and 'timestamp' in data.columns:
            data = data.sort_values('timestamp').reset_index(drop=True)
        
        final_rows = len(data) if data is not None else 0
        compression_ratio = original_rows / final_rows if final_rows > 0 else 1
        print(f"      æœ€ç»ˆ: {final_rows:,} è¡Œ (å‹ç¼©æ¯”: {compression_ratio:.1f}:1)")
        
        return data if data is not None else pd.DataFrame()
        
    except Exception as e:
        print(f"      é”™è¯¯: {e}")
        return pd.DataFrame()

def process_hub_file(file_path):
    """å¤„ç†HUBæ–‡ä»¶ - æ’å€¼ç­–ç•¥"""
    try:
        file_name = os.path.basename(file_path)
        try:
            data = pd.read_csv(file_path)
        except UnicodeDecodeError:
            data = pd.read_csv(file_path, encoding='latin1')
        original_rows = len(data)
        
        if data.empty:
            return pd.DataFrame()
        
        print(f"    HUB {file_name} ({original_rows:,} è¡Œ)")
        
        # 1. æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³ï¼ˆä¿æŒHUBæ•°æ®ç²¾åº¦ï¼‰
        processed_data = interpolate_duplicate_timestamps(data)
        
        # 2. æ’åº
        if processed_data is not None and not processed_data.empty and 'timestamp' in processed_data.columns:
            processed_data = processed_data.sort_values('timestamp').reset_index(drop=True)
        
        final_rows = len(processed_data) if processed_data is not None else 0
        print(f"      æœ€ç»ˆ: {final_rows:,} è¡Œ")
        
        return processed_data if processed_data is not None else pd.DataFrame()
        
    except Exception as e:
        print(f"      é”™è¯¯: {e}")
        return pd.DataFrame()

def load_experiment_smart(experiment_path, calibrated_bp_data, target_freq=100):
    """æ™ºèƒ½åŠ è½½å•ä¸ªå®éªŒï¼ŒåŒ…å«æ ¡å‡†è¡€å‹æ•°æ®å’ŒåŸå§‹biopacæ•°æ®"""
    experiment_name = os.path.basename(experiment_path)
    print(f"\n{'='*50}")
    print(f"å¤„ç†å®éªŒ {experiment_name}")
    print(f"{'='*50}")
    
    result = {'biopac': {}, 'hub': {}}
    
    # å¤„ç†æ ¡å‡†è¡€å‹æ•°æ®
    if experiment_name in calibrated_bp_data:
        bp_data_dict = calibrated_bp_data[experiment_name]
        print(f"\nè¡€å‹æ•°æ® (PhysioNet2025_Calibrated):")
        print(f"      åŸå§‹æ³¢å½¢: {len(bp_data_dict['waveform']):,} è¡Œ")
        print(f"      SBP: {len(bp_data_dict['sbp']):,} ç‚¹")
        print(f"      DBP: {len(bp_data_dict['dbp']):,} ç‚¹")
        
        # å°†è¡€å‹æ•°æ®å­˜å‚¨åˆ°biopacéƒ¨åˆ†
        result['biopac']['waveform'] = bp_data_dict['waveform']
        result['biopac']['sbp'] = bp_data_dict['sbp']
        result['biopac']['dbp'] = bp_data_dict['dbp']
    
    # å¤„ç†Biopacæ•°æ® (é«˜é¢‘ï¼Œå…ˆé™é‡‡æ ·å†æ’å€¼å¤„ç†é‡å¤)
    biopac_path = os.path.join(experiment_path, 'Biopac')
    if os.path.isdir(biopac_path):
        biopac_files = [f for f in os.listdir(biopac_path) if f.endswith('.csv')]
        print(f"\nBiopacæ–‡ä»¶ ({len(biopac_files)} ä¸ª) - é™é‡‡æ ·+æ’å€¼ç­–ç•¥:")
        
        for file in biopac_files:
            file_path = os.path.join(biopac_path, file)
            data = process_biopac_file(file_path, target_freq)
            if data is not None and not data.empty:
                key = file.split('-')[0] if '-' in file else file.replace('.csv', '')
                result['biopac'][key] = data
    
    # å¤„ç†HUBæ•°æ® (ä½é¢‘ï¼Œç”¨æ’å€¼)
    hub_path = os.path.join(experiment_path, 'HUB')
    if os.path.isdir(hub_path):
        hub_files = [f for f in os.listdir(hub_path) if f.endswith('.csv')]
        print(f"\nHUBæ–‡ä»¶ ({len(hub_files)} ä¸ª) - ä½¿ç”¨æ’å€¼ç­–ç•¥:")
        
        for file in hub_files:
            file_path = os.path.join(hub_path, file)
            data = process_hub_file(file_path)
            if not data.empty:
                key = file.replace('.csv', '')
                result['hub'][key] = data
    
    # ç»Ÿè®¡ç»“æœ
    biopac_count = len(result['biopac'])
    hub_count = len(result['hub'])
    print(f"\nå®éªŒ {experiment_name} å®Œæˆ: {biopac_count} ä¸ªBiopacæ–‡ä»¶, {hub_count} ä¸ªHUBæ–‡ä»¶")
    
    return result

def align_data_with_interpolation(data_dict, calibrated_bp_data, output_dir, csv_dir, subject):
    """ä½¿ç”¨æ’å€¼è¿›è¡Œç²¾ç¡®æ•°æ®å¯¹é½ï¼ŒåŒ…å«æ ¡å‡†è¡€å‹æ•°æ®"""
    aligned_data = {}
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\n{'='*50}")
    print("æ’å€¼å¯¹é½é˜¶æ®µ (åŒ…å«movedè¡€å‹æ•°æ®)")
    print(f"{'='*50}")
    
    for experiment_name, experiment_data in data_dict.items():
        try:
            print(f"\nå¯¹é½å®éªŒ {experiment_name}...")
            
            # æŸ¥æ‰¾å‚è€ƒæ—¶é—´åºåˆ—ï¼ˆä¼˜å…ˆä½¿ç”¨sensor2ï¼‰
            ref_data = None
            ref_name = ""
            
            try:
                hub_sensor2 = experiment_data['hub'].get('sensor2', pd.DataFrame())
                if not hub_sensor2.empty and 'timestamp' in hub_sensor2.columns:
                    ref_data = hub_sensor2
                    ref_name = "sensor2"
            except KeyError:
                print(f"  è­¦å‘Š: sensor2 ç¼ºå°‘ timestamp åˆ—")
            
            if ref_data is None:
                min_len = float('inf')
                for data_type in ['hub', 'biopac']:
                    for key, data in experiment_data[data_type].items():
                        if isinstance(data, pd.DataFrame) and not data.empty and 'timestamp' in data.columns:
                            if len(data) < min_len:
                                min_len = len(data)
                                ref_data = data
                                ref_name = f"{data_type}_{key}"
            
            if ref_data is None or ref_data.empty or 'timestamp' not in ref_data.columns:
                print(f"  è­¦å‘Š: experiment {experiment_name} æ— æœ‰æ•ˆå‚è€ƒæ•°æ®æˆ–ç¼ºå°‘ timestamp åˆ—ï¼Œè·³è¿‡")
                continue
            
            try:
                ref_timestamps = ref_data['timestamp'].values
            except KeyError:
                print(f"  é”™è¯¯: å‚è€ƒæ•°æ®ç¼ºå°‘ timestamp åˆ—ï¼Œè·³è¿‡ experiment {experiment_name}")
                continue
        
            print(f"  ä½¿ç”¨ {ref_name} ä½œä¸ºå‚è€ƒ ({len(ref_data):,} è¡Œ)")
            aligned_experiment = {'biopac': {}, 'hub': {}}
            
            # æ’å€¼å¯¹é½æ‰€æœ‰æ•°æ®
            for data_type in ['biopac', 'hub']:
                for key, data in experiment_data[data_type].items():
                    if isinstance(data, pd.DataFrame) and not data.empty and 'timestamp' in data.columns:
                        print(f"    å¯¹é½ {data_type}_{key}...")
                        
                        # æå–éœ€è¦æ’å€¼çš„åˆ—ï¼ˆé™¤äº†timestampï¼‰
                        data_columns = [col for col in data.columns if col != 'timestamp']
                        if data_columns:
                            # ä½¿ç”¨æ’å€¼å¯¹é½
                            interpolated_data = interpolate_with_reftime(
                                data['timestamp'].values,
                                data[data_columns].values,
                                ref_timestamps
                            )
                            
                            if not interpolated_data.empty:
                                # é‡æ–°è®¾ç½®åˆ—å
                                interpolated_data.columns = data_columns + ['timestamp']
                                aligned_experiment[data_type][key] = interpolated_data
                                print(f"      å¯¹é½å®Œæˆ: {len(data):,} -> {len(interpolated_data):,} è¡Œ")
                            else:
                                print(f"      å¯¹é½å¤±è´¥ï¼Œè·³è¿‡")
            
            aligned_data[experiment_name] = aligned_experiment
            
            # ä¿å­˜pklæ ¼å¼
            pkl_path = os.path.join(output_dir, f'experiment_{experiment_name}_aligned.pkl')
            with open(pkl_path, 'wb') as f:
                pickle.dump({experiment_name: aligned_experiment}, f)
            print(f"  ä¿å­˜PKL: {pkl_path}")
            
            # ä¿å­˜npyæ ¼å¼
            npy_path = os.path.join(output_dir, f'experiment_{experiment_name}_aligned.npy')
            npy_data = {
                experiment_name: {
                    data_type: {
                        key: df.to_dict() if isinstance(df, pd.DataFrame) else df
                        for key, df in type_data.items()
                    }
                    for data_type, type_data in aligned_experiment.items()
                }
            }
            np.save(npy_path, npy_data, allow_pickle=True)
            file_size = os.path.getsize(npy_path) / (1024 * 1024)  # MB
            print(f"  ä¿å­˜NPY (å•æ–‡ä»¶): {npy_path}, å¤§å°: {file_size:.2f} MB")
        except Exception as e:
            print(f'å¯¹é½ experiment {experiment_name} å¤±è´¥: {e}')
            continue
    
    # CSV ç”Ÿæˆå¾ªç¯ - ä¸ºæ¯ä¸ª experiment ç”Ÿæˆ
    for exp_name, exp_data in aligned_data.items():
        # æ•´åˆBiopacæ•°æ®ä¸ºå•æ–‡ä»¶CSV
        biopac_data = exp_data['biopac']
        if biopac_data:
            # å®‰å…¨åœ°è·å–å‚è€ƒæ—¶é—´æˆ³
            ref_timestamps = None
            hub_sensor2 = exp_data['hub'].get('sensor2', pd.DataFrame())
            
            # æ£€æŸ¥hub_sensor2æ˜¯å¦æœ‰æ•ˆ
            if not hub_sensor2.empty and 'timestamp' in hub_sensor2.columns:
                ref_timestamps = hub_sensor2['timestamp'].values
                print(f"      ä½¿ç”¨HUB sensor2ä½œä¸ºå‚è€ƒæ—¶é—´æˆ³: {len(ref_timestamps):,} ç‚¹")
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„hub_sensor2ï¼Œå°è¯•ä½¿ç”¨biopacæ•°æ®
            if ref_timestamps is None or len(ref_timestamps) == 0:
                for key, df in biopac_data.items():
                    if isinstance(df, pd.DataFrame) and not df.empty and 'timestamp' in df.columns:
                        ref_timestamps = df['timestamp'].values
                        print(f"      ä½¿ç”¨Biopac {key}ä½œä¸ºå‚è€ƒæ—¶é—´æˆ³: {len(ref_timestamps):,} ç‚¹")
                        break
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´æˆ³ï¼Œè·³è¿‡è¿™ä¸ªå®éªŒ
            if ref_timestamps is None or len(ref_timestamps) == 0:
                print(f"      è­¦å‘Š: æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´æˆ³å‚è€ƒï¼Œè·³è¿‡å®éªŒ {exp_name}")
                continue
            
            merged_biopac = pd.DataFrame({'timestamp': ref_timestamps})
            
            # å¤„ç†movedè¡€å‹æ•°æ®
            if 'sbp' in biopac_data and 'dbp' in biopac_data:
                # å¯¹é½SBPå’ŒDBPæ•°æ®åˆ°å‚è€ƒæ—¶é—´æˆ³
                sbp_data = biopac_data['sbp']
                dbp_data = biopac_data['dbp']
                
                # ä½¿ç”¨æ’å€¼å¯¹é½SBPå’ŒDBP
                if not sbp_data.empty and not dbp_data.empty:
                    aligned_sbp = interpolate_with_reftime(
                        sbp_data['timestamp'].values,
                        sbp_data[['sbp']].values,
                        ref_timestamps
                    )
                    aligned_dbp = interpolate_with_reftime(
                        dbp_data['timestamp'].values,
                        dbp_data[['dbp']].values,
                        ref_timestamps
                    )
                    
                    if not aligned_sbp.empty and not aligned_dbp.empty:
                        merged_biopac['moved_sbp'] = aligned_sbp.iloc[:, 0]  # ç¬¬ä¸€åˆ—æ˜¯sbpå€¼
                        merged_biopac['moved_dbp'] = aligned_dbp.iloc[:, 0]  # ç¬¬ä¸€åˆ—æ˜¯dbpå€¼
                        print(f"      å¯¹é½è¡€å‹æ•°æ®: SBP {len(aligned_sbp):,} ç‚¹, DBP {len(aligned_dbp):,} ç‚¹")
            
            # å¤„ç†å…¶ä»–biopacæ•°æ®ï¼ˆåŸå§‹biopacæ•°æ®ï¼‰
            for key, df in biopac_data.items():
                if key not in ['sbp', 'dbp', 'waveform'] and isinstance(df, pd.DataFrame) and not df.empty:
                    # å¯¹äºåŸå§‹biopacæ•°æ®ï¼Œéœ€è¦å…ˆå¯¹é½åˆ°å‚è€ƒæ—¶é—´æˆ³
                    if 'timestamp' in df.columns:
                        # æå–éœ€è¦æ’å€¼çš„åˆ—ï¼ˆé™¤äº†timestampï¼‰
                        data_columns = [col for col in df.columns if col != 'timestamp']
                        if data_columns:
                            # ä½¿ç”¨æ’å€¼å¯¹é½åŸå§‹biopacæ•°æ®
                            aligned_biopac = interpolate_with_reftime(
                                df['timestamp'].values,
                                df[data_columns].values,
                                ref_timestamps
                            )
                            
                            if not aligned_biopac.empty:
                                # å°†å¯¹é½åçš„æ•°æ®æ·»åŠ åˆ°åˆå¹¶çš„DataFrameä¸­
                                for i, col in enumerate(data_columns):
                                    merged_biopac[f'biopac_{key}_{col}'] = aligned_biopac.iloc[:, i]
                                print(f"      å¯¹é½åŸå§‹biopacæ•°æ® {key}: {len(df):,} -> {len(aligned_biopac):,} è¡Œ")
                    else:
                        # å¦‚æœæ²¡æœ‰timestampåˆ—ï¼Œç›´æ¥åˆå¹¶
                        merged_biopac = merged_biopac.merge(df, on='timestamp', how='left')
            
            # å¡«å……ç¼ºå¤±å€¼
            merged_biopac = merged_biopac.fillna(method='ffill').fillna(method='bfill')
            biopac_csv_path = os.path.join(csv_dir, f'{subject}_{exp_name}_calibrated_bp_aligned.csv')
            merged_biopac.to_csv(biopac_csv_path, index=False)
            print(f'  ä¿å­˜æ•´åˆæ ¡å‡†è¡€å‹CSV: {biopac_csv_path}')
            
            # ä¿å­˜å•ç‹¬çš„æ ¡å‡†è¡€å‹æ•°æ®CSVï¼Œæ–¹ä¾¿äººå·¥æ£€æŸ¥
            if 'sbp' in biopac_data and 'dbp' in biopac_data:
                bp_calibrated_df = pd.DataFrame({
                    'timestamp': ref_timestamps,
                    'calibrated_sbp': merged_biopac['moved_sbp'],
                    'calibrated_dbp': merged_biopac['moved_dbp']
                })
                bp_calibrated_csv_path = os.path.join(csv_dir, f'{subject}_{exp_name}_biopac_calibrated_aligned.csv')
                bp_calibrated_df.to_csv(bp_calibrated_csv_path, index=False)
                print(f'  ä¿å­˜å•ç‹¬æ ¡å‡†è¡€å‹CSV: {bp_calibrated_csv_path}')
        
        # ä¿å­˜HUBæ•°æ®ä¸ºç‹¬ç«‹CSVæ–‡ä»¶
        for key, df in exp_data['hub'].items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                columns = ['timestamp'] + [col for col in df.columns if col != 'timestamp']
                df_reordered = df[columns]
                hub_csv_path = os.path.join(csv_dir, f'{subject}_{exp_name}_hub_{key}_aligned.csv')
                df_reordered.to_csv(hub_csv_path, index=False)
                print(f'  ä¿å­˜HUB CSV: {hub_csv_path}')
    
    return aligned_data

def process_subject(date_folder, subject):
    dataset_root = '/root/shared/PhysioNet2025/'  # æ·»åŠ å¸¸é‡å®šä¹‰
    MAX_EXPERIMENTS = None  # æ·»åŠ å¸¸é‡å®šä¹‰
    TARGET_FREQ = 100  # æ·»åŠ å¸¸é‡å®šä¹‰
    date_path = os.path.join(dataset_root, date_folder)
    subject_path = os.path.join(date_path, subject)
    output_dir = os.path.join('/root/autodl-tmp/', subject, 'output_calibrated')
    csv_dir = os.path.join('/root/autodl-tmp/', subject, 'csv_output_calibrated')
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
    if os.path.exists(output_dir) and os.path.exists(csv_dir):
        # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„è¾“å‡ºæ–‡ä»¶
        pkl_files = [f for f in os.listdir(output_dir) if f.endswith('.pkl')]
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
        
        if pkl_files and csv_files:
            print(f"â­ï¸  {subject}: å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡ (PKL: {len(pkl_files)}ä¸ª, CSV: {len(csv_files)}ä¸ª)")
            return "SKIPPED_ALREADY_PROCESSED"
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(csv_dir, exist_ok=True)
    
    print(f"\nå¤„ç†subject: {subject} åœ¨ {date_folder} (ä½¿ç”¨æ ¡å‡†è¡€å‹æ•°æ®)")
    
    # åŠ è½½æ ¡å‡†è¡€å‹æ•°æ®
    print(f"åŠ è½½PhysioNet2025_Calibratedæ–‡ä»¶å¤¹ä¸­çš„æ ¡å‡†è¡€å‹æ•°æ®...")
    calibrated_bp_data = load_calibrated_bp_data(subject)
    
    if not calibrated_bp_data:
        print(f"  è­¦å‘Š: subject {subject} åœ¨PhysioNet2025_Calibratedæ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°æ ¡å‡†è¡€å‹æ•°æ®")
        return None
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„å®éªŒ
    all_folders = os.listdir(subject_path)
    experiment_folders = [f for f in all_folders if f.isdigit() and os.path.isdir(os.path.join(subject_path, f))]
    experiment_folders.sort(key=lambda x: int(x))
    
    # åªå¤„ç†æœ‰æ ¡å‡†æ•°æ®çš„å®éªŒ
    available_calibrated_experiments = list(calibrated_bp_data.keys())
    experiment_folders = [f for f in experiment_folders if f in available_calibrated_experiments]
    
    if MAX_EXPERIMENTS:
        experiment_folders = experiment_folders[:MAX_EXPERIMENTS]
    
    print(f"å‘ç°å¯ç”¨å®éªŒ: {experiment_folders}")
    print(f"æœ‰æ ¡å‡†æ•°æ®çš„å®éªŒ: {available_calibrated_experiments}")
    
    all_data = {}
    load_start = time.time()
    for experiment in experiment_folders:
        experiment_path = os.path.join(subject_path, experiment)
        experiment_data = load_experiment_smart(experiment_path, calibrated_bp_data, TARGET_FREQ)
        all_data[experiment] = experiment_data
    load_time = time.time() - load_start
    
    align_start = time.time()
    aligned_data = align_data_with_interpolation(all_data, calibrated_bp_data, output_dir, csv_dir, subject)
    align_time = time.time() - align_start
    
    # ç»Ÿè®¡
    total_time = time.time() - load_start  # ä½¿ç”¨ load_start ä½œä¸ºèµ·ç‚¹
    print(f"\n{'='*60}")
    print("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æˆåŠŸå¤„ç†å®éªŒ: {len(aligned_data)}")
    print(f"æ•°æ®åŠ è½½è€—æ—¶: {load_time:.1f} ç§’")
    print(f"æ•°æ®å¯¹é½è€—æ—¶: {align_time:.1f} ç§’")
    print(f"æ€»å¤„ç†è€—æ—¶: {total_time:.1f} ç§’")
    if len(experiment_folders) > 0:
        print(f"å¹³å‡æ¯ä¸ªå®éªŒ: {total_time/len(experiment_folders):.1f} ç§’")
    
    for exp_name, exp_data in aligned_data.items():
        total_biopac = len(exp_data['biopac'])
        total_hub = len(exp_data['hub'])
        print(f"å®éªŒ {exp_name}: {total_biopac} ä¸ªBiopac + {total_hub} ä¸ªHUBæ–‡ä»¶")
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"PKLæ ¼å¼: {output_dir}/experiment_*_aligned.pkl")
    print(f"NPYæ ¼å¼: {output_dir}/experiment_*_aligned.npy")
    print(f"CSVæ ¼å¼: {csv_dir}/*_calibrated_bp_aligned.csv")
    return "SUCCESS"  # æˆåŠŸè¿”å›çŠ¶æ€

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    dataset_root = '/root/shared/PhysioNet2025/'
    
    # è®¾ç½®å‚æ•°
    TARGET_FREQ = 100  # ç›®æ ‡é¢‘ç‡
    MAX_EXPERIMENTS = None  # Noneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å®éªŒ
    
    # è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹
    date_folders = [f for f in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, f)) and f.startswith('20')]
    date_folders.sort()
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„subjects
    all_subjects_to_process = []
    for date_folder in date_folders:
        date_path = os.path.join(dataset_root, date_folder)
        all_subject_folders = [f for f in os.listdir(date_path) if os.path.isdir(os.path.join(date_path, f)) and f.startswith('00')]
        all_subject_folders.sort()
        
        for subject in all_subject_folders:
            # æ£€æŸ¥PhysioNet2025_Calibratedæ–‡ä»¶å¤¹ä¸­æ˜¯å¦å­˜åœ¨è¯¥subjectçš„æ•°æ®
            calibrated_subject_path = os.path.join('/root/shared/PhysioNet2025_Calibrated/', subject)
            if os.path.exists(calibrated_subject_path):
                # æ£€æŸ¥æ˜¯å¦æœ‰æ ¡å‡†è¡€å‹æ•°æ®
                available_experiments = [d for d in os.listdir(calibrated_subject_path) 
                                       if os.path.isdir(os.path.join(calibrated_subject_path, d)) and d.isdigit()]
                if available_experiments:
                    all_subjects_to_process.append((date_folder, subject))
    
    print(f"æ‰¾åˆ° {len(all_subjects_to_process)} ä¸ªsubjectséœ€è¦å¤„ç†")
    print("Subjects:", [f"{date}/{subject}" for date, subject in all_subjects_to_process])
    
    # ä½¿ç”¨8æ ¸å¹¶è¡Œå¤„ç†
    with multiprocessing.Pool(processes=8) as pool:
        results = pool.starmap(process_subject, all_subjects_to_process)
    
    # ç»Ÿè®¡æˆåŠŸã€å¤±è´¥å’Œè·³è¿‡çš„æ•°é‡
    successful = [r for r in results if r == "SUCCESS"]
    skipped = [r for r in results if r == "SKIPPED_ALREADY_PROCESSED"]
    failed = [r for r in results if r not in ["SUCCESS", "SKIPPED_ALREADY_PROCESSED"]]
    
    print(f"\n{'='*60}")
    print("ğŸ“Š å¹¶è¡Œå¤„ç†å®Œæˆç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æ€»subjectsæ•°é‡: {len(all_subjects_to_process)}")
    print(f"æˆåŠŸå¤„ç†: {len(successful)}")
    print(f"è·³è¿‡å·²å¤„ç†: {len(skipped)}")
    print(f"å¤„ç†å¤±è´¥: {len(failed)}")
    
    if failed:
        print(f"\nå¤±è´¥çš„subjects:")
        for error in failed:
            print(f"  - {error}")
    
    if skipped:
        print(f"\nè·³è¿‡çš„subjects (å·²å¤„ç†å®Œæˆ):")
        print(f"  å…±è·³è¿‡ {len(skipped)} ä¸ªsubjects")
    
    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    print(f"\n{'='*60}")
    print("ğŸ“Š æ€»å¤„ç†å®Œæˆç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æ€»å¤„ç†è€—æ—¶: {total_time:.1f} ç§’")
    print(f"ä½¿ç”¨8æ ¸å¹¶è¡Œå¤„ç†ï¼Œå¤„ç†æ‰€æœ‰æœ‰æ ¡å‡†æ•°æ®çš„å®éªŒ")
    print(f"ä½¿ç”¨PhysioNet2025_Calibratedæ–‡ä»¶å¤¹ä¸­çš„æ ¡å‡†è¡€å‹æ•°æ®")

if __name__ == "__main__":
    main()
