#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PTTä¸è¡€å‹ç›¸å…³æ€§åˆ†æ
åŸºäºå¸ˆå…„å»ºè®®ï¼šä½¿ç”¨åˆç†åŒºé—´çš„PTTæ•°æ®åˆ†æä¸è¡€å‹çš„ç›¸å…³æ€§
"""

import os
import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PTTBloodPressureAnalyzer:
    """PTTä¸è¡€å‹ç›¸å…³æ€§åˆ†æå™¨"""
    
    def __init__(self, output_dir="ptt_bp_analysis"):
        self.output_dir = output_dir
        self.ptt_output_dir = "ptt_output2"  # çª—å£åŒ–PTTæ•°æ®ç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¡€å‹æŒ‡æ ‡æ˜ å°„
        self.bp_indicators = {
            'systolic_bp': 'æ”¶ç¼©å‹',
            'diastolic_bp': 'èˆ’å¼ å‹', 
            'mean_bp': 'å¹³å‡åŠ¨è„‰å‹',
            'bp': 'è¿ç»­è¡€å‹'
        }
        
        # PTTä¼ æ„Ÿå™¨ç»„åˆ
        self.ptt_combinations = [
            'sensor2-sensor3', 'sensor2-sensor4', 'sensor2-sensor5',
            'sensor3-sensor4', 'sensor3-sensor5', 'sensor4-sensor5'
        ]
        
        print("ğŸ”¬ PTTä¸è¡€å‹ç›¸å…³æ€§åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ åˆ†æç»“æœå°†ä¿å­˜åˆ°: {self.output_dir}")
    
    def load_ground_truth_bp(self, exp_id):
        """åŠ è½½è¡€å‹çœŸæ ‡æ•°æ®ï¼ˆä»CSVæ–‡ä»¶ï¼‰"""
        try:
            # åŠ è½½CSVæ–‡ä»¶
            csv_file = f"output/csv_output/{exp_id}_biopac_aligned.csv"
            if not os.path.exists(csv_file):
                print(f"âŒ è¡€å‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
                return None
            
            # è¯»å–å®Œæ•´è¡€å‹æ•°æ®
            df = pd.read_csv(csv_file)
            print(f"âœ… åŠ è½½è¡€å‹æ•°æ®: {len(df)}æ¡è®°å½•")
            print(f"ğŸ“Š è¡€å‹æŒ‡æ ‡: {[col for col in df.columns if col != 'timestamp']}")
            
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½è¡€å‹æ•°æ®å¤±è´¥: {e}")
            return None
    
    def load_ptt_data(self, exp_id):
        """åŠ è½½æœ‰æ•ˆçª—å£çš„PTTæ•°æ®"""
        try:
            # åŠ è½½çª—å£éªŒè¯æ•°æ®
            window_file = f"{self.ptt_output_dir}/exp_{exp_id}/window_validation_exp_{exp_id}.csv"
            ptt_file = f"{self.ptt_output_dir}/exp_{exp_id}/ptt_windowed_exp_{exp_id}.csv"
            
            if not (os.path.exists(window_file) and os.path.exists(ptt_file)):
                print(f"âŒ PTTæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: exp_{exp_id}")
                return None
            
            # åŠ è½½çª—å£éªŒè¯ä¿¡æ¯
            window_df = pd.read_csv(window_file)
            # ç­›é€‰æœ‰æ•ˆçª—å£ï¼ˆæ—¶é¢‘åŸŸå¿ƒç‡è¯¯å·®åˆç†ï¼‰
            valid_windows = window_df[
                (window_df['is_valid'] == True) & 
                (window_df['hr_diff_bpm'].abs() <= 5)  # å¿ƒç‡è¯¯å·®â‰¤5BPM
            ]
            
            # åŠ è½½PTTæ•°æ®
            ptt_df = pd.read_csv(ptt_file)
            
            # åªä¿ç•™æœ‰æ•ˆçª—å£çš„PTTæ•°æ®
            valid_ptt = ptt_df[ptt_df['window_id'].isin(valid_windows['window_id'])]
            
            print(f"ğŸ“Š å®éªŒ{exp_id}: æ€»çª—å£{len(window_df)}, æœ‰æ•ˆçª—å£{len(valid_windows)}, æœ‰æ•ˆPTTæ•°æ®{len(valid_ptt)}")
            
            return {
                'window_info': valid_windows,
                'ptt_data': valid_ptt
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½PTTæ•°æ®å¤±è´¥: {e}")
            return None
    
    def synchronize_data(self, ptt_data, bp_data, exp_id):
        """æ—¶é—´åŒæ­¥PTTå’Œè¡€å‹æ•°æ®"""
        synchronized_data = []
        
        for _, ptt_row in ptt_data['ptt_data'].iterrows():
            # PTTæ•°æ®çš„æ—¶é—´ä¿¡æ¯ï¼ˆä¿®æ­£åˆ—åï¼‰
            start_time = ptt_row['window_start_s']
            end_time = ptt_row['window_end_s']
            window_center = (start_time + end_time) / 2
            
            # è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆå‡è®¾è¡€å‹æ•°æ®çš„timestampæ˜¯ç»å¯¹æ—¶é—´æˆ³ï¼‰
            # éœ€è¦æ‰¾åˆ°è¡€å‹æ•°æ®æ—¶é—´æˆ³çš„èµ·å§‹ç‚¹
            bp_start_time = bp_data['timestamp'].iloc[0]
            start_timestamp = bp_start_time + start_time
            end_timestamp = bp_start_time + end_time
            
            # æ‰¾åˆ°æ—¶é—´çª—å£å†…çš„è¡€å‹æ•°æ®
            time_mask = (bp_data['timestamp'] >= start_timestamp) & (bp_data['timestamp'] <= end_timestamp)
            window_bp = bp_data[time_mask]
            
            if len(window_bp) == 0:
                continue  # è·³è¿‡æ²¡æœ‰è¡€å‹æ•°æ®çš„çª—å£
            
            # è®¡ç®—çª—å£å†…è¡€å‹çš„ç»Ÿè®¡é‡
            bp_values = {}
            for bp_col in ['systolic_bp', 'diastolic_bp', 'mean_bp', 'bp']:
                if bp_col in bp_data.columns:
                    bp_values[f'{bp_col}_mean'] = window_bp[bp_col].mean()
                    bp_values[f'{bp_col}_std'] = window_bp[bp_col].std()
                    bp_values[f'{bp_col}_min'] = window_bp[bp_col].min()
                    bp_values[f'{bp_col}_max'] = window_bp[bp_col].max()
                    bp_values[f'{bp_col}_count'] = len(window_bp)
            
            # æ„å»ºåŒæ­¥æ•°æ®è¡Œ
            sync_row = {
                'exp_id': exp_id,
                'window_id': ptt_row['window_id'],
                'start_time': start_time,
                'end_time': end_time,
                'window_center': window_center,
                'sensor_pair': ptt_row['sensor_pair'],
                'ptt_ms': ptt_row['ptt_ms'],
                **bp_values
            }
            
            synchronized_data.append(sync_row)
        
        sync_df = pd.DataFrame(synchronized_data)
        print(f"ğŸ“Š åŒæ­¥å®Œæˆ: {len(sync_df)}ä¸ªæœ‰æ•ˆçª—å£")
        
        return sync_df
    
    def calculate_correlations(self, sync_df):
        """è®¡ç®—PTTä¸è¡€å‹çš„ç›¸å…³æ€§"""
        correlations = {}
        
        # è¡€å‹æŒ‡æ ‡
        bp_metrics = [col for col in sync_df.columns if any(bp in col for bp in ['systolic_bp', 'diastolic_bp', 'mean_bp', 'bp']) and col != 'bp_start_time']
        
        # è·å–æ‰€æœ‰ä¼ æ„Ÿå™¨å¯¹
        sensor_pairs = sync_df['sensor_pair'].unique()
        
        print(f"\nğŸ“Š è®¡ç®—ç›¸å…³æ€§ï¼š{len(sensor_pairs)}ä¸ªä¼ æ„Ÿå™¨å¯¹ Ã— {len(bp_metrics)}ä¸ªè¡€å‹æŒ‡æ ‡")
        
        for sensor_pair in sensor_pairs:
            correlations[sensor_pair] = {}
            
            # æå–è¯¥ä¼ æ„Ÿå™¨å¯¹çš„æ•°æ®
            pair_data = sync_df[sync_df['sensor_pair'] == sensor_pair]
            
            if len(pair_data) < 10:  # è‡³å°‘10ä¸ªæ•°æ®ç‚¹
                continue
            
            for bp_col in bp_metrics:
                # æå–æœ‰æ•ˆæ•°æ®
                mask = ~(pair_data['ptt_ms'].isna() | pair_data[bp_col].isna())
                if mask.sum() < 10:
                    continue
                
                ptt_vals = pair_data.loc[mask, 'ptt_ms']
                bp_vals = pair_data.loc[mask, bp_col]
                
                # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
                try:
                    corr_coef, p_value = stats.pearsonr(ptt_vals, bp_vals)
                    
                    correlations[sensor_pair][bp_col] = {
                        'correlation': corr_coef,
                        'p_value': p_value,
                        'n_samples': len(ptt_vals),
                        'significant': p_value < 0.05
                    }
                except Exception as e:
                    print(f"âš ï¸ è®¡ç®—ç›¸å…³æ€§å¤±è´¥ {sensor_pair}-{bp_col}: {e}")
                    continue
        
        return correlations
    
    def create_correlation_heatmap(self, correlations, title_suffix=""):
        """åˆ›å»ºç›¸å…³æ€§çƒ­å›¾"""
        # å‡†å¤‡æ•°æ®
        sensor_pairs = list(correlations.keys())
        bp_cols = set()
        for pair_data in correlations.values():
            bp_cols.update(pair_data.keys())
        bp_cols = sorted(list(bp_cols))
        
        if len(sensor_pairs) == 0 or len(bp_cols) == 0:
            print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„ç›¸å…³æ€§æ•°æ®æ¥åˆ›å»ºçƒ­å›¾")
            return None
        
        # åˆ›å»ºç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = np.full((len(sensor_pairs), len(bp_cols)), np.nan)
        p_matrix = np.full((len(sensor_pairs), len(bp_cols)), np.nan)
        
        for i, sensor_pair in enumerate(sensor_pairs):
            for j, bp_col in enumerate(bp_cols):
                if bp_col in correlations[sensor_pair]:
                    corr_matrix[i, j] = correlations[sensor_pair][bp_col]['correlation']
                    p_matrix[i, j] = correlations[sensor_pair][bp_col]['p_value']
        
        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # ç»˜åˆ¶çƒ­å›¾
        mask = np.isnan(corr_matrix)
        sns.heatmap(corr_matrix, 
                    xticklabels=[self._format_bp_label(col) for col in bp_cols],
                    yticklabels=[self._format_sensor_pair_label(pair) for pair in sensor_pairs],
                    annot=True, fmt='.3f', cmap='RdBu_r', center=0,
                    mask=mask, square=False, linewidths=0.5,
                    cbar_kws={'label': 'ç›¸å…³ç³»æ•°'})
        
        # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
        for i in range(len(sensor_pairs)):
            for j in range(len(bp_cols)):
                if not np.isnan(p_matrix[i, j]) and p_matrix[i, j] < 0.05:
                    ax.text(j + 0.5, i + 0.5, '*', ha='center', va='center', 
                           color='white', fontsize=16, fontweight='bold')
        
        plt.title(f'PTTä¸è¡€å‹ç›¸å…³æ€§åˆ†æ{title_suffix}', fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('è¡€å‹æŒ‡æ ‡', fontsize=12)
        plt.ylabel('PTTä¼ æ„Ÿå™¨ç»„åˆ', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        filename = f"{self.output_dir}/ptt_bp_correlation_heatmap{title_suffix.replace(' ', '_')}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ ä¿å­˜ç›¸å…³æ€§çƒ­å›¾: {filename}")
        
        return fig
    
    def _format_ptt_label(self, ptt_col):
        """æ ¼å¼åŒ–PTTæ ‡ç­¾"""
        # ptt_sensor2-sensor3_ms -> noseâ†’finger
        sensor_map = {'sensor2': 'nose', 'sensor3': 'finger', 'sensor4': 'wrist', 'sensor5': 'ear'}
        parts = ptt_col.replace('ptt_', '').replace('_ms', '').split('-')
        if len(parts) == 2:
            return f"{sensor_map.get(parts[0], parts[0])}â†’{sensor_map.get(parts[1], parts[1])}"
        return ptt_col
    
    def _format_bp_label(self, bp_col):
        """æ ¼å¼åŒ–è¡€å‹æ ‡ç­¾"""
        label_map = {
            'systolic_bp': 'æ”¶ç¼©å‹',
            'diastolic_bp': 'èˆ’å¼ å‹',
            'mean_bp': 'å¹³å‡åŠ¨è„‰å‹',
            'bp': 'è¿ç»­è¡€å‹'
        }
        
        for bp_type, label in label_map.items():
            if bp_col.startswith(bp_type):
                suffix = bp_col.replace(bp_type, '').replace('_', ' ')
                return f"{label}{suffix}"
        return bp_col
    
    def _format_sensor_pair_label(self, sensor_pair):
        """æ ¼å¼åŒ–ä¼ æ„Ÿå™¨å¯¹æ ‡ç­¾"""
        # sensor2-sensor3 -> noseâ†’finger
        sensor_map = {'sensor2': 'nose', 'sensor3': 'finger', 'sensor4': 'wrist', 'sensor5': 'ear'}
        if '-' in sensor_pair:
            parts = sensor_pair.split('-')
            if len(parts) == 2:
                return f"{sensor_map.get(parts[0], parts[0])}â†’{sensor_map.get(parts[1], parts[1])}"
        return sensor_pair
    
    def build_regression_models(self, sync_df):
        """æ„å»ºPTTâ†’è¡€å‹çš„å›å½’æ¨¡å‹"""
        models = {}
        
        # ä¸»è¦è¡€å‹æŒ‡æ ‡
        main_bp_cols = ['systolic_bp_mean', 'diastolic_bp_mean', 'mean_bp_mean']
        
        # åˆ›å»ºä¼ æ„Ÿå™¨å¯¹çš„é€è§†è¡¨
        ptt_pivot = sync_df.pivot_table(
            index=['exp_id', 'window_id'], 
            columns='sensor_pair', 
            values='ptt_ms',
            aggfunc='mean'
        ).reset_index()
        
        # åˆå¹¶è¡€å‹æ•°æ®ï¼ˆå–å¹³å‡å€¼ï¼‰
        bp_agg = sync_df.groupby(['exp_id', 'window_id']).agg({
            col: 'mean' for col in main_bp_cols if col in sync_df.columns
        }).reset_index()
        
        # åˆå¹¶PTTå’Œè¡€å‹æ•°æ®
        model_data = pd.merge(ptt_pivot, bp_agg, on=['exp_id', 'window_id'], how='inner')
        
        if len(model_data) < 20:
            print(f"âš ï¸ æ¨¡å‹æ•°æ®ä¸è¶³: åªæœ‰{len(model_data)}ä¸ªæ ·æœ¬")
            return models
        
        # è·å–PTTç‰¹å¾åˆ—
        ptt_cols = [col for col in model_data.columns if col not in ['exp_id', 'window_id'] + main_bp_cols]
        # å»é™¤å…¨ç©ºçš„PTTåˆ—
        ptt_cols = [col for col in ptt_cols if not model_data[col].isna().all()]
        
        if len(ptt_cols) == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„PTTç‰¹å¾")
            return models
        
        for bp_col in main_bp_cols:
            if bp_col not in model_data.columns:
                continue
                
            # å‡†å¤‡æ•°æ®
            mask = ~model_data[bp_col].isna()
            for ptt_col in ptt_cols:
                mask &= ~model_data[ptt_col].isna()
            
            if mask.sum() < 20:  # è‡³å°‘20ä¸ªæ ·æœ¬
                continue
            
            X = model_data.loc[mask, ptt_cols].values
            y = model_data.loc[mask, bp_col].values
            
            # æ•°æ®æ ‡å‡†åŒ–
            scaler_X = StandardScaler()
            scaler_y = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)
            y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
            
            # è®­ç»ƒæ¨¡å‹
            model = LinearRegression()
            model.fit(X_scaled, y_scaled)
            
            # é¢„æµ‹
            y_pred_scaled = model.predict(X_scaled)
            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            
            # è¯„ä¼°
            r2 = r2_score(y, y_pred)
            mae = mean_absolute_error(y, y_pred)
            
            models[bp_col] = {
                'model': model,
                'scaler_X': scaler_X,
                'scaler_y': scaler_y,
                'feature_names': ptt_cols,
                'r2_score': r2,
                'mae': mae,
                'n_samples': len(y),
                'y_true': y,
                'y_pred': y_pred
            }
            
            print(f"ğŸ“ˆ {self._format_bp_label(bp_col)}æ¨¡å‹: RÂ²={r2:.3f}, MAE={mae:.2f}, N={len(y)}")
        
        return models
    
    def create_regression_plots(self, models):
        """åˆ›å»ºå›å½’åˆ†æå›¾"""
        n_models = len(models)
        if n_models == 0:
            return
        
        fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))
        if n_models == 1:
            axes = [axes]
        
        for idx, (bp_col, model_data) in enumerate(models.items()):
            ax = axes[idx]
            
            y_true = model_data['y_true']
            y_pred = model_data['y_pred']
            r2 = model_data['r2_score']
            mae = model_data['mae']
            
            # æ•£ç‚¹å›¾
            ax.scatter(y_true, y_pred, alpha=0.6, s=50)
            
            # ç†æƒ³çº¿
            min_val = min(y_true.min(), y_pred.min())
            max_val = max(y_true.max(), y_pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='ç†æƒ³é¢„æµ‹')
            
            # æ ¼å¼åŒ–
            ax.set_xlabel(f'çœŸå®{self._format_bp_label(bp_col)} (mmHg)')
            ax.set_ylabel(f'é¢„æµ‹{self._format_bp_label(bp_col)} (mmHg)')
            ax.set_title(f'{self._format_bp_label(bp_col)}\nRÂ²={r2:.3f}, MAE={mae:.2f}mmHg')
            ax.grid(True, alpha=0.3)
            ax.legend()
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        filename = f"{self.output_dir}/ptt_bp_regression_analysis.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ ä¿å­˜å›å½’åˆ†æå›¾: {filename}")
        
        return fig
    
    def analyze_experiment(self, exp_id):
        """åˆ†æå•ä¸ªå®éªŒ"""
        print(f"\nğŸ” åˆ†æå®éªŒ {exp_id}")
        
        # 1. åŠ è½½æ•°æ®
        bp_data = self.load_ground_truth_bp(exp_id)
        ptt_data = self.load_ptt_data(exp_id)
        
        if bp_data is None or ptt_data is None:
            print(f"âŒ å®éªŒ {exp_id} æ•°æ®åŠ è½½å¤±è´¥")
            return None
        
        # 2. æ—¶é—´åŒæ­¥
        sync_df = self.synchronize_data(ptt_data, bp_data, exp_id)
        print(f"ğŸ“Š åŒæ­¥æ•°æ®: {len(sync_df)}ä¸ªæ—¶é—´çª—å£")
        
        # 3. ç›¸å…³æ€§åˆ†æ
        correlations = self.calculate_correlations(sync_df)
        
        # 4. å›å½’å»ºæ¨¡
        models = self.build_regression_models(sync_df)
        
        return {
            'sync_data': sync_df,
            'correlations': correlations,
            'models': models
        }
    
    def run_comprehensive_analysis(self, experiment_list=None):
        """è¿è¡Œç»¼åˆåˆ†æ"""
        print("ğŸ”¬ å¼€å§‹PTTä¸è¡€å‹ç›¸å…³æ€§ç»¼åˆåˆ†æ")
        
        if experiment_list is None:
            # è‡ªåŠ¨æ£€æµ‹å¯ç”¨å®éªŒ
            experiment_list = []
            for i in range(1, 12):
                if os.path.exists(f"{self.ptt_output_dir}/exp_{i}"):
                    experiment_list.append(i)
        
        print(f"ğŸ“‹ åˆ†æå®éªŒåˆ—è¡¨: {experiment_list}")
        
        all_results = {}
        all_sync_data = []
        
        # åˆ†ææ¯ä¸ªå®éªŒ
        for exp_id in experiment_list:
            result = self.analyze_experiment(exp_id)
            if result is not None:
                all_results[exp_id] = result
                all_sync_data.append(result['sync_data'])
        
        if not all_sync_data:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ")
            return
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_sync_data, ignore_index=True)
        print(f"\nğŸ“Š åˆå¹¶æ•°æ®: {len(combined_df)}ä¸ªæ ·æœ¬ï¼Œæ¥è‡ª{len(all_results)}ä¸ªå®éªŒ")
        
        # æ•´ä½“ç›¸å…³æ€§åˆ†æ
        print("\nğŸ“ˆ è®¡ç®—æ•´ä½“ç›¸å…³æ€§...")
        overall_correlations = self.calculate_correlations(combined_df)
        
        # åˆ›å»ºæ•´ä½“ç›¸å…³æ€§çƒ­å›¾
        self.create_correlation_heatmap(overall_correlations, " (æ•´ä½“åˆ†æ)")
        
        # æ•´ä½“å›å½’åˆ†æ
        print("\nğŸ¯ æ„å»ºæ•´ä½“å›å½’æ¨¡å‹...")
        overall_models = self.build_regression_models(combined_df)
        self.create_regression_plots(overall_models)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.save_analysis_results(combined_df, overall_correlations, overall_models)
        
        print(f"\nâœ… PTTä¸è¡€å‹ç›¸å…³æ€§åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        
        return {
            'combined_data': combined_df,
            'correlations': overall_correlations,
            'models': overall_models,
            'individual_results': all_results
        }
    
    def save_analysis_results(self, combined_df, correlations, models):
        """ä¿å­˜åˆ†æç»“æœ"""
        # 1. ä¿å­˜åŒæ­¥æ•°æ®
        sync_file = f"{self.output_dir}/synchronized_ptt_bp_data.csv"
        combined_df.to_csv(sync_file, index=False)
        print(f"ğŸ’¾ ä¿å­˜åŒæ­¥æ•°æ®: {sync_file}")
        
        # 2. ä¿å­˜ç›¸å…³æ€§ç»“æœ
        corr_results = []
        for sensor_pair, bp_data in correlations.items():
            for bp_col, stats_data in bp_data.items():
                corr_results.append({
                    'sensor_pair': sensor_pair,
                    'bp_metric': bp_col,
                    'correlation': stats_data['correlation'],
                    'p_value': stats_data['p_value'],
                    'n_samples': stats_data['n_samples'],
                    'significant': stats_data['significant']
                })
        
        corr_df = pd.DataFrame(corr_results)
        corr_file = f"{self.output_dir}/ptt_bp_correlations.csv"
        corr_df.to_csv(corr_file, index=False)
        print(f"ğŸ’¾ ä¿å­˜ç›¸å…³æ€§åˆ†æ: {corr_file}")
        
        # 3. ä¿å­˜æ¨¡å‹è¯„ä¼°ç»“æœ
        model_results = []
        for bp_col, model_data in models.items():
            model_results.append({
                'bp_metric': bp_col,
                'r2_score': model_data['r2_score'],
                'mae': model_data['mae'],
                'n_samples': model_data['n_samples']
            })
        
        model_df = pd.DataFrame(model_results)
        model_file = f"{self.output_dir}/ptt_bp_model_evaluation.csv"
        model_df.to_csv(model_file, index=False)
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹è¯„ä¼°: {model_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ©º PTTä¸è¡€å‹ç›¸å…³æ€§åˆ†æ")
    print("="*60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = PTTBloodPressureAnalyzer()
    
    # è¿è¡Œç»¼åˆåˆ†æ
    results = analyzer.run_comprehensive_analysis()
    
    if results:
        print("\nğŸ“‹ åˆ†ææ€»ç»“:")
        print(f"   â€¢ æ€»æ ·æœ¬æ•°: {len(results['combined_data'])}")
        print(f"   â€¢ PTTæŒ‡æ ‡æ•°: {len([col for col in results['combined_data'].columns if col.startswith('ptt_')])}")
        print(f"   â€¢ è¡€å‹æŒ‡æ ‡æ•°: {len([col for col in results['combined_data'].columns if any(bp in col for bp in ['systolic', 'diastolic', 'mean_bp', 'bp'])])}")
        print(f"   â€¢ å›å½’æ¨¡å‹æ•°: {len(results['models'])}")
        
        # æ˜¾ç¤ºæœ€ä½³ç›¸å…³æ€§
        print(f"\nğŸ† æœ€å¼ºç›¸å…³æ€§ (å‰5å):")
        all_corrs = []
        for sensor_pair, bp_data in results['correlations'].items():
            for bp_col, stats_data in bp_data.items():
                if stats_data['significant']:
                    all_corrs.append((abs(stats_data['correlation']), 
                                    analyzer._format_sensor_pair_label(sensor_pair),
                                    analyzer._format_bp_label(bp_col),
                                    stats_data['correlation']))
        
        all_corrs.sort(reverse=True)
        for i, (abs_corr, ptt_label, bp_label, corr) in enumerate(all_corrs[:5]):
            print(f"   {i+1}. {ptt_label} â†â†’ {bp_label}: r={corr:.3f}")

if __name__ == "__main__":
    main() 