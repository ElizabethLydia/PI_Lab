#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PPG-ABPä¿¡å·åˆ†æå’Œå¯è§†åŒ–è„šæœ¬
é’ˆå¯¹17å·å—è¯•è€…ï¼Œå®ç°å¤šç§é¢„å¤„ç†æ–¹æ³•å¹¶è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡
äº¤äº’å¼å¯è§†åŒ–ï¼Œå›¾è¡¨æ ‡ç­¾ä½¿ç”¨è‹±æ–‡ä¾¿äºæ±‡æŠ¥
æ”¯æŒæ‰¹é‡å¤„ç†æ‰€æœ‰å®éªŒå’Œæ‰€æœ‰ä¼ æ„Ÿå™¨
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import signal
from scipy.stats import pearsonr, spearmanr
from scipy.interpolate import interp1d
from sklearn.metrics import mutual_info_score
import pywt
from scipy.ndimage import grey_opening, grey_closing
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibåç«¯ï¼Œç¡®ä¿å›¾è¡¨æ­£ç¡®æ˜¾ç¤ºï¼ˆæœåŠ¡å™¨ç¯å¢ƒå…¼å®¹ï¼‰
import matplotlib
try:
    # å°è¯•ä½¿ç”¨TkAggåç«¯ï¼ˆå¦‚æœæœ‰GUIç¯å¢ƒï¼‰
    matplotlib.use('TkAgg')
    print("âœ… ä½¿ç”¨TkAggåç«¯ï¼Œæ”¯æŒå¼¹çª—æ˜¾ç¤º")
except ImportError:
    try:
        # å›é€€åˆ°Aggåç«¯ï¼ˆæœåŠ¡å™¨ç¯å¢ƒï¼‰
        matplotlib.use('Agg')
        print("âš ï¸  ä½¿ç”¨Aggåç«¯ï¼Œå›¾è¡¨å°†ä¿å­˜åˆ°æ–‡ä»¶")
    except:
        # æœ€åå›é€€åˆ°é»˜è®¤åç«¯
        print("âš ï¸  ä½¿ç”¨é»˜è®¤matplotlibåç«¯")
        pass

# è®¾ç½®è‹±æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼ï¼Œä¾¿äºæ±‡æŠ¥ä½¿ç”¨
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# è®¾ç½®seabornæ ·å¼ï¼Œç¾åŒ–å›¾è¡¨
sns.set_palette("husl")
sns.set_context("notebook", font_scale=1.2)

class PPGABPAnalyzer:
    def __init__(self, subject_id="00017", experiment="1", sensor_name="sensor2"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            subject_id: å—è¯•è€…ID
            experiment: å®éªŒç¼–å·
            sensor_name: ä¼ æ„Ÿå™¨åç§°
        """
        self.subject_id = subject_id
        self.experiment = experiment
        self.sensor_name = sensor_name
        self.base_dir = f'/root/autodl-tmp/blood_pressure_reconstruction/{subject_id}/csv'
        
        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.ppg_file = f'{subject_id}_{experiment}_{sensor_name}.csv'
        self.abp_file = f'{subject_id}_{experiment}_abp.csv'
        
        # åŠ è½½æ•°æ®
        self.ppg_data = None
        self.abp_data = None
        self.load_data()
        
        # é¢„å¤„ç†åçš„æ•°æ®
        self.processed_signals = {}
        
        # ç›¸å…³æ€§æŒ‡æ ‡
        self.correlation_metrics = {}
        
        # è¿åŠ¨æ£€æµ‹ç¼“å­˜
        self.acc_available = False
        self.acc_magnitude = None
        self.motion_mask = None
        
    def load_data(self):
        """åŠ è½½PPGå’ŒABPæ•°æ®"""
        print(f"ğŸ“– åŠ è½½æ•°æ®...")
        print(f"  PPGæ–‡ä»¶: {self.ppg_file}  (ä¼ æ„Ÿå™¨: {self.sensor_name})")
        print(f"  ABPæ–‡ä»¶: {self.abp_file}")
        
        try:
            # åŠ è½½PPGæ•°æ®
            ppg_path = os.path.join(self.base_dir, self.ppg_file)
            self.ppg_data = pd.read_csv(ppg_path)
            print(f"  âœ… PPGæ•°æ®åŠ è½½æˆåŠŸ: {len(self.ppg_data)} è¡Œ")
            print(f"    åˆ—: {list(self.ppg_data.columns)}")
            
            # åŠ è½½ABPæ•°æ®
            abp_path = os.path.join(self.base_dir, self.abp_file)
            self.abp_data = pd.read_csv(abp_path)
            print(f"  âœ… ABPæ•°æ®åŠ è½½æˆåŠŸ: {len(self.abp_data)} è¡Œ")
            print(f"    åˆ—: {list(self.abp_data.columns)}")
            
            # æ£€æŸ¥æ•°æ®é•¿åº¦
            print(f"  ğŸ“Š æ•°æ®é•¿åº¦å¯¹æ¯”:")
            print(f"    PPG: {len(self.ppg_data)} è¡Œ")
            print(f"    ABP: {len(self.abp_data)} è¡Œ")
            
            # æ—¶é—´èŒƒå›´
            ppg_time_range = self.ppg_data['timestamp'].max() - self.ppg_data['timestamp'].min()
            abp_time_range = self.abp_data['timestamp'].max() - self.abp_data['timestamp'].min()
            print(f"  â±ï¸  æ—¶é—´èŒƒå›´:")
            print(f"    PPG: {ppg_time_range:.2f} ç§’")
            print(f"    ABP: {abp_time_range:.2f} ç§’")
            
        except Exception as e:
            print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _compute_motion_mask(self):
        """åŸºäºåŠ é€Ÿåº¦å¹…å€¼è®¡ç®—è¿åŠ¨æ©ç å¹¶ç¼“å­˜ã€‚"""
        if not self.acc_available:
            self.motion_mask = None
            return
        try:
            acc_mag = self.acc_magnitude
            threshold = np.mean(acc_mag) + 2.0 * np.std(acc_mag)
            self.motion_mask = acc_mag > threshold
            print(f"  ğŸƒ è¿åŠ¨æ£€æµ‹: ä½¿ç”¨ä¼ æ„Ÿå™¨{self.sensor_name}çš„åŠ é€Ÿåº¦(ax, ay, az)ï¼Œé˜ˆå€¼={threshold:.3f}ï¼Œè¿åŠ¨å æ¯”={(np.mean(self.motion_mask)*100):.1f}%")
        except Exception as e:
            print(f"  âš ï¸  è¿åŠ¨æ©ç è®¡ç®—å¤±è´¥: {e}")
            self.motion_mask = None
    
    def _spec(self, x, fs=None):
        """å®Œå…¨æŒ‰ç…§MATLAB specå‡½æ•°å®ç°çš„é¢‘è°±åˆ†æ
        Input:  x   - 1xN pulse signal
                fs  - 1x1 camera frame rate
        Output: spc - 1xM pulse spectrogram
                rate - 1xM heart rate signal
        """
        if fs is None:
            fs = 100  # é»˜è®¤é‡‡æ ·ç‡
        
        # ç¡®ä¿xæ˜¯1Dæ•°ç»„
        x = np.asarray(x).flatten()
        
        # å®šä¹‰å‚æ•° (MATLAB: L = fps * 10)
        L = int(fs * 10)
        
        # åˆå§‹åŒ–SçŸ©é˜µ (MATLAB: S = zeros(size(signal,2)-L+1,L))
        S = np.zeros((len(x) - L + 1, L))
        
        # æ»‘åŠ¨çª—å£å¤„ç† (MATLAB: for idx = 1:size(signal,2)-L+1)
        for idx in range(len(x) - L + 1):
            p = x[idx:idx + L]
            # æ ‡å‡†åŒ– (MATLAB: p = (p-mean(p))/(eps+std(p)))
            S[idx, :] = (p - np.mean(p)) / (np.std(p) + np.finfo(float).eps)
        
        # å»é™¤å‡å€¼ (MATLAB: S = S-repmat(mean(S,2),[1,L]))
        S = S - np.mean(S, axis=1, keepdims=True)
        
        # åº”ç”¨Hannçª— (MATLAB: S = S .* repmat(hann(L)',[size(S,1),L]))
        hann_window = signal.windows.hann(L, sym=False)
        S = S * hann_window
        
        # FFT (MATLAB: S = abs(fft(S,fps*60,2)))
        fft_size = fs * 60
        S_fft = np.abs(np.fft.fft(S, fft_size, axis=1))
        
        # å–å‰åŠéƒ¨åˆ†å¹¶è½¬ç½® (MATLAB: spc = S(:,1:fps*60/2)')
        spc = S_fft[:, :fft_size//2].T
        
        # è®¡ç®—å¿ƒç‡ (MATLAB: [~, rate] = max(spc,[],1); rate = rate - 1)
        rate = np.argmax(spc, axis=0)
        rate = rate - 1
        
        return spc, rate
    
    def pca_extraction(self, rgb_data):
        """PCAæ–¹æ³•æå–PPGä¿¡å·"""
        try:
            from sklearn.decomposition import PCA
            
            # å¤„ç†NaNå€¼
            rgb_clean = rgb_data.copy()
            if np.any(np.isnan(rgb_clean)):
                # ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……NaN
                for i in range(rgb_clean.shape[1]):
                    rgb_clean[:, i] = pd.Series(rgb_clean[:, i]).fillna(method='ffill').fillna(method='bfill').values
                if np.any(np.isnan(rgb_clean)):
                    return None, None
            
            # é¢„å¤„ç†æ¯ä¸ªé€šé“
            processed_data = np.zeros_like(rgb_clean)
            for i in range(3):
                processed_data[:, i] = self.preprocess_signal(rgb_clean[:, i])
            
            # åº”ç”¨PCA
            pca = PCA(n_components=3)
            pca_result = pca.fit_transform(processed_data)
            
            # ç¬¬ä¸€ä¸»æˆåˆ†é€šå¸¸åŒ…å«PPGä¿¡å·
            ppg_signal = pca_result[:, 0]
            explained_variance = pca.explained_variance_ratio_
            
            return ppg_signal, explained_variance
            
        except Exception as e:
            print(f"    PCAæå–å¤±è´¥: {e}")
            return None, None
    
    def svd_extraction(self, rgb_data):
        """SVDæ–¹æ³•æå–PPGä¿¡å·"""
        try:
            from scipy.linalg import svd
            
            # å¤„ç†NaNå€¼
            rgb_clean = rgb_data.copy()
            if np.any(np.isnan(rgb_clean)):
                # ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……NaN
                for i in range(rgb_clean.shape[1]):
                    rgb_clean[:, i] = pd.Series(rgb_clean[:, i]).fillna(method='ffill').fillna(method='bfill').values
                if np.any(np.isnan(rgb_clean)):
                    return None, None
            
            # é¢„å¤„ç†æ¯ä¸ªé€šé“
            processed_data = np.zeros_like(rgb_clean)
            for i in range(3):
                processed_data[:, i] = self.preprocess_signal(rgb_clean[:, i])
            
            # åº”ç”¨SVD
            U, s, Vt = svd(processed_data, full_matrices=False)
            
            # ç¬¬ä¸€å¥‡å¼‚å€¼å¯¹åº”çš„å·¦å¥‡å¼‚å‘é‡é€šå¸¸åŒ…å«PPGä¿¡å·
            ppg_signal = U[:, 0] * s[0]
            singular_values = s
            
            return ppg_signal, singular_values
            
        except Exception as e:
            print(f"    SVDæå–å¤±è´¥: {e}")
            return None, None
    
    def pos_extraction(self, rgb_data):
        """POS (Plane Orthogonal to Skin) æ–¹æ³•æå–PPGä¿¡å·"""
        try:
            # å¤„ç†NaNå€¼
            rgb_clean = rgb_data.copy()
            if np.any(np.isnan(rgb_clean)):
                # ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……NaN
                for i in range(rgb_clean.shape[1]):
                    rgb_clean[:, i] = pd.Series(rgb_clean[:, i]).fillna(method='ffill').fillna(method='bfill').values
                if np.any(np.isnan(rgb_clean)):
                    return None
            
            # é¢„å¤„ç†æ¯ä¸ªé€šé“
            processed_data = np.zeros_like(rgb_clean)
            for i in range(3):
                processed_data[:, i] = self.preprocess_signal(rgb_clean[:, i])
            
            # POSç®—æ³•å®ç°
            # 1. è®¡ç®—RGBé€šé“çš„å½’ä¸€åŒ–å€¼
            r_norm = processed_data[:, 0] / np.mean(processed_data[:, 0])
            g_norm = processed_data[:, 1] / np.mean(processed_data[:, 1])
            b_norm = processed_data[:, 2] / np.mean(processed_data[:, 2])
            
            # 2. è®¡ç®—POSä¿¡å·
            # POS = (r_norm - g_norm) + (r_norm - b_norm)
            pos_signal = (r_norm - g_norm) + (r_norm - b_norm)
            
            # 3. å†æ¬¡åº”ç”¨å¸¦é€šæ»¤æ³¢
            ppg_signal = self.butterworth_filter(pos_signal, lowcut=0.5, highcut=4.0, order=4)
            
            return ppg_signal
            
        except Exception as e:
            print(f"    POSæå–å¤±è´¥: {e}")
            return None
    
    def chrom_extraction(self, rgb_data):
        """CHROM (Chrominance-based) æ–¹æ³•æå–PPGä¿¡å·"""
        try:
            # å¤„ç†NaNå€¼
            rgb_clean = rgb_data.copy()
            if np.any(np.isnan(rgb_clean)):
                # ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……NaN
                for i in range(rgb_clean.shape[1]):
                    rgb_clean[:, i] = pd.Series(rgb_clean[:, i]).fillna(method='ffill').fillna(method='bfill').values
                if np.any(np.isnan(rgb_clean)):
                    return None
            
            # é¢„å¤„ç†æ¯ä¸ªé€šé“
            processed_data = np.zeros_like(rgb_clean)
            for i in range(3):
                processed_data[:, i] = self.preprocess_signal(rgb_clean[:, i])
            
            # CHROMç®—æ³•å®ç°
            # 1. è®¡ç®—å½’ä¸€åŒ–RGBå€¼
            r_norm = processed_data[:, 0] / np.mean(processed_data[:, 0])
            g_norm = processed_data[:, 1] / np.mean(processed_data[:, 1])
            b_norm = processed_data[:, 2] / np.mean(processed_data[:, 2])
            
            # 2. è®¡ç®—è‰²åº¦ä¿¡å·
            # CHROM = (r_norm - g_norm) / (r_norm + g_norm - 2*b_norm)
            denominator = r_norm + g_norm - 2 * b_norm
            
            # é¿å…é™¤é›¶
            epsilon = 1e-10
            denominator = np.where(np.abs(denominator) < epsilon, epsilon, denominator)
            
            chrom_signal = (r_norm - g_norm) / denominator
            
            # 3. å†æ¬¡åº”ç”¨å¸¦é€šæ»¤æ³¢
            ppg_signal = self.butterworth_filter(chrom_signal, lowcut=0.5, highcut=4.0, order=4)
            
            return ppg_signal
            
        except Exception as e:
            print(f"    CHROMæå–å¤±è´¥: {e}")
            return None
    
    def ica_extraction(self, rgb_data):
        """ICA (ç‹¬ç«‹åˆ†é‡åˆ†æ) æ–¹æ³•æå–PPGä¿¡å·"""
        try:
            from sklearn.decomposition import FastICA
            
            # å¤„ç†NaNå€¼
            rgb_clean = rgb_data.copy()
            if np.any(np.isnan(rgb_clean)):
                # ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……NaN
                for i in range(rgb_clean.shape[1]):
                    rgb_clean[:, i] = pd.Series(rgb_clean[:, i]).fillna(method='ffill').fillna(method='bfill').values
                if np.any(np.isnan(rgb_clean)):
                    return None
            
            # é¢„å¤„ç†æ¯ä¸ªé€šé“
            processed_data = np.zeros_like(rgb_clean)
            for i in range(3):
                processed_data[:, i] = self.preprocess_signal(rgb_clean[:, i])
            
            # åº”ç”¨ICA
            ica = FastICA(n_components=3, random_state=42)
            ica_result = ica.fit_transform(processed_data)
            
            # é€‰æ‹©æ–¹å·®æœ€å¤§çš„åˆ†é‡ä½œä¸ºPPGä¿¡å·
            variances = np.var(ica_result, axis=0)
            best_component = np.argmax(variances)
            ppg_signal = ica_result[:, best_component]
            
            return ppg_signal
            
        except Exception as e:
            print(f"    ICAæå–å¤±è´¥: {e}")
            return None
    
    def nmf_extraction(self, rgb_data):
        """NMF (éè´ŸçŸ©é˜µåˆ†è§£) æ–¹æ³•æå–PPGä¿¡å·"""
        try:
            from sklearn.decomposition import NMF
            
            # å¤„ç†NaNå€¼
            rgb_clean = rgb_data.copy()
            if np.any(np.isnan(rgb_clean)):
                # ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……NaN
                for i in range(rgb_clean.shape[1]):
                    rgb_clean[:, i] = pd.Series(rgb_clean[:, i]).fillna(method='ffill').fillna(method='bfill').values
                if np.any(np.isnan(rgb_clean)):
                    return None
            
            # é¢„å¤„ç†ï¼šç¡®ä¿æ•°æ®éè´Ÿ
            processed_data = np.abs(rgb_clean)
            
            # åº”ç”¨NMF
            nmf = NMF(n_components=3, random_state=42, max_iter=200)
            nmf_result = nmf.fit_transform(processed_data)
            
            # é€‰æ‹©é‡æ„è¯¯å·®æœ€å°çš„åˆ†é‡ä½œä¸ºPPGä¿¡å·
            reconstruction_errors = []
            for i in range(3):
                # é‡æ„å•ä¸ªåˆ†é‡ - ä¿®å¤çŸ©é˜µç»´åº¦é—®é¢˜
                component_i = nmf.components_[i:i+1]  # (1, 3)
                scores_i = nmf_result[:, i:i+1]      # (n_samples, 1)
                reconstructed = scores_i @ component_i  # (n_samples, 3)
                error = np.mean((processed_data - reconstructed) ** 2)
                reconstruction_errors.append(error)
            
            best_component = np.argmin(reconstruction_errors)
            ppg_signal = nmf_result[:, best_component]
            
            # åº”ç”¨å¸¦é€šæ»¤æ³¢
            ppg_signal = self.butterworth_filter(ppg_signal, lowcut=0.5, highcut=4.0, order=4)
            
            return ppg_signal
            
        except Exception as e:
            print(f"    NMFæå–å¤±è´¥: {e}")
            return None
    
    def preprocess_signal(self, data):
        """é¢„å¤„ç†ä¿¡å·ï¼šå½’ä¸€åŒ– + å¸¦é€šæ»¤æ³¢"""
        # 1. å½’ä¸€åŒ–
        normalized = (data - np.mean(data)) / (np.std(data) + 1e-10)
        
        # 2. å¸¦é€šæ»¤æ³¢
        filtered = self.butterworth_filter(normalized, lowcut=0.5, highcut=4.0, order=4)
        
        return filtered
    
    def align_signals(self):
        """å¯¹é½PPGå’ŒABPä¿¡å·åˆ°ç›¸åŒçš„æ—¶é—´æˆ³"""
        print(f"\nğŸ”„ å¯¹é½ä¿¡å·...")
        
        # ä½¿ç”¨PPGæ—¶é—´æˆ³ä½œä¸ºå‚è€ƒ
        ref_timestamps = self.ppg_data['timestamp'].values
        ppg_ir = self.ppg_data['ir'].values
        ppg_red = self.ppg_data['red'].values
        ppg_green = self.ppg_data['green'].values
        
        # æ’å€¼ABPæ•°æ®åˆ°PPGæ—¶é—´æˆ³
        abp_interpolated = np.interp(
            ref_timestamps, 
            self.abp_data['timestamp'].values, 
            self.abp_data['abp'].values
        )
        
        # åˆ›å»ºå¯¹é½åçš„æ•°æ®
        self.aligned_data = pd.DataFrame({
            'timestamp': ref_timestamps,
            'ppg_ir': ppg_ir,
            'ppg_red': ppg_red,
            'ppg_green': ppg_green,
            'abp': abp_interpolated
        })
        
        # æ·»åŠ åŠ é€Ÿåº¦æ•°æ®å¹¶è®¡ç®—è¿åŠ¨æ©ç 
        if all(col in self.ppg_data.columns for col in ['ax', 'ay', 'az']):
            self.aligned_data['ax'] = np.interp(
                ref_timestamps,
                self.ppg_data['timestamp'].values,
                self.ppg_data['ax'].values
            )
            self.aligned_data['ay'] = np.interp(
                ref_timestamps,
                self.ppg_data['timestamp'].values,
                self.ppg_data['ay'].values
            )
            self.aligned_data['az'] = np.interp(
                ref_timestamps,
                self.ppg_data['timestamp'].values,
                self.ppg_data['az'].values
            )
            # è®¡ç®—åŠ é€Ÿåº¦å¹…åº¦
            self.acc_magnitude = np.sqrt(self.aligned_data['ax']**2 + self.aligned_data['ay']**2 + self.aligned_data['az']**2).values
            self.acc_available = True
            print(f"  âœ… åŠ é€Ÿåº¦åˆ—å­˜åœ¨(ax, ay, az)ï¼Œå°†ç”¨äºè¿åŠ¨ä¼ªå½±æ£€æµ‹å’Œå»é™¤")
            self._compute_motion_mask()
        else:
            self.acc_available = False
            self.acc_magnitude = None
            self.motion_mask = None
            print(f"  âš ï¸  æœªå‘ç°åŠ é€Ÿåº¦åˆ—(ax, ay, az)ï¼Œè·³è¿‡è¿åŠ¨ä¼ªå½±å»é™¤")
        
        print(f"  âœ… ä¿¡å·å¯¹é½å®Œæˆ: {len(self.aligned_data)} è¡Œ")
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        abp_nan_count = self.aligned_data['abp'].isna().sum()
        if abp_nan_count > 0:
            print(f"  âš ï¸  ABPæ•°æ®ä¸­æœ‰ {abp_nan_count} ä¸ªNaNå€¼")
        
        return self.aligned_data
    
    def _auto_detect_fs(self):
        """è‡ªåŠ¨æ£€æµ‹é‡‡æ ·ç‡"""
        try:
            if hasattr(self, 'aligned_data') and len(self.aligned_data) > 1:
                timestamps = self.aligned_data['timestamp'].values
                time_diff = np.diff(timestamps)
                # è®¡ç®—å¹³å‡æ—¶é—´é—´éš”
                mean_interval = np.mean(time_diff)
                # é‡‡æ ·ç‡ = 1 / æ—¶é—´é—´éš”
                fs = 1.0 / mean_interval
                print(f"  ğŸ“Š è‡ªåŠ¨æ£€æµ‹é‡‡æ ·ç‡: {fs:.2f} Hz")
                return fs
            else:
                print(f"  âš ï¸  æ— æ³•æ£€æµ‹é‡‡æ ·ç‡ï¼Œä½¿ç”¨é»˜è®¤å€¼: 100 Hz")
                return 100.0
        except Exception as e:
            print(f"  âš ï¸  é‡‡æ ·ç‡æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼: 100 Hz")
            return 100.0
    
    def butterworth_filter(self, signal_data, lowcut=0.5, highcut=4.0, fs=None, order=4):
        """Butterworthå¸¦é€šæ»¤æ³¢å™¨ - å»é™¤ä½é¢‘æ¼‚ç§»å’Œé«˜é¢‘å™ªå£°"""
        if fs is None:
            fs = self._auto_detect_fs()
        
        nyquist = fs / 2
        low = lowcut / nyquist
        high = highcut / nyquist
        b, a = signal.butter(order, [low, high], btype='band')
        filtered = signal.filtfilt(b, a, signal_data)
        return filtered
    
    def pca_denoising(self, signal_data, n_components=3):
        """PCAå»å™ª - å…ˆå¸¦é€šæ»¤æ³¢ï¼Œå†è¿›è¡ŒPCAé™å™ª"""
        try:
            from sklearn.decomposition import PCA
            
            # æ­¥éª¤1: å…ˆè¿›è¡Œå¸¦é€šæ»¤æ³¢
            fs = self._auto_detect_fs()
            filtered_signal = self.butterworth_filter(signal_data, lowcut=0.5, highcut=4.0, fs=fs, order=4)
            
            # æ­¥éª¤2: å°†ä¿¡å·åˆ†æ®µï¼Œæ¯æ®µ100ä¸ªç‚¹
            segment_length = 100
            n_segments = len(filtered_signal) // segment_length
            
            if n_segments < 2:
                return filtered_signal
            
            # åˆ›å»ºä¿¡å·çŸ©é˜µ
            signal_matrix = []
            for i in range(n_segments):
                segment = filtered_signal[i*segment_length:(i+1)*segment_length]
                signal_matrix.append(segment)
            
            # å¦‚æœæœ€åä¸€æ®µä¸å®Œæ•´ï¼Œç”¨å‰ä¸€æ®µå¡«å……
            if len(filtered_signal) % segment_length != 0:
                last_segment = filtered_signal[n_segments*segment_length:]
                if len(last_segment) > 0:
                    # ç”¨å‰ä¸€æ®µçš„å¯¹åº”éƒ¨åˆ†å¡«å……
                    padding = signal_matrix[-1][:segment_length-len(last_segment)]
                    last_segment = np.concatenate([last_segment, padding])
                    signal_matrix.append(last_segment)
            
            signal_matrix = np.array(signal_matrix)
            
            # æ­¥éª¤3: åº”ç”¨PCA
            pca = PCA(n_components=min(n_components, signal_matrix.shape[1]))
            signal_reconstructed = pca.fit_transform(signal_matrix)
            signal_denoised = pca.inverse_transform(signal_reconstructed)
            
            # é‡æ„å®Œæ•´ä¿¡å·
            denoised_signal = []
            for i in range(n_segments):
                denoised_signal.extend(signal_denoised[i][:segment_length])
            
            # å¤„ç†å‰©ä½™éƒ¨åˆ†
            if len(filtered_signal) % segment_length != 0:
                remaining = filtered_signal[n_segments*segment_length:]
                denoised_signal.extend(remaining)
            
            return np.array(denoised_signal[:len(filtered_signal)])
            
        except Exception as e:
            print(f"    PCAå»å™ªå¤±è´¥: {e}")
            return signal_data
    

    
    def emd_denoising(self, signal_data, max_imfs=5):
        """ç»éªŒæ¨¡æ€åˆ†è§£(EMD)å»å™ª"""
        try:
            from PyEMD import EMD
            
            # åˆ›å»ºEMDå¯¹è±¡
            emd = EMD()
            emd.emd(signal_data, max_imf=max_imfs)
            
            # è·å–IMFåˆ†é‡
            imfs = emd.imfs
            
            # è®¡ç®—æ¯ä¸ªIMFçš„èƒ½é‡
            energies = []
            for imf in imfs:
                energy = np.sum(imf**2)
                energies.append(energy)
            
            # é€‰æ‹©èƒ½é‡è¾ƒå¤§çš„IMFä½œä¸ºä¿¡å·ï¼Œè¾ƒå°çš„ä½œä¸ºå™ªå£°
            threshold = np.mean(energies) * 0.1
            signal_imfs = []
            
            for i, energy in enumerate(energies):
                if energy > threshold:
                    signal_imfs.append(imfs[i])
            
            # é‡æ„ä¿¡å·
            if signal_imfs:
                denoised = np.sum(signal_imfs, axis=0)
            else:
                denoised = signal_data
            
            return denoised
            
        except Exception as e:
            print(f"    EMDå»å™ªå¤±è´¥: {e}")
            return signal_data
    
    def kalman_filter(self, signal_data, Q=0.1, R=1.0):
        """å¡å°”æ›¼æ»¤æ³¢ - çŠ¶æ€ä¼°è®¡å»å™ª"""
        try:
            n = len(signal_data)
            
            # åˆå§‹åŒ–
            x_hat = np.zeros(n)  # çŠ¶æ€ä¼°è®¡
            P = np.zeros(n)      # è¯¯å·®åæ–¹å·®
            
            # åˆå§‹å€¼
            x_hat[0] = signal_data[0]
            P[0] = 1.0
            
            # å¡å°”æ›¼æ»¤æ³¢è¿­ä»£
            for k in range(1, n):
                # é¢„æµ‹æ­¥éª¤
                x_hat_minus = x_hat[k-1]
                P_minus = P[k-1] + Q
                
                # æ›´æ–°æ­¥éª¤
                K = P_minus / (P_minus + R)  # å¡å°”æ›¼å¢ç›Š
                x_hat[k] = x_hat_minus + K * (signal_data[k] - x_hat_minus)
                P[k] = (1 - K) * P_minus
            
            return x_hat
            
        except Exception as e:
            print(f"    å¡å°”æ›¼æ»¤æ³¢å¤±è´¥: {e}")
            return signal_data
    
    def wavelet_denoising(self, signal_data, wavelet='db4', level=4):
        """å°æ³¢å»å™ª - å»é™¤éšæœºå™ªå£°ï¼Œä¿æŒä¿¡å·ç‰¹å¾"""
        try:
            # å°æ³¢åˆ†è§£
            coeffs = pywt.wavedec(signal_data, wavelet, level=level)
            
            # é˜ˆå€¼å¤„ç†
            threshold = np.std(coeffs[-1]) * np.sqrt(2 * np.log(len(signal_data)))
            coeffs[1:] = [pywt.threshold(c, threshold, mode='soft') for c in coeffs[1:]]
            
            # å°æ³¢é‡æ„
            denoised = pywt.waverec(coeffs, wavelet)
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            if len(denoised) > len(signal_data):
                denoised = denoised[:len(signal_data)]
            elif len(denoised) < len(signal_data):
                denoised = np.pad(denoised, (0, len(signal_data) - len(denoised)), 'edge')
            
            return denoised
        except Exception as e:
            print(f"    å°æ³¢å»å™ªå¤±è´¥: {e}")
            return signal_data
    
    def morphological_filter(self, signal_data, size=5):
        """å½¢æ€å­¦æ»¤æ³¢ - å»é™¤åŸºçº¿æ¼‚ç§»ï¼Œå¹³æ»‘ä¿¡å·"""
        try:
            # å¼€è¿ç®—ï¼ˆå…ˆè…èš€åè†¨èƒ€ï¼‰
            opened = grey_opening(signal_data, size=size)
            # é—­è¿ç®—ï¼ˆå…ˆè†¨èƒ€åè…èš€ï¼‰
            closed = grey_closing(opened, size=size)
            return closed
        except Exception as e:
            print(f"    å½¢æ€å­¦æ»¤æ³¢å¤±è´¥: {e}")
            return signal_data
    
    def improved_motion_removal(self, ppg_signal, acc_data, window_size=100):
        """æ”¹è¿›çš„è¿åŠ¨ä¼ªå½±å»é™¤ - å…ˆå¸¦é€šæ»¤æ³¢ï¼Œå†è¿›è¡Œè¿åŠ¨æ£€æµ‹å’Œå»é™¤"""
        try:
            if acc_data is None or len(acc_data) == 0:
                return ppg_signal
            
            # æ­¥éª¤1: å…ˆè¿›è¡Œå¸¦é€šæ»¤æ³¢
            fs = self._auto_detect_fs()
            filtered_ppg = self.butterworth_filter(ppg_signal, lowcut=0.5, highcut=4.0, fs=fs, order=4)
            
            # æ­¥éª¤2: è®¡ç®—åŠ é€Ÿåº¦å¹…åº¦
            acc_magnitude = np.sqrt(acc_data['ax']**2 + acc_data['ay']**2 + acc_data['az']**2)
            
            # æ­¥éª¤3: ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—åŠ¨æ€é˜ˆå€¼
            acc_threshold = np.zeros_like(acc_magnitude)
            for i in range(len(acc_magnitude)):
                start_idx = max(0, i - window_size // 2)
                end_idx = min(len(acc_magnitude), i + window_size // 2)
                local_mean = np.mean(acc_magnitude[start_idx:end_idx])
                local_std = np.std(acc_magnitude[start_idx:end_idx])
                acc_threshold[i] = local_mean + 1.5 * local_std  # é™ä½é˜ˆå€¼
            
            # æ­¥éª¤4: åˆ›å»ºè¿åŠ¨æ©ç 
            motion_mask = acc_magnitude > acc_threshold
            
            # æ­¥éª¤5: å½¢æ€å­¦æ“ä½œï¼šå»é™¤å­¤ç«‹çš„è¿åŠ¨ç‚¹
            from scipy.ndimage import binary_opening, binary_closing
            motion_mask = binary_opening(motion_mask, structure=np.ones(5))
            motion_mask = binary_closing(motion_mask, structure=np.ones(10))
            
            # æ­¥éª¤6: å¯¹è¿åŠ¨æ®µè¿›è¡Œæ›´æ™ºèƒ½çš„æ’å€¼
            ppg_cleaned = filtered_ppg.copy()
            if np.any(motion_mask):
                # æ‰¾åˆ°è¿ç»­çš„è¿åŠ¨æ®µ
                motion_changes = np.diff(np.concatenate([[False], motion_mask, [False]]))
                motion_starts = np.where(motion_changes)[0][::2]
                motion_ends = np.where(motion_changes)[0][1::2]
                
                for start, end in zip(motion_starts, motion_ends):
                    if start < len(filtered_ppg) and end <= len(filtered_ppg):
                        # è·å–è¿åŠ¨æ®µå‰åçš„æœ‰æ•ˆæ•°æ®
                        pre_motion = filtered_ppg[max(0, start-50):start]
                        post_motion = filtered_ppg[end:min(len(filtered_ppg), end+50)]
                        
                        if len(pre_motion) > 0 and len(post_motion) > 0:
                            # ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼
                            from scipy.interpolate import CubicSpline
                            try:
                                pre_indices = np.arange(max(0, start-50), start)
                                post_indices = np.arange(end, min(len(filtered_ppg), end+50))
                                
                                # åˆ›å»ºæ’å€¼å‡½æ•°
                                all_indices = np.concatenate([pre_indices, post_indices])
                                all_values = np.concatenate([pre_motion, post_motion])
                                
                                if len(all_indices) > 3:
                                    cs = CubicSpline(all_indices, all_values)
                                    motion_indices = np.arange(start, end)
                                    ppg_cleaned[motion_indices] = cs(motion_indices)
                            except:
                                # å¦‚æœæ ·æ¡æ’å€¼å¤±è´¥ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼
                                motion_indices = np.arange(start, end)
                                ppg_cleaned[motion_indices] = np.interp(
                                    motion_indices, 
                                    np.concatenate([pre_indices, post_indices]), 
                                    np.concatenate([pre_motion, post_motion])
                                )
            
            return ppg_cleaned
            
        except Exception as e:
            print(f"    æ”¹è¿›è¿åŠ¨ä¼ªå½±å»é™¤å¤±è´¥: {e}")
            return ppg_signal
    
    def apply_preprocessing_methods(self):
        """åº”ç”¨å¤šç§é¢„å¤„ç†æ–¹æ³•"""
        print(f"\nğŸ”§ åº”ç”¨é¢„å¤„ç†æ–¹æ³•...")
        
        # è·å–å¯¹é½åçš„æ•°æ®
        if not hasattr(self, 'aligned_data'):
            self.align_signals()
        
        # æå–ä¿¡å·
        ppg_ir = self.aligned_data['ppg_ir'].values
        ppg_red = self.aligned_data['ppg_red'].values
        ppg_green = self.aligned_data['ppg_green'].values
        abp = self.aligned_data['abp'].values
        
        # åˆ›å»ºRGBæ•°æ®çŸ©é˜µ
        rgb_data = np.column_stack([ppg_ir, ppg_red, ppg_green])
        
        # 1. åŸå§‹ä¿¡å·
        self.processed_signals['Original Signal'] = {
            'ppg': ppg_ir,
            'abp': abp
        }
        
        # 2. Butterworthæ»¤æ³¢ (0.5-4Hzï¼Œé€‚åˆPPGä¿¡å·)
        print(f"  ğŸ”§ Butterworthæ»¤æ³¢ (0.5-4Hz)...")
        ppg_butter = self.butterworth_filter(ppg_ir, lowcut=0.5, highcut=4.0, order=4)
        self.processed_signals['Butterworth Filter'] = {
            'ppg': ppg_butter,
            'abp': abp
        }
        
        # 3. å°æ³¢å»å™ª
        print(f"  ğŸ”§ å°æ³¢å»å™ª...")
        ppg_wavelet = self.wavelet_denoising(ppg_ir)
        self.processed_signals['Wavelet Denoising'] = {
            'ppg': ppg_wavelet,
            'abp': abp
        }
        
        # 4. å½¢æ€å­¦æ»¤æ³¢
        print(f"  ğŸ”§ å½¢æ€å­¦æ»¤æ³¢...")
        ppg_morph = self.morphological_filter(ppg_ir)
        self.processed_signals['Morphological Filter'] = {
            'ppg': ppg_morph,
            'abp': abp
        }
        
        # 5. PCAå»å™ªï¼ˆå¸¦é€šæ»¤æ³¢åï¼‰
        print(f"  ğŸ”§ PCAå»å™ªï¼ˆå¸¦é€šæ»¤æ³¢åï¼‰...")
        ppg_pca = self.pca_denoising(ppg_ir, n_components=3)
        self.processed_signals['PCA Denoising (Bandpass)'] = {
            'ppg': ppg_pca,
            'abp': abp
        }
        
        # 6. å¡å°”æ›¼æ»¤æ³¢
        print(f"  ğŸ”§ å¡å°”æ›¼æ»¤æ³¢...")
        ppg_kalman = self.kalman_filter(ppg_ir, Q=0.1, R=1.0)
        self.processed_signals['Kalman Filter'] = {
            'ppg': ppg_kalman,
            'abp': abp
        }
        
        # 7. ç»„åˆæ»¤æ³¢ï¼ˆButterworth + å°æ³¢ï¼‰
        print(f"  ğŸ”§ ç»„åˆæ»¤æ³¢ (Butterworth + å°æ³¢)...")
        ppg_combined = self.butterworth_filter(ppg_wavelet, lowcut=0.5, highcut=4.0, order=4)
        self.processed_signals['Combined Filter'] = {
            'ppg': ppg_combined,
            'abp': abp
        }
        
        # 8. æ”¹è¿›çš„è¿åŠ¨ä¼ªå½±å»é™¤ï¼ˆå¸¦é€šæ»¤æ³¢åï¼‰
        if self.acc_available:
            print(f"  ğŸ”§ æ”¹è¿›è¿åŠ¨ä¼ªå½±å»é™¤ï¼ˆå¸¦é€šæ»¤æ³¢åï¼Œä½¿ç”¨{self.sensor_name}çš„ax, ay, azï¼‰...")
            acc_data = self.aligned_data[['ax', 'ay', 'az']]
            ppg_motion_removed = self.improved_motion_removal(ppg_ir, acc_data)
            self.processed_signals['Motion Removal (Bandpass)'] = {
                'ppg': ppg_motion_removed,
                'abp': abp
            }
        else:
            print(f"  âš ï¸  æ— åŠ é€Ÿåº¦æ•°æ®ï¼Œè·³è¿‡è¿åŠ¨ä¼ªå½±å»é™¤")
        
        # 9. å¤šé€šé“èåˆï¼ˆIR + Red + Greenï¼‰
        print(f"  ğŸ”§ å¤šé€šé“èåˆ (IR + Red + Green)...")
        # ç®€å•åŠ æƒå¹³å‡
        ppg_fusion = 0.6 * ppg_ir + 0.3 * ppg_red + 0.1 * ppg_green
        self.processed_signals['Multi-Channel Fusion'] = {
            'ppg': ppg_fusion,
            'abp': abp
        }
        
        # 10. PCAæå–æ–¹æ³•
        print(f"  ğŸ”§ PCAæå–æ–¹æ³•...")
        ppg_pca_extracted, pca_variance = self.pca_extraction(rgb_data)
        if ppg_pca_extracted is not None:
            self.processed_signals['PCA Extraction'] = {
                'ppg': ppg_pca_extracted,
                'abp': abp
            }
        
        # 11. SVDæå–æ–¹æ³•
        print(f"  ğŸ”§ SVDæå–æ–¹æ³•...")
        ppg_svd_extracted, svd_values = self.svd_extraction(rgb_data)
        if ppg_svd_extracted is not None:
            self.processed_signals['SVD Extraction'] = {
                'ppg': ppg_svd_extracted,
                'abp': abp
            }
        
        # 12. POSæå–æ–¹æ³•
        print(f"  ğŸ”§ POSæå–æ–¹æ³•...")
        ppg_pos_extracted = self.pos_extraction(rgb_data)
        if ppg_pos_extracted is not None:
            self.processed_signals['POS Extraction'] = {
                'ppg': ppg_pos_extracted,
                'abp': abp
            }
        
        # 13. CHROMæå–æ–¹æ³•
        print(f"  ğŸ”§ CHROMæå–æ–¹æ³•...")
        ppg_chrom_extracted = self.chrom_extraction(rgb_data)
        if ppg_chrom_extracted is not None:
            self.processed_signals['CHROM Extraction'] = {
                'ppg': ppg_chrom_extracted,
                'abp': abp
            }
        
        # 14. ICAæå–æ–¹æ³•
        print(f"  ğŸ”§ ICAæå–æ–¹æ³•...")
        ppg_ica_extracted = self.ica_extraction(rgb_data)
        if ppg_ica_extracted is not None:
            self.processed_signals['ICA Extraction'] = {
                'ppg': ppg_ica_extracted,
                'abp': abp
            }
        
        # 15. NMFæå–æ–¹æ³•
        print(f"  ğŸ”§ NMFæå–æ–¹æ³•...")
        ppg_nmf_extracted = self.nmf_extraction(rgb_data)
        if ppg_nmf_extracted is not None:
            self.processed_signals['NMF Extraction'] = {
                'ppg': ppg_nmf_extracted,
                'abp': abp
            }
        
        print(f"  âœ… é¢„å¤„ç†å®Œæˆï¼Œå…± {len(self.processed_signals)} ç§æ–¹æ³•")
        
        return self.processed_signals
    
    def calculate_correlation_metrics(self):
        """è®¡ç®—å„ç§ç›¸å…³æ€§æŒ‡æ ‡"""
        print(f"\nğŸ“Š è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡...")
        
        for method_name, signals in self.processed_signals.items():
            ppg = signals['ppg']
            abp = signals['abp']
            
            # å»é™¤NaNå€¼
            valid_mask = ~(np.isnan(ppg) | np.isnan(abp))
            if np.sum(valid_mask) < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆç‚¹
                continue
            
            ppg_valid = ppg[valid_mask]
            abp_valid = abp[valid_mask]
            
            metrics = {}
            
            try:
                # 1. Pearsonç›¸å…³ç³»æ•°
                pearson_r, pearson_p = pearsonr(ppg_valid, abp_valid)
                metrics['pearson_r'] = pearson_r
                metrics['pearson_p'] = pearson_p
                
                # 2. Spearmanç›¸å…³ç³»æ•°
                spearman_r, spearman_p = spearmanr(ppg_valid, abp_valid)
                metrics['spearman_r'] = spearman_r
                metrics['spearman_p'] = spearman_p
                
                # 3. äº’ä¿¡æ¯
                # å°†è¿ç»­å€¼åˆ†ç®±ä»¥è®¡ç®—äº’ä¿¡æ¯
                ppg_binned = pd.cut(ppg_valid, bins=20, labels=False)
                abp_binned = pd.cut(abp_valid, bins=20, labels=False)
                mutual_info = mutual_info_score(ppg_binned, abp_binned)
                metrics['mutual_info'] = mutual_info
                
                # 4. é¢‘ç‡åŸŸç›¸å…³æ€§
                ppg_fft = np.abs(np.fft.fft(ppg_valid))
                abp_fft = np.abs(np.fft.fft(abp_valid))
                freq_corr, _ = pearsonr(ppg_fft, abp_fft)
                metrics['freq_correlation'] = freq_corr
                
                # 5. ç›¸å¹²æ€§
                if len(ppg_valid) > 100:
                    f, coh = signal.coherence(ppg_valid, abp_valid, fs=100)
                    metrics['coherence_mean'] = np.mean(coh)
                    metrics['coherence_max'] = np.max(coh)
                else:
                    metrics['coherence_mean'] = np.nan
                    metrics['coherence_max'] = np.nan
                
                # 6. ä¿¡å·è´¨é‡æŒ‡æ ‡
                ppg_snr = self.calculate_snr(ppg_valid)
                abp_snr = self.calculate_snr(abp_valid)
                metrics['ppg_snr'] = ppg_snr
                metrics['abp_snr'] = abp_snr
                
                self.correlation_metrics[method_name] = metrics
                
            except Exception as e:
                print(f"    âŒ {method_name} ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
                continue
        
        print(f"  âœ… ç›¸å…³æ€§è®¡ç®—å®Œæˆï¼Œå…± {len(self.correlation_metrics)} ç§æ–¹æ³•")
        return self.correlation_metrics
    
    def calculate_snr(self, signal_data):
        """è®¡ç®—ä¿¡å™ªæ¯” - ä½¿ç”¨BVPSNRæ–¹æ³• (G. de Haan, TBME, 2013)"""
        try:
            from scipy.signal import periodogram
            from scipy.signal.windows import hamming
            
            # ä¼°è®¡å¿ƒç‡ï¼ˆä½¿ç”¨FFTæ‰¾åˆ°æœ€å¤§åŠŸç‡é¢‘ç‡ï¼‰
            fft = np.fft.fft(signal_data)
            freqs = np.fft.fftfreq(len(signal_data), 1/100)  # å‡è®¾100Hzé‡‡æ ·ç‡
            
            # åªè€ƒè™‘æ­£é¢‘ç‡å’Œ0.5-4HzèŒƒå›´
            pos_mask = (freqs > 0) & (freqs >= 0.5) & (freqs <= 4)
            pos_freqs = freqs[pos_mask]
            pos_power = np.abs(fft[pos_mask])**2
            
            if len(pos_power) == 0:
                return np.nan
            
            # æ‰¾åˆ°æœ€å¤§åŠŸç‡å¯¹åº”çš„é¢‘ç‡
            max_power_idx = np.argmax(pos_power)
            hr_freq = pos_freqs[max_power_idx]  # Hz
            
            # è½¬æ¢ä¸ºBPM
            hr_bpm = hr_freq * 60
            
            # ä½¿ç”¨BVPSNRæ–¹æ³•è®¡ç®—SNR
            return self._calculate_bvpsnr(signal_data, 100, hr_bpm)
            
        except Exception as e:
            print(f"    SNRè®¡ç®—å¤±è´¥: {e}")
            return np.nan
    
    def _calculate_bvpsnr(self, bvp, fs, hr_bpm, plot_tf=False):
        """BVPSNRæ–¹æ³•è®¡ç®—ä¿¡å™ªæ¯” (G. de Haan, TBME, 2013)"""
        try:
            from scipy.signal import periodogram
            from scipy.signal.windows import hamming
            
            # å¤„ç†NaNå€¼
            bvp_clean = bvp.copy()
            if np.any(np.isnan(bvp_clean)):
                # ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……NaN
                bvp_clean = pd.Series(bvp_clean).fillna(method='ffill').fillna(method='bfill').values
                if np.any(np.isnan(bvp_clean)):
                    return np.nan
            
            # è½¬æ¢å¿ƒç‡ä¸ºHz
            hr_f = hr_bpm / 60
            
            # è®¡ç®—åŠŸç‡è°±å¯†åº¦
            nyquist_f = fs / 2
            f_res_bpm = 0.5  # åˆ†è¾¨ç‡ (bpm)
            n = round((60 * 2 * nyquist_f) / f_res_bpm)  # åŠŸç‡è°±ä¸­çš„binæ•°é‡
            
            # æ„é€ å‘¨æœŸå›¾
            f, pxx = periodogram(bvp_clean, fs=fs)
            
            # åˆ›å»ºæ©ç 
            # 1. å¿ƒç‡å³°å€¼åŒºåŸŸ (Â±0.1 Hz)
            gt_mask1 = (f >= hr_f - 0.1) & (f <= hr_f + 0.1)
            
            # 2. ä¸€æ¬¡è°æ³¢åŒºåŸŸ (Â±0.2 Hz)
            gt_mask2 = (f >= hr_f * 2 - 0.2) & (f <= hr_f * 2 + 0.2)
            
            # 3. ä¿¡å·åŠŸç‡ (å¿ƒç‡å³°å€¼ + ä¸€æ¬¡è°æ³¢)
            s_power = np.sum(pxx[gt_mask1 | gt_mask2])
            
            # 4. æ€»åŠŸç‡ (0.5-4 Hz)
            f_mask2 = (f >= 0.5) & (f <= 4)
            all_power = np.sum(pxx[f_mask2])
            
            # 5. è®¡ç®—SNR
            if (all_power - s_power) > 0:
                snr = 10 * np.log10(s_power / (all_power - s_power))
            else:
                snr = np.nan
            
            # å¯é€‰ï¼šç»˜åˆ¶åŠŸç‡è°±å’ŒSNRåŒºåŸŸ
            if plot_tf:
                self._plot_snr_regions(f, pxx, hr_f)
            
            return snr
            
        except Exception as e:
            print(f"    BVPSNRè®¡ç®—å¤±è´¥: {e}")
            return np.nan
    
    def _plot_snr_regions(self, f, pxx, hr_f):
        """ç»˜åˆ¶åŠŸç‡è°±å’ŒSNRåŒºåŸŸ"""
        try:
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # ç»˜åˆ¶åŠŸç‡è°±
            ax.plot(f, 10 * np.log10(pxx + 1e-12))
            ax.set_title('Power Spectrum and SNR Regions')
            ax.set_xlabel('Frequency (Hz)')
            ax.set_ylabel('Power (dB)')
            ax.set_xlim([0.5, 4])
            
            ylim_reg = ax.get_ylim()
            ax.hold = True
            
            # å¿ƒç‡å³°å€¼åŒºåŸŸ
            ax.axvline(x=hr_f-0.1, color='red', linestyle='--', label='HR-0.1Hz')
            ax.axvline(x=hr_f+0.1, color='red', linestyle='--', label='HR+0.1Hz')
            
            # ä¸€æ¬¡è°æ³¢åŒºåŸŸ
            ax.axvline(x=hr_f*2-0.2, color='red', linestyle='--', label='2HR-0.2Hz')
            ax.axvline(x=hr_f*2+0.2, color='red', linestyle='--', label='2HR+0.2Hz')
            
            # æ€»åŠŸç‡åŒºåŸŸ
            ax.axvline(x=0.5, color='black', linestyle='-', label='0.5Hz')
            ax.axvline(x=4.0, color='black', linestyle='-', label='4Hz')
            
            ax.set_xlim([0, 4.5])
            ax.set_ylim(ylim_reg)
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
        except Exception as e:
            print(f"    SNRåŒºåŸŸç»˜å›¾å¤±è´¥: {e}")
    
    def _shade_motion_regions(self, ax, time_relative, start_idx, end_idx):
        """åœ¨å›¾ä¸Šé«˜äº®æ ‡æ³¨è¿åŠ¨æ®µã€‚"""
        if self.motion_mask is None:
            return
        mask_segment = self.motion_mask[start_idx:end_idx]
        if not np.any(mask_segment):
            return
        # æ‰¾è¿ç»­åŒºåŸŸ
        idx = np.where(mask_segment)[0]
        if len(idx) == 0:
            return
        # åˆ†æ®µ
        splits = np.split(idx, np.where(np.diff(idx) != 1)[0] + 1)
        for seg in splits:
            t0 = time_relative.iloc[seg[0]]
            t1 = time_relative.iloc[seg[-1]]
            ax.axvspan(t0, t1, color='red', alpha=0.08, linewidth=0)
    
    def plot_full_length_signals(self):
        """ç»˜åˆ¶å…¨é•¿åº¦ä¿¡å·å¯¹æ¯”å›¾"""
        print(f"\nğŸ“ˆ ç»˜åˆ¶å…¨é•¿åº¦ä¿¡å·å¯¹æ¯”å›¾...")
        
        if not self.processed_signals:
            self.apply_preprocessing_methods()
        
        if not self.correlation_metrics:
            self.calculate_correlation_metrics()
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 1, figsize=(16, 10))
        
        # æ—¶é—´è½´ï¼ˆç›¸å¯¹æ—¶é—´ï¼‰
        time_relative = self.aligned_data['timestamp'] - self.aligned_data['timestamp'].iloc[0]
        
        # è·å–åŸå§‹ä¿¡å·
        ppg_original = self.processed_signals['Original Signal']['ppg']
        abp_original = self.processed_signals['Original Signal']['abp']
        
        # è·å–ç›¸å…³æ€§æŒ‡æ ‡ç”¨äºæ ‡é¢˜
        metrics = self.correlation_metrics.get('Original Signal', {})
        title_suffix = ""
        if metrics:
            title_suffix = f" | Pearson r={metrics.get('pearson_r', 'N/A'):.3f}, Spearman r={metrics.get('spearman_r', 'N/A'):.3f}, MI={metrics.get('mutual_info', 'N/A'):.3f}"
        
        # PPGä¿¡å·
        ax1 = axes[0]
        ax1.plot(time_relative, ppg_original, 'b-', linewidth=0.8, alpha=0.8, label=f'PPG (IR, {self.sensor_name})')
        ax1.set_title(f'Subject {self.subject_id} Experiment {self.experiment} - PPG Signal (Full Length){title_suffix}', 
                     fontsize=14, fontweight='bold')
        ax1.set_xlabel('Time (seconds)', fontsize=12)
        ax1.set_ylabel('PPG Value', fontsize=12)
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=11)
        
        # ABPä¿¡å·
        ax2 = axes[1]
        ax2.plot(time_relative, abp_original, 'r-', linewidth=0.8, alpha=0.8, label='ABP (Arterial Blood Pressure)')
        ax2.set_title(f'Subject {self.subject_id} Experiment {self.experiment} - ABP Signal (Full Length){title_suffix}', 
                     fontsize=14, fontweight='bold')
        ax2.set_xlabel('Time (seconds)', fontsize=12)
        ax2.set_ylabel('Blood Pressure (mmHg)', fontsize=12)
        ax2.grid(True, alpha=0.3)
        ax2.legend(fontsize=11)
        
        plt.tight_layout()
        
        # ä¿å­˜å¹¶æ˜¾ç¤º
        output_dir = f'/root/autodl-tmp/blood_pressure_reconstruction/{self.subject_id}/analysis_results'
        os.makedirs(output_dir, exist_ok=True)
        
        plot_file = os.path.join(output_dir, f'{self.subject_id}_exp{self.experiment}_{self.sensor_name}_full_length_signals.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"  ğŸ’¾ å…¨é•¿åº¦ä¿¡å·å›¾å·²ä¿å­˜: {plot_file}")
        
        # å°è¯•æ˜¾ç¤ºå›¾è¡¨ï¼ˆå¦‚æœç¯å¢ƒæ”¯æŒï¼‰
        try:
            plt.show()
        except:
            print("  âš ï¸  å›¾è¡¨å·²ä¿å­˜ï¼Œä½†æ— æ³•åœ¨ç»ˆç«¯ä¸­æ˜¾ç¤º")
        
        return fig
    
    def plot_segment_signals(self, segment_length=4000, start_idx=None):
        """ç»˜åˆ¶åˆ†æ®µä¿¡å·å¯¹æ¯”å›¾ - åŒ…å«ä¿¡å™ªæ¯”ä¿¡æ¯ï¼Œå¹¶é«˜äº®è¿åŠ¨æ®µ"""
        print(f"\nğŸ“Š ç»˜åˆ¶åˆ†æ®µä¿¡å·å¯¹æ¯”å›¾...")
        
        if not self.processed_signals:
            self.apply_preprocessing_methods()
        
        if not self.correlation_metrics:
            self.calculate_correlation_metrics()
        
        # ç¡®å®šç»˜å›¾æ®µ
        total_length = len(self.aligned_data)
        if start_idx is None:
            start_idx = total_length // 4  # ä»1/4å¤„å¼€å§‹
        
        end_idx = min(start_idx + segment_length, total_length)
        
        print(f"  ğŸ§® ç»˜å›¾æ®µ: {start_idx} - {end_idx} (å…± {end_idx - start_idx} ä¸ªç‚¹)")
        
        # åˆ›å»ºå­å›¾ - æ¯ä¸ªæ–¹æ³•3ä¸ªå­å›¾ï¼šPPGå¤„ç†ç»“æœã€åŸå§‹ABPã€PPGå’ŒABPå¯¹æ¯”
        n_methods = len(self.processed_signals)
        fig, axes = plt.subplots(n_methods, 3, figsize=(24, 4*n_methods))
        
        if n_methods == 1:
            axes = axes.reshape(1, -1)
        
        # æ—¶é—´è½´
        time_segment = self.aligned_data['timestamp'].iloc[start_idx:end_idx]
        time_relative = time_segment - time_segment.iloc[0]
        
        # è·å–åŸå§‹ABPä¿¡å·ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        abp_original = self.aligned_data['abp'].values[start_idx:end_idx]
        
        # ç»˜åˆ¶æ¯ç§é¢„å¤„ç†æ–¹æ³•çš„ç»“æœ
        for i, (method_name, signals) in enumerate(self.processed_signals.items()):
            ppg = signals['ppg'][start_idx:end_idx]
            abp = signals['abp'][start_idx:end_idx]
            
            # è·å–ç›¸å…³æ€§æŒ‡æ ‡
            metrics = self.correlation_metrics.get(method_name, {})
            
            # æ„å»ºåŒ…å«ç›¸å…³æ€§ä¿¡æ¯å’Œä¿¡å™ªæ¯”çš„æ ‡é¢˜
            title_ppg = f'{method_name} - PPG Signal ({self.sensor_name})\n'
            title_abp = f'Original ABP Signal\n'
            title_compare = f'{method_name} - PPG vs ABP Comparison\n'
            
            if metrics:
                # PPGæ ‡é¢˜ï¼šç›¸å…³æ€§æŒ‡æ ‡
                title_ppg += f'Pearson r={metrics.get("pearson_r", "N/A"):.3f}, '
                title_ppg += f'Spearman r={metrics.get("spearman_r", "N/A"):.3f}, '
                title_ppg += f'MI={metrics.get("mutual_info", "N/A"):.3f}'
                
                # ABPæ ‡é¢˜ï¼šä¿¡å™ªæ¯”
                title_abp += f'ABP SNR={metrics.get("abp_snr", "N/A"):.1f}dB'
                
                # å¯¹æ¯”æ ‡é¢˜ï¼šç›¸å…³æ€§æŒ‡æ ‡
                title_compare += f'Correlation: r={metrics.get("pearson_r", "N/A"):.3f}, SNR={metrics.get("ppg_snr", "N/A"):.1f}dB'
            
            # ç¬¬ä¸€åˆ—ï¼šPPGå¤„ç†ç»“æœ
            ax1 = axes[i, 0]
            ax1.plot(time_relative, ppg, 'b-', linewidth=1, alpha=0.85, label=f'PPG (IR, {self.sensor_name})')
            ax1.set_title(title_ppg, fontsize=11, fontweight='bold')
            ax1.set_xlabel('Time (seconds)', fontsize=10)
            ax1.set_ylabel('PPG Value', fontsize=10)
            ax1.grid(True, alpha=0.3)
            ax1.legend(fontsize=9)
            # é«˜äº®è¿åŠ¨æ®µ
            self._shade_motion_regions(ax1, time_relative, start_idx, end_idx)
            
            # åœ¨PPGå›¾ä¸Šæ·»åŠ ä¿¡å™ªæ¯”ä¿¡æ¯
            if metrics and not np.isnan(metrics.get('ppg_snr', np.nan)):
                snr_text = f"PPG SNR: {metrics.get('ppg_snr', 'N/A'):.1f} dB"
                ax1.text(0.02, 0.98, snr_text, transform=ax1.transAxes, 
                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
            
            # ç¬¬äºŒåˆ—ï¼šåŸå§‹ABPä¿¡å·
            ax2 = axes[i, 1]
            ax2.plot(time_relative, abp_original, 'r-', linewidth=1, alpha=0.85, label='ABP (Arterial Blood Pressure)')
            ax2.set_title(title_abp, fontsize=11, fontweight='bold')
            ax2.set_xlabel('Time (seconds)', fontsize=10)
            ax2.set_ylabel('Blood Pressure (mmHg)', fontsize=10)
            ax2.grid(True, alpha=0.3)
            ax2.legend(fontsize=9)
            # é«˜äº®è¿åŠ¨æ®µï¼ˆåŒä¸€æ—¶é—´åŒºåŸŸï¼‰
            self._shade_motion_regions(ax2, time_relative, start_idx, end_idx)
            
            # åœ¨ABPå›¾ä¸Šæ·»åŠ ä¿¡å™ªæ¯”ä¿¡æ¯
            if metrics and not np.isnan(metrics.get('abp_snr', np.nan)):
                snr_text = f"ABP SNR: {metrics.get('abp_snr', 'N/A'):.1f} dB"
                ax2.text(0.02, 0.98, snr_text, transform=ax2.transAxes, 
                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))
            
            # ç¬¬ä¸‰åˆ—ï¼šPPGå’ŒABPå¯¹æ¯”å›¾
            ax3 = axes[i, 2]
            # åŒYè½´æ˜¾ç¤º
            ax3_twin = ax3.twinx()
            
            # ç»˜åˆ¶PPGä¿¡å·ï¼ˆå·¦Yè½´ï¼‰
            line1 = ax3.plot(time_relative, ppg, 'b-', linewidth=1.5, alpha=0.8, label=f'PPG ({self.sensor_name})')
            ax3.set_xlabel('Time (seconds)', fontsize=10)
            ax3.set_ylabel('PPG Value', fontsize=10, color='blue')
            ax3.tick_params(axis='y', labelcolor='blue')
            
            # ç»˜åˆ¶ABPä¿¡å·ï¼ˆå³Yè½´ï¼‰
            line2 = ax3_twin.plot(time_relative, abp_original, 'r-', linewidth=1.5, alpha=0.8, label='ABP')
            ax3_twin.set_ylabel('Blood Pressure (mmHg)', fontsize=10, color='red')
            ax3_twin.tick_params(axis='y', labelcolor='red')
            
            # è®¾ç½®æ ‡é¢˜
            ax3.set_title(title_compare, fontsize=11, fontweight='bold')
            
            # åˆå¹¶å›¾ä¾‹
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            ax3.legend(lines, labels, loc='upper right', fontsize=9)
            
            # é«˜äº®è¿åŠ¨æ®µ
            self._shade_motion_regions(ax3, time_relative, start_idx, end_idx)
            
            # æ·»åŠ ç›¸å…³æ€§ä¿¡æ¯
            if metrics and not np.isnan(metrics.get('pearson_r', np.nan)):
                corr_text = f"r={metrics.get('pearson_r', 'N/A'):.3f}"
                ax3.text(0.02, 0.98, corr_text, transform=ax3.transAxes, 
                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜å¹¶æ˜¾ç¤º
        output_dir = f'/root/autodl-tmp/blood_pressure_reconstruction/{self.subject_id}/analysis_results'
        os.makedirs(output_dir, exist_ok=True)
        
        plot_file = os.path.join(output_dir, f'{self.subject_id}_exp{self.experiment}_{self.sensor_name}_segment_signals.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"  ğŸ’¾ åˆ†æ®µä¿¡å·å›¾å·²ä¿å­˜: {plot_file}")
        
        # å°è¯•æ˜¾ç¤ºå›¾è¡¨ï¼ˆå¦‚æœç¯å¢ƒæ”¯æŒï¼‰
        try:
            plt.show()
        except:
            print("  âš ï¸  å›¾è¡¨å·²ä¿å­˜ï¼Œä½†æ— æ³•åœ¨ç»ˆç«¯ä¸­æ˜¾ç¤º")
        
        return fig
    
    def plot_correlation_summary(self):
        """ç»˜åˆ¶ç›¸å…³æ€§æ€»ç»“å›¾ - ä½¿ç”¨seabornç¾åŒ–"""
        print(f"\nğŸ“Š ç»˜åˆ¶ç›¸å…³æ€§æ€»ç»“å›¾...")
        
        if not self.correlation_metrics:
            self.calculate_correlation_metrics()
        
        # å‡†å¤‡æ•°æ®
        methods = list(self.correlation_metrics.keys())
        metrics_names = ['pearson_r', 'spearman_r', 'mutual_info', 'freq_correlation', 'coherence_mean']
        
        # åˆ›å»ºç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = np.zeros((len(methods), len(metrics_names)))
        
        for i, method in enumerate(methods):
            for j, metric in enumerate(metrics_names):
                corr_matrix[i, j] = self.correlation_metrics[method].get(metric, np.nan)
        
        # åˆ›å»ºDataFrameç”¨äºseaborn
        df_corr = pd.DataFrame(corr_matrix, 
                              index=methods, 
                              columns=['Pearson r', 'Spearman r', 'Mutual Info', 'Freq Corr', 'Coherence'])
        
        # åˆ›å»ºç¾åŒ–åçš„çƒ­åŠ›å›¾ - å¢å¤§å›¾è¡¨å°ºå¯¸
        fig, ax = plt.subplots(figsize=(18, 14))
        
        # ä½¿ç”¨seabornç»˜åˆ¶ç¾åŒ–çƒ­åŠ›å›¾
        sns.heatmap(df_corr, 
                   annot=True, 
                   fmt='.3f', 
                   cmap='RdYlBu_r',  # çº¢è“é…è‰²ï¼Œæ›´ç¾è§‚
                   center=0,
                   square=True,      # æ­£æ–¹å½¢å•å…ƒæ ¼
                   linewidths=0.8,   # å¢åŠ ç½‘æ ¼çº¿å®½åº¦
                   cbar_kws={'shrink': 0.8},  # é¢œè‰²æ¡è®¾ç½®
                   ax=ax,
                   annot_kws={'size': 10})  # å¢å¤§æ³¨é‡Šå­—ä½“
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾ - å¢å¤§å­—ä½“
        ax.set_title(f'Subject {self.subject_id} Experiment {self.experiment} - Correlation Metrics Summary ({self.sensor_name})', 
                    fontsize=18, fontweight='bold', pad=25)
        ax.set_xlabel('Correlation Metrics', fontsize=16, fontweight='bold')
        ax.set_ylabel('Preprocessing Methods', fontsize=16, fontweight='bold')
        
        # æ—‹è½¬æ ‡ç­¾å¹¶å¢å¤§å­—ä½“
        plt.xticks(rotation=45, ha='right', fontsize=12)
        plt.yticks(rotation=0, fontsize=11)
        
        # è°ƒæ•´å¸ƒå±€ï¼Œå¢åŠ è¾¹è·
        plt.tight_layout(pad=2.0)
        
        # ä¿å­˜å¹¶æ˜¾ç¤º
        output_dir = f'/root/autodl-tmp/blood_pressure_reconstruction/{self.subject_id}/analysis_results'
        os.makedirs(output_dir, exist_ok=True)
        
        heatmap_file = os.path.join(output_dir, f'{self.subject_id}_exp{self.experiment}_{self.sensor_name}_correlation_heatmap.png')
        plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
        print(f"  ğŸ’¾ ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_file}")
        
        # å°è¯•æ˜¾ç¤ºå›¾è¡¨ï¼ˆå¦‚æœç¯å¢ƒæ”¯æŒï¼‰
        try:
            plt.show()
        except:
            print("  âš ï¸  å›¾è¡¨å·²ä¿å­˜ï¼Œä½†æ— æ³•åœ¨ç»ˆç«¯ä¸­æ˜¾ç¤º")
        
        return fig
    
    def run_visualization_analysis(self, segment_length=2000, start_idx=None):
        """è¿è¡Œå¯è§†åŒ–åˆ†æ"""
        print(f"\nğŸš€ å¼€å§‹å¯è§†åŒ–åˆ†æ...")
        print(f"{'='*60}")
        print(f"  å—è¯•è€…: {self.subject_id}")
        print(f"  å®éªŒç¼–å·: {self.experiment}")
        print(f"  ä½¿ç”¨ä¼ æ„Ÿå™¨: {self.sensor_name}")
        
        try:
            # 1. åº”ç”¨é¢„å¤„ç†æ–¹æ³•
            self.apply_preprocessing_methods()
            
            # 2. è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡
            self.calculate_correlation_metrics()
            
            # 3. ç»˜åˆ¶å…¨é•¿åº¦ä¿¡å·å¯¹æ¯”å›¾
            self.plot_full_length_signals()
            
            # 4. ç»˜åˆ¶åˆ†æ®µä¿¡å·å¯¹æ¯”å›¾
            self.plot_segment_signals(segment_length, start_idx)
            
            # 5. ç»˜åˆ¶ç›¸å…³æ€§æ€»ç»“å›¾
            self.plot_correlation_summary()
            
            print(f"\nğŸ‰ å¯è§†åŒ–åˆ†æå®Œæˆï¼")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: /root/autodl-tmp/blood_pressure_reconstruction/{self.subject_id}/analysis_results/")
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise

class BatchAnalyzer:
    """æ‰¹é‡åˆ†æå™¨ - å¤„ç†æ‰€æœ‰å®éªŒå’Œæ‰€æœ‰ä¼ æ„Ÿå™¨"""
    
    def __init__(self, subject_id="00017"):
        self.subject_id = subject_id
        self.base_dir = f'/root/autodl-tmp/blood_pressure_reconstruction/{subject_id}/csv'
        
    def get_available_experiments_and_sensors(self):
        """è·å–å¯ç”¨çš„å®éªŒç¼–å·å’Œä¼ æ„Ÿå™¨åˆ—è¡¨"""
        if not os.path.exists(self.base_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {self.base_dir}")
            return [], []
        
        # è·å–æ‰€æœ‰æ–‡ä»¶
        files = os.listdir(self.base_dir)
        
        # æå–å®éªŒç¼–å·
        experiments = set()
        sensors = set()
        
        for file in files:
            if file.endswith('.csv'):
                parts = file.split('_')
                if len(parts) >= 3:
                    # æ ¼å¼: 00017_1_sensor2.csv æˆ– 00017_1_abp.csv
                    exp_num = parts[1]
                    if exp_num.isdigit():
                        experiments.add(exp_num)
                    
                    if len(parts) >= 3:
                        sensor_part = parts[2].replace('.csv', '')
                        if sensor_part.startswith('sensor'):
                            sensors.add(sensor_part)
        
        experiments = sorted(list(experiments), key=int)
        sensors = sorted(list(sensors))
        
        return experiments, sensors
    
    def run_batch_analysis(self, experiments=None, sensors=None, segment_length=2000):
        """è¿è¡Œæ‰¹é‡åˆ†æ"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ - å—è¯•è€… {self.subject_id}")
        print(f"{'='*80}")
        
        # è·å–å¯ç”¨çš„å®éªŒå’Œä¼ æ„Ÿå™¨
        available_experiments, available_sensors = self.get_available_experiments_and_sensors()
        
        if not available_experiments:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å®éªŒæ•°æ®")
            return
        
        if not available_sensors:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä¼ æ„Ÿå™¨æ•°æ®")
            return
        
        # ä½¿ç”¨æŒ‡å®šçš„å®éªŒå’Œä¼ æ„Ÿå™¨ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šåˆ™ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„
        if experiments is None:
            experiments = available_experiments
        if sensors is None:
            sensors = available_sensors
        
        print(f"ğŸ“‹ å¯ç”¨å®éªŒ: {available_experiments}")
        print(f"ğŸ“‹ å¯ç”¨ä¼ æ„Ÿå™¨: {available_sensors}")
        print(f"ğŸ¯ å°†åˆ†æå®éªŒ: {experiments}")
        print(f"ğŸ¯ å°†åˆ†æä¼ æ„Ÿå™¨: {sensors}")
        
        total_combinations = len(experiments) * len(sensors)
        current = 0
        
        results = []
        
        for exp in experiments:
            for sensor in sensors:
                current += 1
                print(f"\n{'='*60}")
                print(f"ğŸ”¬ è¿›åº¦: {current}/{total_combinations}")
                print(f"ğŸ“Š åˆ†æ: å®éªŒ{exp} - {sensor}")
                print(f"{'='*60}")
                
                try:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    ppg_file = f'{self.subject_id}_{exp}_{sensor}.csv'
                    abp_file = f'{self.subject_id}_{exp}_abp.csv'
                    
                    ppg_path = os.path.join(self.base_dir, ppg_file)
                    abp_path = os.path.join(self.base_dir, abp_file)
                    
                    if not os.path.exists(ppg_path):
                        print(f"  âš ï¸  PPGæ–‡ä»¶ä¸å­˜åœ¨: {ppg_file}")
                        continue
                    
                    if not os.path.exists(abp_path):
                        print(f"  âš ï¸  ABPæ–‡ä»¶ä¸å­˜åœ¨: {abp_file}")
                        continue
                    
                    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
                    analyzer = PPGABPAnalyzer(self.subject_id, exp, sensor)
                    analyzer.run_visualization_analysis(segment_length=segment_length)
                    
                    results.append({
                        'experiment': exp,
                        'sensor': sensor,
                        'status': 'success'
                    })
                    
                    print(f"  âœ… å®éªŒ{exp} - {sensor} åˆ†æå®Œæˆ")
                    
                except Exception as e:
                    print(f"  âŒ å®éªŒ{exp} - {sensor} åˆ†æå¤±è´¥: {e}")
                    results.append({
                        'experiment': exp,
                        'sensor': sensor,
                        'status': 'failed',
                        'error': str(e)
                    })
        
        # ç”Ÿæˆæ‰¹é‡åˆ†ææŠ¥å‘Š
        self._generate_batch_report(results)
        
        print(f"\nğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸ: {len([r for r in results if r['status'] == 'success'])}")
        print(f"âŒ å¤±è´¥: {len([r for r in results if r['status'] == 'failed'])}")
    
    def _generate_batch_report(self, results):
        """ç”Ÿæˆæ‰¹é‡åˆ†ææŠ¥å‘Š"""
        output_dir = f'/root/autodl-tmp/blood_pressure_reconstruction/{self.subject_id}/analysis_results'
        os.makedirs(output_dir, exist_ok=True)
        
        report_file = os.path.join(output_dir, f'{self.subject_id}_batch_analysis_report.txt')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"å—è¯•è€… {self.subject_id} æ‰¹é‡åˆ†ææŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            f.write(f"åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»ç»„åˆæ•°: {len(results)}\n\n")
            
            # æˆåŠŸå’Œå¤±è´¥çš„ç»Ÿè®¡
            successful = [r for r in results if r['status'] == 'success']
            failed = [r for r in results if r['status'] == 'failed']
            
            f.write(f"æˆåŠŸåˆ†æ: {len(successful)} ä¸ªç»„åˆ\n")
            f.write(f"å¤±è´¥åˆ†æ: {len(failed)} ä¸ªç»„åˆ\n")
            f.write(f"æˆåŠŸç‡: {len(successful)/len(results)*100:.1f}%\n\n")
            
            # æˆåŠŸçš„ç»„åˆ
            if successful:
                f.write("æˆåŠŸåˆ†æçš„ç»„åˆ:\n")
                f.write("-"*30 + "\n")
                for result in successful:
                    f.write(f"  å®éªŒ{result['experiment']} - {result['sensor']}\n")
                f.write("\n")
            
            # å¤±è´¥çš„ç»„åˆ
            if failed:
                f.write("å¤±è´¥åˆ†æçš„ç»„åˆ:\n")
                f.write("-"*30 + "\n")
                for result in failed:
                    f.write(f"  å®éªŒ{result['experiment']} - {result['sensor']}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}\n")
                f.write("\n")
            
            f.write("è¾“å‡ºæ–‡ä»¶å‘½åè§„åˆ™:\n")
            f.write("-"*25 + "\n")
            f.write(f"  å…¨é•¿åº¦ä¿¡å·å›¾: {self.subject_id}_exp<å®éªŒç¼–å·>_<ä¼ æ„Ÿå™¨>_full_length_signals.png\n")
            f.write(f"  åˆ†æ®µä¿¡å·å›¾: {self.subject_id}_exp<å®éªŒç¼–å·>_<ä¼ æ„Ÿå™¨>_segment_signals.png\n")
            f.write(f"  ç›¸å…³æ€§çƒ­åŠ›å›¾: {self.subject_id}_exp<å®éªŒç¼–å·>_<ä¼ æ„Ÿå™¨>_correlation_heatmap.png\n")
        
        print(f"ğŸ“ æ‰¹é‡åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ PPG-ABPä¿¡å·å¯è§†åŒ–åˆ†æç³»ç»Ÿ")
    print("="*60)
    
    # é€‰æ‹©åˆ†ææ¨¡å¼
    print("è¯·é€‰æ‹©åˆ†ææ¨¡å¼:")
    print("1. å•ä¸ªåˆ†æ (æŒ‡å®šå®éªŒå’Œä¼ æ„Ÿå™¨)")
    print("2. æ‰¹é‡åˆ†æ (æ‰€æœ‰å®éªŒå’Œä¼ æ„Ÿå™¨)")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
    
    if choice == "2":
        # æ‰¹é‡åˆ†æ
        batch_analyzer = BatchAnalyzer(subject_id="00017")
        batch_analyzer.run_batch_analysis()
    else:
        # å•ä¸ªåˆ†æ
        print("\nå•ä¸ªåˆ†ææ¨¡å¼:")
        experiment = input("è¯·è¾“å…¥å®éªŒç¼–å· (é»˜è®¤1): ").strip() or "1"
        sensor = input("è¯·è¾“å…¥ä¼ æ„Ÿå™¨åç§° (é»˜è®¤sensor2): ").strip() or "sensor2"
        
        print(f"\nå¼€å§‹åˆ†æ: å®éªŒ{experiment} - {sensor}")
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = PPGABPAnalyzer(subject_id="00017", experiment=experiment, sensor_name=sensor)
        
        # è¿è¡Œå¯è§†åŒ–åˆ†æ
        analyzer.run_visualization_analysis(
            segment_length=2000,  # åˆ†æ®µé•¿åº¦
            start_idx=None        # è‡ªåŠ¨é€‰æ‹©èµ·å§‹ä½ç½®
        )

if __name__ == "__main__":
    main()
