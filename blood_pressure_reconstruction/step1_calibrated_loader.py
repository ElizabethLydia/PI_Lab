#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºstep1_preprocess.pyçš„æ ¡å‡†æ•°æ®åŠ è½½å™¨
å®Œå…¨ç…§æ¬step1é€»è¾‘ï¼Œåªæ˜¯BPæ•°æ®æºæ¢æˆæ ¡å‡†åçš„æ•°æ®é›†
å®ç°åˆ†å±‚å­˜å‚¨ï¼šcsvã€pklã€npyã€processing_logsã€summary_reports
æ”¯æŒ8æ ¸å¹¶è¡Œå¤„ç†
"""

import os
import numpy as np
import pandas as pd
import pickle
from tqdm import tqdm
from scipy.interpolate import interp1d
import warnings
import time
import multiprocessing
import warnings
from multiprocessing import Manager, Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import threading
warnings.filterwarnings('ignore')

# å…¨å±€é…ç½®
N_CORES = 2  # å¹¶è¡Œæ ¸å¿ƒæ•°
MAX_WORKERS = min(N_CORES, cpu_count())  # å®é™…ä½¿ç”¨çš„æ ¸å¿ƒæ•°

def interpolate_duplicate_timestamps(df, time_col='timestamp'):
    """
    æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³ - ç”¨äºHUBæ•°æ®ä¿æŒç²¾åº¦
    """
    if df.empty or time_col not in df.columns:
        return df
    
    df = df.copy()
    unique_times = df[time_col].unique()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
    if len(unique_times) == len(df):
        return df
    
    print(f"      æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³...")
    
    new_timestamps = []
    for t in unique_times:
        indices = df[df[time_col] == t].index
        n_points = len(indices)

        if n_points > 1:
            current_idx = np.where(unique_times == t)[0][0]
            if current_idx == len(unique_times) - 1:
                delta = 0.001  # æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ä½¿ç”¨é»˜è®¤å°é—´éš”
            else:
                next_t = unique_times[current_idx + 1]
                delta = (next_t - t) / n_points

            for i, idx in enumerate(indices):
                new_timestamps.append((t + i * delta, idx))
        else:
            new_timestamps.append((t, indices[0]))

    # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—æ—¶é—´æˆ³
    new_timestamps.sort(key=lambda x: x[1])
    df[time_col] = [t for t, _ in new_timestamps]
    
    duplicates_fixed = len(df) - len(unique_times)
    if duplicates_fixed > 0:
        print(f"      ä¿®å¤äº† {duplicates_fixed} ä¸ªé‡å¤æ—¶é—´æˆ³")
    
    return df

def quick_downsample_biopac(df, time_col='timestamp', target_freq=100):
    """å¯¹Biopacé«˜é¢‘æ•°æ®é™é‡‡æ ·"""
    if df.empty or time_col not in df.columns:
        return df
    
    original_len = len(df)
    
    if len(df) > 1:
        time_range = df[time_col].max() - df[time_col].min()
        current_freq = len(df) / time_range if time_range > 0 else 1
        
        print(f"      ä¼°ç®—é¢‘ç‡: {current_freq:.1f}Hz -> ç›®æ ‡: {target_freq}Hz")
        
        if current_freq > target_freq * 1.5:  # åªæœ‰æ˜æ˜¾é«˜é¢‘æ‰é™é‡‡æ ·
            step = max(1, int(current_freq / target_freq))
            result = df.iloc[::step].copy()
            print(f"      é™é‡‡æ ·: {original_len:,} -> {len(result):,} è¡Œ (æ­¥é•¿: {step})")
            return result
        else:
            print(f"      é¢‘ç‡é€‚ä¸­ï¼Œè·³è¿‡é™é‡‡æ ·")
    
    return df

def interpolate_with_reftime(time, data, reftime):
    """
    ä½¿ç”¨æ’å€¼å¯¹é½åˆ°å‚è€ƒæ—¶é—´æˆ³
    """
    if len(time) < 2 or len(data) < 2:
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])
    
    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    time = np.asarray(time, dtype=float)
    reftime = np.asarray(reftime, dtype=float)
    
    # é™åˆ¶æ’å€¼èŒƒå›´åˆ°æ•°æ®å®é™…èŒƒå›´å†…
    min_time, max_time = time.min(), time.max()
    valid_reftime_mask = (reftime >= min_time) & (reftime <= max_time)
    valid_reftime = reftime[valid_reftime_mask]
    
    if len(valid_reftime) == 0:
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])
    
    try:
        # ä½¿ç”¨çº¿æ€§æ’å€¼
        interp_func = interp1d(time, data, axis=0, kind='linear', bounds_error=False, fill_value=np.nan)
        interpolated_data = interp_func(valid_reftime)
        
        # åˆ›å»ºå®Œæ•´ç»“æœDataFrame
        if interpolated_data.ndim == 1:
            interpolated_data = interpolated_data.reshape(-1, 1)
        
        # åˆ›å»ºä¸å‚è€ƒæ—¶é—´é•¿åº¦ç›¸åŒçš„ç»“æœ
        full_result = np.full((len(reftime), interpolated_data.shape[1]), np.nan)
        full_result[valid_reftime_mask] = interpolated_data
        
        result_df = pd.DataFrame(full_result, columns=data.columns if hasattr(data, 'columns') else [f'col_{i}' for i in range(full_result.shape[1])])
        result_df['timestamp'] = reftime
        
        return result_df
    except Exception as e:
        print(f"        æ’å€¼é”™è¯¯: {e}")
        return pd.DataFrame(columns=data.columns if hasattr(data, 'columns') else ['value'])

def process_biopac_file(file_path, target_freq=100):
    """å¤„ç†Biopacæ–‡ä»¶ - å…ˆé™é‡‡æ ·å†æ’å€¼å¤„ç†é‡å¤"""
    try:
        file_name = os.path.basename(file_path)
        try:
            data = pd.read_csv(file_path)
        except UnicodeDecodeError:
            data = pd.read_csv(file_path, encoding='latin1')
        original_rows = len(data)
        
        if data.empty:
            return pd.DataFrame()
        
        print(f"    Biopac {file_name} ({original_rows:,} è¡Œ)")
        
        # 1. å…ˆé™é‡‡æ ·åˆ°ç›®æ ‡é¢‘ç‡ï¼ˆä¸å»é‡ï¼‰
        data = quick_downsample_biopac(data, target_freq=target_freq)
        
        # 2. å†ç”¨æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³
        data = interpolate_duplicate_timestamps(data)
        
        # 3. æ’åº
        if data is not None and not data.empty and 'timestamp' in data.columns:
            data = data.sort_values('timestamp').reset_index(drop=True)
        
        final_rows = len(data) if data is not None else 0
        compression_ratio = original_rows / final_rows if final_rows > 0 else 1
        print(f"      æœ€ç»ˆ: {final_rows:,} è¡Œ (å‹ç¼©æ¯”: {compression_ratio:.1f}:1)")
        
        return data if data is not None else pd.DataFrame()
        
    except Exception as e:
        print(f"      é”™è¯¯: {e}")
        return pd.DataFrame()

def load_calibrated_bp_data(subject_id, experiment_number):
    """åŠ è½½æ ¡å‡†åçš„è¡€å‹æ•°æ®"""
    calibrated_dir = "/root/shared/PhysioNet2025_Calibrated"
    bp_file = os.path.join(calibrated_dir, subject_id, str(experiment_number), "bp.csv")
    
    if not os.path.exists(bp_file):
        print(f"  æ ¡å‡†è¡€å‹æ–‡ä»¶ä¸å­˜åœ¨: {bp_file}")
        return None, "calibrated_not_found"
    
    try:
        # è¯»å–è¡€å‹æ•°æ®ï¼ˆç¬¬ä¸€åˆ—æ—¶é—´æˆ³ï¼Œç¬¬äºŒåˆ—è¡€å‹å€¼ï¼‰
        bp_data = pd.read_csv(bp_file, header=None, names=['abp', 'timestamp'])
        # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šæ—¶é—´æˆ³åœ¨å‰ï¼Œè¡€å‹å€¼åœ¨å
        bp_data = bp_data[['timestamp', 'abp']]
        print(f"  åŠ è½½æ ¡å‡†è¡€å‹æ•°æ®: {len(bp_data)} è¡Œ")
        return bp_data, "calibrated"
    except Exception as e:
        print(f"  åŠ è½½æ ¡å‡†è¡€å‹æ•°æ®å¤±è´¥: {e}")
        return None, "calibrated_error"

def load_original_bp_data(experiment_path, subject_id, experiment_number):
    """åŠ è½½åŸå§‹æ•°æ®é›†ä¸­çš„BPæ•°æ®"""
    biopac_path = os.path.join(experiment_path, 'Biopac')
    if not os.path.exists(biopac_path):
        print(f"  åŸå§‹Biopacç›®å½•ä¸å­˜åœ¨: {biopac_path}")
        return None, "original_not_found"
    
    # æŸ¥æ‰¾bpç›¸å…³çš„æ–‡ä»¶
    bp_files = []
    for file in os.listdir(biopac_path):
        if file.endswith('.csv') and ('bp' in file.lower() or 'blood' in file.lower()):
            bp_files.append(file)
    
    if not bp_files:
        print(f"  åŸå§‹Biopacç›®å½•ä¸­æœªæ‰¾åˆ°BPç›¸å…³æ–‡ä»¶")
        return None, "original_no_bp_files"
    
    # ä¼˜å…ˆé€‰æ‹©bp.csvï¼Œç„¶åæ˜¯å…¶ä»–åŒ…å«bpçš„æ–‡ä»¶
    bp_file = None
    for file in bp_files:
        if file.lower() == 'bp.csv':
            bp_file = file
            break
    if bp_file is None:
        bp_file = bp_files[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„BPæ–‡ä»¶
    
    try:
        file_path = os.path.join(biopac_path, bp_file)
        data = pd.read_csv(file_path)
        
        # æ£€æŸ¥åˆ—åï¼Œæ‰¾åˆ°è¡€å‹åˆ—
        bp_column = None
        for col in data.columns:
            if 'bp' in col.lower() or 'blood' in col.lower():
                bp_column = col
                break
        
        if bp_column is None:
            print(f"  åœ¨{file_path}ä¸­æœªæ‰¾åˆ°è¡€å‹åˆ—")
            return None, "original_no_bp_column"
        
        # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„è¡€å‹æ•°æ®
        bp_data = pd.DataFrame({
            'timestamp': data['timestamp'],
            'abp': data[bp_column]
        })
        
        print(f"  åŠ è½½åŸå§‹è¡€å‹æ•°æ®: {file_path} ({len(bp_data)} è¡Œ, åˆ—: {bp_column})")
        return bp_data, "original"
        
    except Exception as e:
        print(f"  åŠ è½½åŸå§‹è¡€å‹æ•°æ®å¤±è´¥: {e}")
        return None, "original_error"

def process_hub_file(file_path):
    """å¤„ç†HUBæ–‡ä»¶ - æ’å€¼ç­–ç•¥"""
    try:
        file_name = os.path.basename(file_path)
        try:
            data = pd.read_csv(file_path)
        except UnicodeDecodeError:
            data = pd.read_csv(file_path, encoding='latin1')
        original_rows = len(data)
        
        if data.empty:
            return pd.DataFrame()
        
        print(f"    HUB {file_name} ({original_rows:,} è¡Œ)")
        
        # 1. æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³ï¼ˆä¿æŒHUBæ•°æ®ç²¾åº¦ï¼‰
        processed_data = interpolate_duplicate_timestamps(data)
        
        # 2. æ’åº
        if processed_data is not None and not processed_data.empty and 'timestamp' in processed_data.columns:
            processed_data = processed_data.sort_values('timestamp').reset_index(drop=True)
        
        final_rows = len(processed_data) if processed_data is not None else 0
        print(f"      æœ€ç»ˆ: {final_rows:,} è¡Œ")
        
        return processed_data if processed_data is not None else pd.DataFrame()
        
    except Exception as e:
        print(f"      é”™è¯¯: {e}")
        return pd.DataFrame()

def load_experiment_smart(experiment_path, target_freq=100):
    """æ™ºèƒ½åŠ è½½å•ä¸ªå®éªŒ"""
    experiment_name = os.path.basename(experiment_path)
    print(f"\n{'='*50}")
    print(f"å¤„ç†å®éªŒ {experiment_name}")
    print(f"{'='*50}")
    
    result = {'biopac': {}, 'hub': {}}
    
    # å¤„ç†Biopacæ•°æ® (é«˜é¢‘ï¼Œå…ˆé™é‡‡æ ·å†æ’å€¼å¤„ç†é‡å¤)
    biopac_path = os.path.join(experiment_path, 'Biopac')
    if os.path.isdir(biopac_path):
        biopac_files = [f for f in os.listdir(biopac_path) if f.endswith('.csv')]
        print(f"\nBiopacæ–‡ä»¶ ({len(biopac_files)} ä¸ª) - é™é‡‡æ ·+æ’å€¼ç­–ç•¥:")
        
        for file in biopac_files:
            file_path = os.path.join(biopac_path, file)
            data = process_biopac_file(file_path, target_freq)
            if data is not None and not data.empty:
                key = file.split('-')[0] if '-' in file else file.replace('.csv', '')
                result['biopac'][key] = data
    
    # å¤„ç†HUBæ•°æ® (ä½é¢‘ï¼Œç”¨æ’å€¼)
    hub_path = os.path.join(experiment_path, 'HUB')
    if os.path.isdir(hub_path):
        hub_files = [f for f in os.listdir(hub_path) if f.endswith('.csv')]
        print(f"\nHUBæ–‡ä»¶ ({len(hub_files)} ä¸ª) - ä½¿ç”¨æ’å€¼ç­–ç•¥:")
        
        for file in hub_files:
            file_path = os.path.join(hub_path, file)
            data = process_hub_file(file_path)
            if not data.empty:
                key = file.replace('.csv', '')
                result['hub'][key] = data
    
    # ç»Ÿè®¡ç»“æœ
    biopac_count = len(result['biopac'])
    hub_count = len(result['hub'])
    print(f"\nå®éªŒ {experiment_name} å®Œæˆ: {biopac_count} ä¸ªBiopacæ–‡ä»¶, {hub_count} ä¸ªHUBæ–‡ä»¶")
    
    return result

def load_experiment_with_calibrated_bp(experiment_path, subject_id, experiment_number, target_freq=100):
    """æ™ºèƒ½åŠ è½½å•ä¸ªå®éªŒï¼Œä¼˜å…ˆä½¿ç”¨æ ¡å‡†åçš„è¡€å‹æ•°æ®ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®"""
    experiment_name = os.path.basename(experiment_path)
    print(f"\n{'='*50}")
    print(f"å¤„ç†å®éªŒ {experiment_name} (ä¼˜å…ˆæ ¡å‡†è¡€å‹æ•°æ®)")
    print(f"{'='*50}")
    
    result = {'biopac': {}, 'hub': {}}
    bp_source = "none"
    bp_status = "failed"
    
    # 1. ä¼˜å…ˆå°è¯•åŠ è½½æ ¡å‡†åçš„è¡€å‹æ•°æ®
    bp_data, calibrated_status = load_calibrated_bp_data(subject_id, experiment_number)
    if bp_data is not None:
        # å¤„ç†æ ¡å‡†è¡€å‹æ•°æ®ï¼ˆé™é‡‡æ ·å’Œæ’å€¼ï¼‰
        bp_processed = process_biopac_file_dataframe(bp_data, target_freq)
        if bp_processed is not None and not bp_processed.empty:
            result['biopac']['bp'] = bp_processed
            bp_source = "calibrated"
            bp_status = "success"
            print(f"  âœ… ä½¿ç”¨æ ¡å‡†è¡€å‹æ•°æ®: {len(bp_processed)} è¡Œ")
        else:
            print(f"  âŒ æ ¡å‡†è¡€å‹æ•°æ®å¤„ç†å¤±è´¥")
            bp_status = "processing_failed"
    else:
        print(f"  âš ï¸  æ ¡å‡†è¡€å‹æ•°æ®ä¸å¯ç”¨: {calibrated_status}")
        
        # 2. å›é€€åˆ°åŸå§‹æ•°æ®é›†ä¸­çš„è¡€å‹æ•°æ®
        print(f"  ğŸ”„ å°è¯•åŠ è½½åŸå§‹è¡€å‹æ•°æ®...")
        bp_data, original_status = load_original_bp_data(experiment_path, subject_id, experiment_number)
        if bp_data is not None:
            # å¤„ç†åŸå§‹è¡€å‹æ•°æ®ï¼ˆé™é‡‡æ ·å’Œæ’å€¼ï¼‰
            bp_processed = process_biopac_file_dataframe(bp_data, target_freq)
            if bp_processed is not None and not bp_processed.empty:
                result['biopac']['bp'] = bp_processed
                bp_source = "original"
                bp_status = "success"
                print(f"  âœ… ä½¿ç”¨åŸå§‹è¡€å‹æ•°æ®: {len(bp_processed)} è¡Œ")
            else:
                print(f"  âŒ åŸå§‹è¡€å‹æ•°æ®å¤„ç†å¤±è´¥")
                bp_status = "processing_failed"
        else:
            print(f"  âŒ åŸå§‹è¡€å‹æ•°æ®ä¹Ÿä¸å¯ç”¨: {original_status}")
            bp_status = "all_failed"
    
    # å¤„ç†HUBæ•°æ® (ä½é¢‘ï¼Œç”¨æ’å€¼)
    hub_path = os.path.join(experiment_path, 'HUB')
    if os.path.isdir(hub_path):
        hub_files = [f for f in os.listdir(hub_path) if f.endswith('.csv')]
        print(f"\nHUBæ–‡ä»¶ ({len(hub_files)} ä¸ª) - ä½¿ç”¨æ’å€¼ç­–ç•¥:")
        
        for file in hub_files:
            file_path = os.path.join(hub_path, file)
            data = process_hub_file(file_path)
            if not data.empty:
                key = file.replace('.csv', '')
                result['hub'][key] = data
    
    # ç»Ÿè®¡ç»“æœ
    biopac_count = len(result['biopac'])
    hub_count = len(result['hub'])
    print(f"\nå®éªŒ {experiment_name} å®Œæˆ: {biopac_count} ä¸ªBiopacæ–‡ä»¶, {hub_count} ä¸ªHUBæ–‡ä»¶")
    
    # è¿”å›ç»“æœå’ŒBPæ•°æ®æºä¿¡æ¯
    return result, bp_source, bp_status

def process_biopac_file_dataframe(df, target_freq=100):
    """å¤„ç†DataFrameæ ¼å¼çš„Biopacæ•°æ®"""
    if df.empty:
        return df
    
    original_rows = len(df)
    
    # 1. å…ˆé™é‡‡æ ·åˆ°ç›®æ ‡é¢‘ç‡
    df = quick_downsample_biopac(df, target_freq=target_freq)
    
    # 2. å†ç”¨æ’å€¼å¤„ç†é‡å¤æ—¶é—´æˆ³
    df = interpolate_duplicate_timestamps(df)
    
    # 3. æ’åº
    if df is not None and not df.empty and 'timestamp' in df.columns:
        df = df.sort_values('timestamp').reset_index(drop=True)
    
    final_rows = len(df) if df is not None else 0
    compression_ratio = original_rows / final_rows if final_rows > 0 else 1
    print(f"      æ ¡å‡†è¡€å‹æ•°æ®å¤„ç†: {original_rows:,} -> {final_rows:,} è¡Œ (å‹ç¼©æ¯”: {compression_ratio:.1f}:1)")
    
    return df if df is not None else pd.DataFrame()

def align_data_with_interpolation(data_dict, output_base_dir, subject):
    """ä½¿ç”¨æ’å€¼è¿›è¡Œç²¾ç¡®æ•°æ®å¯¹é½ï¼Œå¹¶åˆ†å±‚å­˜å‚¨"""
    aligned_data = {}
    
    # åˆ›å»ºåˆ†å±‚å­˜å‚¨ç›®å½•ç»“æ„
    csv_dir = os.path.join(output_base_dir, 'csv')
    pkl_dir = os.path.join(output_base_dir, 'pkl')
    npy_dir = os.path.join(output_base_dir, 'npy')
    
    os.makedirs(csv_dir, exist_ok=True)
    os.makedirs(pkl_dir, exist_ok=True)
    os.makedirs(npy_dir, exist_ok=True)
    
    print(f"\n{'='*50}")
    print("æ’å€¼å¯¹é½é˜¶æ®µ - åˆ†å±‚å­˜å‚¨")
    print(f"{'='*50}")
    
    for experiment_name, experiment_data in data_dict.items():
        try:
            print(f"\nå¯¹é½å®éªŒ {experiment_name}...")
            
            # æŸ¥æ‰¾å‚è€ƒæ—¶é—´åºåˆ—ï¼ˆä¼˜å…ˆä½¿ç”¨sensor2ï¼‰
            ref_data = None
            ref_name = ""
            
            try:
                hub_sensor2 = experiment_data['hub'].get('sensor2', pd.DataFrame())
                if not hub_sensor2.empty and 'timestamp' in hub_sensor2.columns:
                    ref_data = hub_sensor2
                    ref_name = "sensor2"
            except KeyError:
                print(f"  è­¦å‘Š: sensor2 ç¼ºå°‘ timestamp åˆ—")
            
            if ref_data is None:
                min_len = float('inf')
                for data_type in ['hub', 'biopac']:
                    for key, data in experiment_data[data_type].items():
                        if isinstance(data, pd.DataFrame) and not data.empty and 'timestamp' in data.columns:
                            if len(data) < min_len:
                                min_len = len(data)
                                ref_data = data
                                ref_name = f"{data_type}_{key}"
            
            if ref_data is None or ref_data.empty or 'timestamp' not in ref_data.columns:
                print(f"  è­¦å‘Š: experiment {experiment_name} æ— æœ‰æ•ˆå‚è€ƒæ•°æ®æˆ–ç¼ºå°‘ timestamp åˆ—ï¼Œè·³è¿‡")
                continue
            
            try:
                ref_timestamps = ref_data['timestamp'].values
            except KeyError:
                print(f"  é”™è¯¯: å‚è€ƒæ•°æ®ç¼ºå°‘ timestamp åˆ—ï¼Œè·³è¿‡ experiment {experiment_name}")
                continue
        
            print(f"  ä½¿ç”¨ {ref_name} ä½œä¸ºå‚è€ƒ ({len(ref_data):,} è¡Œ)")
            aligned_experiment = {'biopac': {}, 'hub': {}}
            
            # æ’å€¼å¯¹é½æ‰€æœ‰æ•°æ®
            for data_type in ['biopac', 'hub']:
                for key, data in experiment_data[data_type].items():
                    if isinstance(data, pd.DataFrame) and not data.empty and 'timestamp' in data.columns:
                        print(f"    å¯¹é½ {data_type}_{key}...")
                        
                        # æå–éœ€è¦æ’å€¼çš„åˆ—ï¼ˆé™¤äº†timestampï¼‰
                        data_columns = [col for col in data.columns if col != 'timestamp']
                        if data_columns:
                            # ä½¿ç”¨æ’å€¼å¯¹é½
                            interpolated_data = interpolate_with_reftime(
                                data['timestamp'].values,
                                data[data_columns].values,
                                ref_timestamps
                            )
                            
                            if not interpolated_data.empty:
                                # é‡æ–°è®¾ç½®åˆ—å
                                interpolated_data.columns = data_columns + ['timestamp']
                                aligned_experiment[data_type][key] = interpolated_data
                                print(f"      å¯¹é½å®Œæˆ: {len(data):,} -> {len(interpolated_data):,} è¡Œ")
                            else:
                                print(f"      å¯¹é½å¤±è´¥ï¼Œè·³è¿‡")
            
            aligned_data[experiment_name] = aligned_experiment
            
            # åˆ†å±‚å­˜å‚¨æ•°æ®
            save_layered_data(experiment_name, aligned_experiment, csv_dir, pkl_dir, npy_dir, subject)
            
        except Exception as e:
            print(f'å¯¹é½ experiment {experiment_name} å¤±è´¥: {e}')
            continue
    
    return aligned_data

def save_layered_data(experiment_name, aligned_experiment, csv_dir, pkl_dir, npy_dir, subject):
    """åˆ†å±‚ä¿å­˜æ•°æ®åˆ°ä¸åŒæ ¼å¼"""
    print(f"  åˆ†å±‚å­˜å‚¨å®éªŒ {experiment_name}...")
    
    # 1. ä¿å­˜CSVæ ¼å¼ï¼ˆä¸»è¦ä½¿ç”¨ï¼‰
    save_csv_data(experiment_name, aligned_experiment, csv_dir, subject)
    
    # 2. ä¿å­˜PKLæ ¼å¼ï¼ˆå¿«é€ŸåŠ è½½ï¼‰
    save_pkl_data(experiment_name, aligned_experiment, pkl_dir, subject)
    
    # 3. ä¿å­˜NPYæ ¼å¼ï¼ˆæ•°å€¼è®¡ç®—ï¼‰
    save_npy_data(experiment_name, aligned_experiment, npy_dir, subject)

def save_csv_data(experiment_name, aligned_experiment, csv_dir, subject):
    """ä¿å­˜CSVæ ¼å¼æ•°æ®"""
    print(f"    ä¿å­˜CSVæ ¼å¼...")
    
    # ä¿å­˜HUBæ•°æ®ä¸ºç‹¬ç«‹CSVæ–‡ä»¶
    for key, df in aligned_experiment['hub'].items():
        if isinstance(df, pd.DataFrame) and not df.empty:
            columns = ['timestamp'] + [col for col in df.columns if col != 'timestamp']
            df_reordered = df[columns]
            hub_csv_path = os.path.join(csv_dir, f'{subject}_{experiment_name}_{key}.csv')
            df_reordered.to_csv(hub_csv_path, index=False)
            print(f"      ä¿å­˜HUB CSV: {hub_csv_path}")
    
    # ä¿å­˜è¡€å‹æ•°æ®CSV
    if 'bp' in aligned_experiment['biopac']:
        bp_df = aligned_experiment['biopac']['bp']
        if isinstance(bp_df, pd.DataFrame) and not bp_df.empty:
            # ç¡®ä¿æ—¶é—´æˆ³åœ¨ç¬¬ä¸€åˆ—
            columns = ['timestamp'] + [col for col in bp_df.columns if col != 'timestamp']
            bp_df_reordered = bp_df[columns]
            bp_csv_path = os.path.join(csv_dir, f'{subject}_{experiment_name}_abp.csv')
            bp_df_reordered.to_csv(bp_csv_path, index=False)
            print(f"      ä¿å­˜è¡€å‹CSV: {bp_csv_path}")

def save_pkl_data(experiment_name, aligned_experiment, pkl_dir, subject):
    """ä¿å­˜PKLæ ¼å¼æ•°æ®"""
    print(f"    ä¿å­˜PKLæ ¼å¼...")
    
    # ä¿å­˜æ¯ä¸ªä¼ æ„Ÿå™¨/æ•°æ®ç±»å‹çš„ç‹¬ç«‹PKLæ–‡ä»¶
    for data_type in ['hub', 'biopac']:
        for key, df in aligned_experiment[data_type].items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                # ç‰¹æ®Šå¤„ç†è¡€å‹æ•°æ®ï¼Œæ”¹åä¸ºabp
                if key == 'bp':
                    pkl_path = os.path.join(pkl_dir, f'{subject}_{experiment_name}_abp.pkl')
                else:
                    pkl_path = os.path.join(pkl_dir, f'{subject}_{experiment_name}_{key}.pkl')
                with open(pkl_path, 'wb') as f:
                    pickle.dump(df, f)
                print(f"      ä¿å­˜PKL: {pkl_path}")

def save_npy_data(experiment_name, aligned_experiment, npy_dir, subject):
    """ä¿å­˜NPYæ ¼å¼æ•°æ®"""
    print(f"    ä¿å­˜NPYæ ¼å¼...")
    
    # ä¿å­˜æ¯ä¸ªä¼ æ„Ÿå™¨/æ•°æ®ç±»å‹çš„ç‹¬ç«‹NPYæ–‡ä»¶
    for data_type in ['hub', 'biopac']:
        for key, df in aligned_experiment[data_type].items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                # åªä¿å­˜æ•°å€¼åˆ—ï¼Œä¸ä¿å­˜timestamp
                numeric_columns = [col for col in df.columns if col != 'timestamp']
                if numeric_columns:
                    numeric_data = df[numeric_columns].values
                    # ç‰¹æ®Šå¤„ç†è¡€å‹æ•°æ®ï¼Œæ”¹åä¸ºabp
                    if key == 'bp':
                        npy_path = os.path.join(npy_dir, f'{subject}_{experiment_name}_abp.npy')
                    else:
                        npy_path = os.path.join(npy_dir, f'{subject}_{experiment_name}_{key}.npy')
                    np.save(npy_path, numeric_data)
                    print(f"      ä¿å­˜NPY: {npy_path}")

def process_experiment_parallel(args):
    """å¹¶è¡Œå¤„ç†å•ä¸ªå®éªŒçš„å‡½æ•°"""
    experiment_path, subject_id, experiment_number, target_freq = args
    
    try:
        # åŠ è½½å®éªŒæ•°æ®
        experiment_data, bp_source, bp_status = load_experiment_with_calibrated_bp(
            experiment_path, subject_id, experiment_number, target_freq
        )
        
        return {
            'experiment': experiment_number,
            'data': experiment_data,
            'bp_source': bp_source,
            'bp_status': bp_status,
            'success': True
        }
    except Exception as e:
        return {
            'experiment': experiment_number,
            'data': None,
            'bp_source': 'none',
            'bp_status': 'error',
            'success': False,
            'error': str(e)
        }

def process_subject_parallel(date_folder, subject):
    """å¹¶è¡Œå¤„ç†å•ä¸ªå—è¯•è€…çš„æ‰€æœ‰å®éªŒ"""
    dataset_root = '/root/shared/PhysioNet2025/'
    MAX_EXPERIMENTS = None
    TARGET_FREQ = 100
    
    date_path = os.path.join(dataset_root, date_folder)
    subject_path = os.path.join(date_path, subject)
    
    # æ–°çš„è¾“å‡ºç›®å½•ç»“æ„
    output_base_dir = os.path.join('/root/autodl-tmp/blood_pressure_reconstruction', subject)
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆè·³è¿‡å·²å¤„ç†çš„å—è¯•è€…ï¼‰
    status_file = os.path.join(output_base_dir, 'processing_logs', 'step1_calibrated_succ.txt')
    bp_report_file = os.path.join(output_base_dir, 'processing_logs', f'{subject}_bp_source_report.txt')
    csv_dir = os.path.join(output_base_dir, 'csv')
    
    # æ£€æŸ¥å¤šä¸ªæŒ‡æ ‡æ¥åˆ¤æ–­æ˜¯å¦å·²å¤„ç†
    already_processed = False
    skip_reason = ""
    
    # 1. æ£€æŸ¥æˆåŠŸçŠ¶æ€æ–‡ä»¶
    if os.path.exists(status_file):
        try:
            with open(status_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥çŠ¶æ€æ˜¯å¦ä¸ºSUCCESS
            if 'Status: SUCCESS' in content:
                already_processed = True
                skip_reason = "step1_calibrated_succ.txt exists with SUCCESS status"
        except Exception as e:
            print(f"âš ï¸  è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼Œç»§ç»­æ£€æŸ¥å…¶ä»–æŒ‡æ ‡: {e}")
    
    # 2. æ£€æŸ¥BPæºæŠ¥å‘Šæ–‡ä»¶
    if not already_processed and os.path.exists(bp_report_file):
        try:
            with open(bp_report_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å¦‚æœBPæºæŠ¥å‘Šå­˜åœ¨ï¼Œè¯´æ˜å·²ç»å¤„ç†è¿‡
            already_processed = True
            skip_reason = "bp_source_report.txt exists"
        except Exception as e:
            print(f"âš ï¸  è¯»å–BPæºæŠ¥å‘Šå¤±è´¥ï¼Œç»§ç»­æ£€æŸ¥å…¶ä»–æŒ‡æ ‡: {e}")
    
    # 3. æ£€æŸ¥CSVç›®å½•å’Œæ–‡ä»¶
    if not already_processed and os.path.exists(csv_dir):
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
        if len(csv_files) > 0:
            # å¦‚æœæœ‰CSVæ–‡ä»¶ï¼Œè¯´æ˜å·²ç»å¤„ç†è¿‡
            already_processed = True
            skip_reason = f"CSV directory exists with {len(csv_files)} files"
    
    # å¦‚æœå·²ç»å¤„ç†è¿‡ï¼Œåˆ™è·³è¿‡
    if already_processed:
        print(f"\n{'='*60}")
        print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„å—è¯•è€…: {subject}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_base_dir}")
        print(f"ğŸ” è·³è¿‡åŸå› : {skip_reason}")
        print(f"âœ… çŠ¶æ€: å·²å®Œæˆ")
        print(f"{'='*60}")
        
        # è¿”å›è·³è¿‡çŠ¶æ€
        return {
            'subject': subject,
            'status': 'skipped',
            'total_experiments': 0,
            'successful': 0,
            'failed': 0,
            'parallel_time': 0,
            'align_time': 0,
            'total_time': 0,
            'success_rate': 100.0,
            'skip_reason': skip_reason
        }
    
    # åˆ›å»ºä¸»ç›®å½•å’Œå­ç›®å½•
    os.makedirs(output_base_dir, exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¹¶è¡Œå¤„ç†å—è¯•è€…: {subject} åœ¨ {date_folder}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_base_dir}")
    print(f"âš¡ å¹¶è¡Œæ ¸å¿ƒæ•°: {MAX_WORKERS}")
    print(f"{'='*60}")
    
    all_folders = os.listdir(subject_path)
    experiment_folders = [f for f in all_folders if f.isdigit() and os.path.isdir(os.path.join(subject_path, f))]
    experiment_folders.sort(key=lambda x: int(x))
    
    if MAX_EXPERIMENTS:
        experiment_folders = experiment_folders[:MAX_EXPERIMENTS]
    
    print(f"ğŸ“‹ å‘ç°å®éªŒ: {experiment_folders}")
    print(f"ğŸ”¢ æ€»å®éªŒæ•°: {len(experiment_folders)}")
    
    # å‡†å¤‡å¹¶è¡Œå¤„ç†å‚æ•°
    parallel_args = []
    for experiment in experiment_folders:
        experiment_path = os.path.join(subject_path, experiment)
        parallel_args.append((experiment_path, subject, experiment, TARGET_FREQ))
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰å®éªŒ
    print(f"\nâš¡ å¼€å§‹å¹¶è¡Œå¤„ç† {len(experiment_folders)} ä¸ªå®éªŒ...")
    parallel_start = time.time()
    
    all_data = {}
    bp_source_info = {}
    successful_experiments = 0
    failed_experiments = 0
    
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_experiment = {executor.submit(process_experiment_parallel, args): args for args in parallel_args}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(parallel_args), desc="å¹¶è¡Œå¤„ç†å®éªŒ", unit="å®éªŒ") as pbar:
            for future in as_completed(future_to_experiment):
                result = future.result()
                experiment_num = result['experiment']
                
                if result['success']:
                    all_data[experiment_num] = result['data']
                    bp_source_info[experiment_num] = {
                        'source': result['bp_source'],
                        'status': result['bp_status']
                    }
                    successful_experiments += 1
                    pbar.set_postfix({
                        'æˆåŠŸ': successful_experiments,
                        'å¤±è´¥': failed_experiments,
                        'å½“å‰': f"å®éªŒ{experiment_num}"
                    })
                else:
                    failed_experiments += 1
                    print(f"âŒ å®éªŒ {experiment_num} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    pbar.set_postfix({
                        'æˆåŠŸ': successful_experiments,
                        'å¤±è´¥': failed_experiments,
                        'å½“å‰': f"å®éªŒ{experiment_num}"
                    })
                
                pbar.update(1)
    
    parallel_time = time.time() - parallel_start
    
    # æ•°æ®å¯¹é½é˜¶æ®µ
    print(f"\nğŸ”„ å¼€å§‹æ•°æ®å¯¹é½é˜¶æ®µ...")
    align_start = time.time()
    aligned_data = align_data_with_interpolation(all_data, output_base_dir, subject)
    align_time = time.time() - align_start
    
    # ç»Ÿè®¡
    total_time = time.time() - parallel_start
    print(f"\n{'='*60}")
    print("ğŸ“Š å¹¶è¡Œå¤„ç†å®Œæˆç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"âœ… æˆåŠŸå¤„ç†å®éªŒ: {successful_experiments}")
    print(f"âŒ å¤±è´¥å®éªŒ: {failed_experiments}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {successful_experiments/(successful_experiments+failed_experiments)*100:.1f}%")
    print(f"âš¡ å¹¶è¡Œå¤„ç†è€—æ—¶: {parallel_time:.1f} ç§’")
    print(f"ğŸ”„ æ•°æ®å¯¹é½è€—æ—¶: {align_time:.1f} ç§’")
    print(f"â±ï¸  æ€»å¤„ç†è€—æ—¶: {total_time:.1f} ç§’")
    if successful_experiments > 0:
        print(f"ğŸ“Š å¹³å‡æ¯ä¸ªå®éªŒ: {total_time/successful_experiments:.1f} ç§’")
        print(f"ğŸš€ å¹¶è¡ŒåŠ é€Ÿæ¯”: {len(experiment_folders)*total_time/successful_experiments/total_time:.1f}x")
    
    for exp_name, exp_data in aligned_data.items():
        total_biopac = len(exp_data['biopac'])
        total_hub = len(exp_data['hub'])
        print(f"å®éªŒ {exp_name}: {total_biopac} ä¸ªBiopac + {total_hub} ä¸ªHUBæ–‡ä»¶")
    
    print(f"\nâœ… å¹¶è¡Œå¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
    print(f"   CSVæ ¼å¼: {output_base_dir}/csv/")
    print(f"   PKLæ ¼å¼: {output_base_dir}/pkl/")
    print(f"   NPYæ ¼å¼: {output_base_dir}/npy/")
    
    # ç”ŸæˆBPæ•°æ®æºè®°å½•
    generate_bp_source_report(subject, bp_source_info, output_base_dir)
    
    # ç”ŸæˆæˆåŠŸçŠ¶æ€æ–‡ä»¶
    create_success_status_file(subject, output_base_dir, len(experiment_folders), successful_experiments, failed_experiments, total_time)
    
    return {
        'subject': subject,
        'status': 'success',
        'total_experiments': len(experiment_folders),
        'successful': successful_experiments,
        'failed': failed_experiments,
        'parallel_time': parallel_time,
        'align_time': align_time,
        'total_time': total_time,
        'success_rate': successful_experiments/(successful_experiments+failed_experiments)*100
    }

def process_subject(date_folder, subject):
    """å…¼å®¹æ€§å‡½æ•°ï¼Œè°ƒç”¨å¹¶è¡Œç‰ˆæœ¬"""
    return process_subject_parallel(date_folder, subject)

def generate_bp_source_report(subject_id, bp_source_info, output_base_dir):
    """ç”ŸæˆBPæ•°æ®æºä½¿ç”¨è®°å½•"""
    # åˆ›å»ºprocessing_logsç›®å½•
    logs_dir = os.path.join(output_base_dir, 'processing_logs')
    os.makedirs(logs_dir, exist_ok=True)
    
    report_file = os.path.join(logs_dir, f'{subject_id}_bp_source_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"å—è¯•è€… {subject_id} BPæ•°æ®æºä½¿ç”¨è®°å½•\n")
        f.write("="*50 + "\n\n")
        f.write("å¤„ç†ç­–ç•¥: ä¼˜å…ˆä½¿ç”¨æ ¡å‡†è¡€å‹æ•°æ®ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®\n")
        f.write(f"å¹¶è¡Œæ ¸å¿ƒæ•°: {MAX_WORKERS}\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_experiments = len(bp_source_info)
        calibrated_count = sum(1 for info in bp_source_info.values() if info['source'] == 'calibrated')
        original_count = sum(1 for info in bp_source_info.values() if info['source'] == 'original')
        failed_count = sum(1 for info in bp_source_info.values() if info['status'] == 'all_failed')
        
        f.write(f"æ€»å®éªŒæ•°: {total_experiments}\n")
        f.write(f"ä½¿ç”¨æ ¡å‡†æ•°æ®: {calibrated_count}\n")
        f.write(f"ä½¿ç”¨åŸå§‹æ•°æ®: {original_count}\n")
        f.write(f"å®Œå…¨å¤±è´¥: {failed_count}\n\n")
        
        # è¯¦ç»†è®°å½•
        f.write("è¯¦ç»†è®°å½•:\n")
        f.write("-"*30 + "\n")
        for exp_num, info in sorted(bp_source_info.items()):
            status_emoji = {
                'success': 'âœ…',
                'processing_failed': 'âŒ',
                'all_failed': 'ğŸ’¥'
            }.get(info['status'], 'â“')
            
            source_desc = {
                'calibrated': 'æ ¡å‡†è¡€å‹æ•°æ®',
                'original': 'åŸå§‹è¡€å‹æ•°æ®',
                'none': 'æ— æ•°æ®'
            }.get(info['source'], 'æœªçŸ¥')
            
            f.write(f"å®éªŒ {exp_num}: {status_emoji} {source_desc} ({info['status']})\n")
        
        f.write(f"\næŠ¥å‘Šç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    print(f"BPæ•°æ®æºè®°å½•å·²ä¿å­˜åˆ°: {report_file}")

def create_success_status_file(subject_id, output_base_dir, total_experiments, successful_experiments, failed_experiments, total_time):
    """ç”ŸæˆæˆåŠŸå¤„ç†çš„çŠ¶æ€æ–‡ä»¶"""
    # åˆ›å»ºprocessing_logsç›®å½•
    logs_dir = os.path.join(output_base_dir, 'processing_logs')
    os.makedirs(logs_dir, exist_ok=True)
    
    status_file = os.path.join(logs_dir, 'step1_calibrated_succ.txt')
    
    with open(status_file, 'w', encoding='utf-8') as f:
        f.write(f"Step1 Calibrated å¤„ç†æˆåŠŸçŠ¶æ€\n")
        f.write("="*50 + "\n\n")
        f.write(f"Subject ID: {subject_id}\n")
        f.write(f"Status: SUCCESS\n")
        f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Output Directory: {output_base_dir}\n\n")
        
        f.write(f"Processing Summary:\n")
        f.write(f"- Total Experiments: {total_experiments}\n")
        f.write(f"- Successful: {successful_experiments}\n")
        f.write(f"- Failed: {failed_experiments}\n")
        f.write(f"- Success Rate: {successful_experiments/(successful_experiments+failed_experiments)*100:.1f}%\n")
        f.write(f"- Total Processing Time: {total_time:.1f} seconds\n\n")
        
        f.write(f"Output Structure:\n")
        f.write(f"- CSV files: {output_base_dir}/csv/\n")
        f.write(f"- PKL files: {output_base_dir}/pkl/\n")
        f.write(f"- NPY files: {output_base_dir}/npy/\n")
        f.write(f"- Processing logs: {output_base_dir}/processing_logs/\n\n")
        
        f.write(f"Note: This subject has been successfully processed.\n")
        f.write(f"Future runs will skip this subject to avoid reprocessing.\n")
    
    print(f"âœ… æˆåŠŸçŠ¶æ€æ–‡ä»¶å·²ç”Ÿæˆ: {status_file}")

def main():
    """ä¸»å‡½æ•° - å¹¶è¡Œå¤„ç†ç‰ˆæœ¬"""
    start_time = time.time()
    
    dataset_root = '/root/shared/PhysioNet2025/'
    
    # è®¾ç½®å‚æ•°
    TARGET_FREQ = 100  # ç›®æ ‡é¢‘ç‡
    MAX_EXPERIMENTS = None  # Noneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å®éªŒ
    
    # å¤„ç†æ‰€æœ‰å—è¯•è€…
    print(f"\n{'='*80}")
    print("ğŸš€ PPG-ABPé‡æ„æ•°æ®å¹¶è¡Œå¤„ç†ç³»ç»Ÿ")
    print(f"{'='*80}")
    print(f"âš¡ å¹¶è¡Œæ ¸å¿ƒæ•°: {MAX_WORKERS}")
    print(f"ğŸ¯ ç›®æ ‡: å¤„ç†æ‰€æœ‰å—è¯•è€…")
    print(f"ğŸ“Š ç›®æ ‡é¢‘ç‡: {TARGET_FREQ}Hz")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: /root/autodl-tmp/blood_pressure_reconstruction/")
    print(f"{'='*80}")
    
    # è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹
    date_folders = [f for f in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, f)) and f.startswith('20')]
    date_folders.sort()
    
    results = []  # ç”¨äºæ”¶é›†ç»“æœ
    total_subjects = 0
    total_experiments = 0
    total_successful = 0
    total_failed = 0
    all_subjects = []  # è®°å½•æ‰€æœ‰æ‰¾åˆ°çš„å—è¯•è€…
    
    # é¦–å…ˆæ”¶é›†æ‰€æœ‰å¯ç”¨çš„å—è¯•è€…
    print(f"\nğŸ” æ‰«ææ‰€æœ‰å¯ç”¨çš„å—è¯•è€…...")
    for date_folder in date_folders:
        date_path = os.path.join(dataset_root, date_folder)
        all_subject_folders = [f for f in os.listdir(date_path) if os.path.isdir(os.path.join(date_path, f)) and f.startswith('00')]
        all_subjects.extend(all_subject_folders)
    
    # å»é‡å¹¶æ’åº
    all_subjects = sorted(list(set(all_subjects)))
    print(f"ğŸ“‹ å‘ç°å—è¯•è€…: {all_subjects}")
    print(f"ğŸ”¢ æ€»å—è¯•è€…æ•°: {len(all_subjects)}")
    
    # å‡†å¤‡å¹¶è¡Œå¤„ç†å‚æ•°
    parallel_args = []
    for subject in all_subjects:
        # æ‰¾åˆ°åŒ…å«è¯¥å—è¯•è€…çš„æ—¥æœŸæ–‡ä»¶å¤¹
        subject_date_folder = None
        for date_folder in date_folders:
            subject_path = os.path.join(dataset_root, date_folder, subject)
            if os.path.exists(subject_path):
                subject_date_folder = date_folder
                break
        
        if subject_date_folder is None:
            print(f"âŒ å—è¯•è€… {subject} åœ¨æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹ä¸­éƒ½æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
            continue
            
        print(f"ğŸ“… åœ¨æ—¥æœŸæ–‡ä»¶å¤¹ {subject_date_folder} ä¸­æ‰¾åˆ°å—è¯•è€… {subject}")
        parallel_args.append((subject_date_folder, subject))
    
    print(f"\nâš¡ å¼€å§‹çœŸæ­£çš„è·¨å—è¯•è€…å¹¶è¡Œå¤„ç†...")
    print(f"ğŸ¯ å°†å¹¶è¡Œå¤„ç† {len(parallel_args)} ä¸ªå—è¯•è€…ï¼Œæ¯ä¸ªå—è¯•è€…å†…éƒ¨ä¹Ÿå¹¶è¡Œå¤„ç†å®éªŒ")
    
    # çœŸæ­£çš„è·¨å—è¯•è€…å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰å—è¯•è€…ä»»åŠ¡
        future_to_subject = {executor.submit(process_subject_parallel, date_folder, subject): (date_folder, subject) 
                           for date_folder, subject in parallel_args}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(parallel_args), desc="å¹¶è¡Œå¤„ç†å—è¯•è€…", unit="å—è¯•è€…") as pbar:
            for future in as_completed(future_to_subject):
                date_folder, subject = future_to_subject[future]
                try:
                    res = future.result()
                    results.append(res)
                    
                    if res and res.get('status') == 'skipped':
                        print(f"â­ï¸  å—è¯•è€… {subject} å·²è·³è¿‡ï¼ˆä¹‹å‰å·²å¤„ç†å®Œæˆï¼‰")
                    elif res and res.get('status') == 'success':
                        total_subjects += 1
                        total_experiments += res['total_experiments']
                        total_successful += res['successful']
                        total_failed += res['failed']
                    
                    pbar.set_postfix({
                        'æˆåŠŸ': total_subjects,
                        'è·³è¿‡': len([r for r in results if r and r.get('status') == 'skipped']),
                        'å®éªŒ': total_experiments,
                        'å½“å‰': f"{subject}"
                    })
                except Exception as e:
                    print(f"âŒ å—è¯•è€… {subject} å¤„ç†å¤±è´¥: {e}")
                
                pbar.update(1)
    
    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    skipped_subjects = len([r for r in results if r and r.get('status') == 'skipped'])
    
    print(f"\n{'='*80}")
    print("ğŸ“Š æ€»å¹¶è¡Œå¤„ç†å®Œæˆç»Ÿè®¡")
    print(f"{'='*80}")
    print(f"â±ï¸  æ€»å¤„ç†è€—æ—¶: {total_time:.1f} ç§’")
    print(f"ğŸ‘¥ å¤„ç†å—è¯•è€…: {total_subjects}")
    print(f"â­ï¸  è·³è¿‡å—è¯•è€…: {skipped_subjects}")
    print(f"ğŸ”¬ æ€»å®éªŒæ•°: {total_experiments}")
    print(f"âœ… æˆåŠŸå®éªŒ: {total_successful}")
    print(f"âŒ å¤±è´¥å®éªŒ: {total_failed}")
    if total_experiments > 0:
        print(f"ğŸ“ˆ æ€»ä½“æˆåŠŸç‡: {total_successful/total_experiments*100:.1f}%")
        print(f"ğŸš€ å¹³å‡æ¯ä¸ªå®éªŒ: {total_time/total_experiments:.1f} ç§’")
        print(f"ğŸš€ å¹³å‡æ¯ä¸ªå—è¯•è€…: {total_time/total_subjects:.1f} ç§’")
    
    # åˆ›å»ºsummary_reportsç›®å½•å’Œæ€»æŠ¥å‘Š
    create_summary_report(all_subjects, total_time, results, skipped_subjects)

def create_summary_report(subjects, total_time, results, skipped_subjects):
    """åˆ›å»ºæ€»å¤„ç†æŠ¥å‘Š - åŒ…å«å¹¶è¡Œå¤„ç†ç»Ÿè®¡"""
    summary_dir = '/root/autodl-tmp/blood_pressure_reconstruction/summary_reports'
    os.makedirs(summary_dir, exist_ok=True)
    
    report_file = os.path.join(summary_dir, f'parallel_processing_summary_{time.strftime("%Y%m%d_%H%M%S")}.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("PPG-ABPé‡æ„æ•°æ®å¹¶è¡Œå¤„ç†æ€»æŠ¥å‘Š\n")
        f.write("="*60 + "\n\n")
        f.write(f"å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»è€—æ—¶: {total_time:.1f} ç§’\n")
        f.write(f"å¹¶è¡Œæ ¸å¿ƒæ•°: {MAX_WORKERS}\n")
        f.write(f"å¤„ç†å—è¯•è€…: {', '.join(subjects)}\n\n")
        
        # æ€»ä½“ç»Ÿè®¡
        total_experiments = sum(r['total_experiments'] for r in results if r and r.get('status') == 'success')
        total_successful = sum(r['successful'] for r in results if r and r.get('status') == 'success')
        total_failed = sum(r['failed'] for r in results if r and r.get('status') == 'success')
        
        f.write("æ€»ä½“ç»Ÿè®¡:\n")
        f.write("-"*30 + "\n")
        f.write(f"æ€»å—è¯•è€…æ•°: {len(subjects)}\n")
        f.write(f"æ–°å¤„ç†å—è¯•è€…: {len([r for r in results if r and r.get('status') == 'success'])}\n")
        f.write(f"è·³è¿‡å—è¯•è€…: {skipped_subjects}\n")
        f.write(f"æ€»å®éªŒæ•°: {total_experiments}\n")
        f.write(f"æˆåŠŸå®éªŒ: {total_successful}\n")
        f.write(f"å¤±è´¥å®éªŒ: {total_failed}\n")
        if total_experiments > 0:
            f.write(f"æˆåŠŸç‡: {total_successful/total_experiments*100:.1f}%\n")
        
        f.write("\nå—è¯•è€…è¯¦ç»†ç»Ÿè®¡:\n")
        f.write("-"*40 + "\n")
        for result in results:
            if result:
                if result.get('status') == 'skipped':
                    f.write(f"å—è¯•è€… {result['subject']}: â­ï¸ å·²è·³è¿‡ï¼ˆä¹‹å‰å·²å¤„ç†å®Œæˆï¼‰\n")
                    f.write(f"  è·³è¿‡åŸå› : {result.get('skip_reason', 'unknown')}\n\n")
                else:
                    f.write(f"å—è¯•è€… {result['subject']}:\n")
                    f.write(f"  æ€»å®éªŒæ•°: {result['total_experiments']}\n")
                    f.write(f"  æˆåŠŸ: {result['successful']}\n")
                    f.write(f"  å¤±è´¥: {result['failed']}\n")
                    f.write(f"  æˆåŠŸç‡: {result['success_rate']:.1f}%\n")
                    f.write(f"  å¹¶è¡Œå¤„ç†æ—¶é—´: {result['parallel_time']:.1f}ç§’\n")
                    f.write(f"  å¯¹é½æ—¶é—´: {result['align_time']:.1f}ç§’\n")
                    f.write(f"  æ€»æ—¶é—´: {result['total_time']:.1f}ç§’\n\n")
        
        f.write("è¾“å‡ºç›®å½•ç»“æ„:\n")
        f.write("-"*30 + "\n")
        for subject in subjects:
            f.write(f"/root/autodl-tmp/blood_pressure_reconstruction/{subject}/\n")
            f.write(f"  â”œâ”€â”€ csv/          # CSVæ ¼å¼ï¼ˆä¸»è¦ä½¿ç”¨ï¼‰\n")
            f.write(f"  â”œâ”€â”€ pkl/          # Pickleæ ¼å¼ï¼ˆå¿«é€ŸåŠ è½½ï¼‰\n")
            f.write(f"  â”œâ”€â”€ npy/          # Numpyæ ¼å¼ï¼ˆæ•°å€¼è®¡ç®—ï¼‰\n")
            f.write(f"  â””â”€â”€ processing_logs/  # å¤„ç†æ—¥å¿—\n")
        
        f.write(f"\nè·³è¿‡æœºåˆ¶è¯´æ˜:\n")
        f.write("-"*30 + "\n")
        f.write(f"ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æŸ¥æ¯ä¸ªå—è¯•è€…çš„çŠ¶æ€æ–‡ä»¶:\n")
        f.write(f"  /root/autodl-tmp/blood_pressure_reconstruction/{{subject}}/processing_logs/step1_calibrated_succ.txt\n")
        f.write(f"å¦‚æœçŠ¶æ€ä¸º 'Status: SUCCESS'ï¼Œåˆ™è·³è¿‡è¯¥å—è¯•è€…ä»¥é¿å…é‡å¤å¤„ç†ã€‚\n")
        f.write(f"æœ¬æ‰¹æ¬¡è·³è¿‡äº† {skipped_subjects} ä¸ªå·²å¤„ç†çš„å—è¯•è€…ã€‚\n\n")
        
        f.write(f"æ€»æŠ¥å‘Šä½ç½®: {summary_dir}\n")
    
    print(f"å¹¶è¡Œå¤„ç†æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # åˆ›å»ºç±»ä¼¼step3çš„è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
    create_detailed_status_report(subjects, results, skipped_subjects)

def create_detailed_status_report(subjects, results, skipped_subjects):
    """åˆ›å»ºè¯¦ç»†çš„å¤„ç†çŠ¶æ€æŠ¥å‘Šï¼ˆç±»ä¼¼step3ï¼‰"""
    print("ğŸ“Š åˆ›å»ºstep1_calibratedå¤„ç†çŠ¶æ€æ±‡æ€»æŠ¥å‘Š")
    print("="*50)
    
    # åˆ›å»ºä¸“é—¨çš„æ£€æŸ¥æ–‡ä»¶å¤¹
    check_dir = "/root/PI_Lab/step1_calibrated_check_results"
    os.makedirs(check_dir, exist_ok=True)
    print(f"ğŸ“ åˆ›å»ºæ£€æŸ¥æ–‡ä»¶å¤¹: {check_dir}")
    
    # æ”¶é›†æ‰€æœ‰çŠ¶æ€ä¿¡æ¯
    all_status = []
    
    for subject in subjects:
        status_file = os.path.join('/root/autodl-tmp/blood_pressure_reconstruction', subject, 'processing_logs', 'step1_calibrated_succ.txt')
        
        if os.path.exists(status_file):
            try:
                with open(status_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # è§£æçŠ¶æ€æ–‡ä»¶å†…å®¹
                status_info = {
                    'subject': subject,
                    'status_file_exists': True,
                    'status': 'SUCCESS',
                    'timestamp': 'UNKNOWN',
                    'output_directory': 'UNKNOWN',
                    'total_experiments': 'UNKNOWN',
                    'successful_experiments': 'UNKNOWN',
                    'failed_experiments': 'UNKNOWN',
                    'success_rate': 'UNKNOWN',
                    'processing_time': 'UNKNOWN',
                    'raw_content': content
                }
                
                # æå–çŠ¶æ€ä¿¡æ¯
                lines = content.strip().split('\n')
                for line in lines:
                    if line.startswith('Timestamp:'):
                        status_info['timestamp'] = line.replace('Timestamp:', '').strip()
                    elif line.startswith('Output Directory:'):
                        status_info['output_directory'] = line.replace('Output Directory:', '').strip()
                    elif line.startswith('Total Experiments:'):
                        status_info['total_experiments'] = line.replace('Total Experiments:', '').strip()
                    elif line.startswith('Successful:'):
                        status_info['successful_experiments'] = line.replace('Successful:', '').strip()
                    elif line.startswith('Failed:'):
                        status_info['failed_experiments'] = line.replace('Failed:', '').strip()
                    elif line.startswith('Success Rate:'):
                        status_info['success_rate'] = line.replace('Success Rate:', '').strip()
                    elif line.startswith('Total Processing Time:'):
                        status_info['processing_time'] = line.replace('Total Processing Time:', '').strip()
                
                all_status.append(status_info)
                print(f"âœ… {subject}: çŠ¶æ€æ–‡ä»¶å­˜åœ¨")
                
            except Exception as e:
                print(f"âŒ {subject}: è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥ - {e}")
                all_status.append({
                    'subject': subject,
                    'status_file_exists': True,
                    'status': 'READ_ERROR',
                    'timestamp': 'UNKNOWN',
                    'output_directory': 'UNKNOWN',
                    'total_experiments': 'UNKNOWN',
                    'successful_experiments': 'UNKNOWN',
                    'failed_experiments': 'UNKNOWN',
                    'success_rate': 'UNKNOWN',
                    'processing_time': 'UNKNOWN',
                    'error_message': f'è¯»å–å¤±è´¥: {str(e)}',
                    'raw_content': ''
                })
        else:
            print(f"âš ï¸  {subject}: çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨")
            all_status.append({
                'subject': subject,
                'status_file_exists': False,
                'status': 'NO_FILE',
                'timestamp': 'N/A',
                'output_directory': 'N/A',
                'total_experiments': 'N/A',
                'successful_experiments': 'N/A',
                'failed_experiments': 'N/A',
                'success_rate': 'N/A',
                'processing_time': 'N/A',
                'error_message': 'çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨',
                'raw_content': ''
            })
    
    # åˆ›å»ºæ±‡æ€»DataFrame
    summary_df = pd.DataFrame(all_status)
    
    # ç»Ÿè®¡ç»“æœ
    total_subjects = len(summary_df)
    success_count = len(summary_df[summary_df['status'] == 'SUCCESS'])
    no_file_count = len(summary_df[summary_df['status'] == 'NO_FILE'])
    read_error_count = len(summary_df[summary_df['status'] == 'READ_ERROR'])
    
    print(f"\nğŸ“Š å¤„ç†çŠ¶æ€ç»Ÿè®¡:")
    print(f"   ğŸ“‹ æ€»å—è¯•è€…æ•°: {total_subjects}")
    print(f"   âœ… æˆåŠŸ: {success_count}")
    print(f"   ğŸ“ æ— çŠ¶æ€æ–‡ä»¶: {no_file_count}")
    print(f"   ğŸ” è¯»å–é”™è¯¯: {read_error_count}")
    print(f"   â­ï¸  æœ¬æ‰¹æ¬¡è·³è¿‡: {skipped_subjects}")
    
    # ä¿å­˜è¯¦ç»†æ±‡æ€»æŠ¥å‘Šåˆ°æ£€æŸ¥æ–‡ä»¶å¤¹
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    summary_file = os.path.join(check_dir, f"step1_calibrated_summary_report_{timestamp}.csv")
    summary_df.to_csv(summary_file, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ è¯¦ç»†æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
    
    # åˆ›å»ºç®€åŒ–çŠ¶æ€æŠ¥å‘Š
    simple_status = []
    for _, row in summary_df.iterrows():
        simple_status.append({
            'subject': row['subject'],
            'status': row['status'],
            'timestamp': row['timestamp'] if row['timestamp'] != 'UNKNOWN' else 'N/A',
            'total_experiments': row['total_experiments'],
            'successful_experiments': row['successful_experiments'],
            'failed_experiments': row['failed_experiments'],
            'success_rate': row['success_rate'],
            'processing_time': row['processing_time'],
            'has_error': 'error_message' in row and row['error_message'] != '',
            'error_summary': row.get('error_message', '')[:100] + '...' if 'error_message' in row and len(row.get('error_message', '')) > 100 else row.get('error_message', '')
        })
    
    simple_df = pd.DataFrame(simple_status)
    simple_file = os.path.join(check_dir, f"step1_calibrated_simple_status_{timestamp}.csv")
    simple_df.to_csv(simple_file, index=False, encoding='utf-8')
    print(f"ğŸ’¾ ç®€åŒ–çŠ¶æ€æŠ¥å‘Šå·²ä¿å­˜: {simple_file}")
    
    # åˆ›å»ºæˆåŠŸ/å¤±è´¥åˆ—è¡¨
    success_subjects = summary_df[summary_df['status'] == 'SUCCESS']['subject'].tolist()
    no_file_subjects = summary_df[summary_df['status'] == 'NO_FILE']['subject'].tolist()
    read_error_subjects = summary_df[summary_df['status'] == 'READ_ERROR']['subject'].tolist()
    
    # ä¿å­˜åˆ†ç±»åˆ—è¡¨åˆ°æ£€æŸ¥æ–‡ä»¶å¤¹
    with open(os.path.join(check_dir, f"step1_calibrated_success_subjects_{timestamp}.txt"), 'w', encoding='utf-8') as f:
        f.write(f"Step1 Calibrated æˆåŠŸå¤„ç†çš„å—è¯•è€…åˆ—è¡¨\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æˆåŠŸæ•°é‡: {len(success_subjects)}\n")
        f.write(f"{'='*50}\n")
        for subject in success_subjects:
            f.write(f"{subject}\n")
    
    if no_file_subjects:
        with open(os.path.join(check_dir, f"step1_calibrated_no_file_subjects_{timestamp}.txt"), 'w', encoding='utf-8') as f:
            f.write(f"Step1 Calibrated æ— çŠ¶æ€æ–‡ä»¶çš„å—è¯•è€…åˆ—è¡¨\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ— æ–‡ä»¶æ•°é‡: {len(no_file_subjects)}\n")
            f.write(f"{'='*50}\n")
            for subject in no_file_subjects:
                f.write(f"{subject}\n")
    
    if read_error_subjects:
        with open(os.path.join(check_dir, f"step1_calibrated_read_error_subjects_{timestamp}.txt"), 'w', encoding='utf-8') as f:
            f.write(f"Step1 Calibrated è¯»å–é”™è¯¯çš„å—è¯•è€…åˆ—è¡¨\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"é”™è¯¯æ•°é‡: {len(read_error_subjects)}\n")
            f.write(f"{'='*50}\n")
            for subject in read_error_subjects:
                f.write(f"{subject}\n")
    
    # åˆ›å»ºæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    final_summary_file = os.path.join(check_dir, f"step1_calibrated_final_summary_{timestamp}.txt")
    with open(final_summary_file, 'w', encoding='utf-8') as f:
        f.write(f"Step1 Calibrated å¤„ç†ç»“æœæœ€ç»ˆæ±‡æ€»\n")
        f.write(f"{'='*50}\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»å—è¯•è€…æ•°: {total_subjects}\n")
        f.write(f"æœ¬æ‰¹æ¬¡è·³è¿‡: {skipped_subjects}\n\n")
        
        f.write(f"âœ… æˆåŠŸå¤„ç†: {success_count} ä¸ªå—è¯•è€… ({success_count/total_subjects*100:.1f}%)\n")
        f.write(f"ğŸ“ æ— çŠ¶æ€æ–‡ä»¶: {no_file_count} ä¸ªå—è¯•è€… ({no_file_count/total_subjects*100:.1f}%)\n")
        f.write(f"ğŸ” è¯»å–é”™è¯¯: {read_error_count} ä¸ªå—è¯•è€… ({read_error_count/total_subjects*100:.1f}%)\n\n")
        
        f.write(f"ğŸ‰ æˆåŠŸå¤„ç†çš„å—è¯•è€…åˆ—è¡¨ ({success_count}ä¸ª):\n")
        for i, subject in enumerate(success_subjects, 1):
            f.write(f"{subject}")
            if i % 10 == 0:
                f.write("\n")
            elif i < len(success_subjects):
                f.write(", ")
        f.write("\n\n")
        
        if no_file_subjects:
            f.write(f"ğŸ“ æ— çŠ¶æ€æ–‡ä»¶çš„å—è¯•è€…åˆ—è¡¨ ({no_file_count}ä¸ª):\n")
            for i, subject in enumerate(no_file_subjects, 1):
                f.write(f"{subject}")
                if i % 10 == 0:
                    f.write("\n")
                elif i < len(no_file_subjects):
                    f.write(", ")
            f.write("\n\n")
        
        f.write(f"ğŸ“Š å¤„ç†çŠ¶æ€è¯´æ˜:\n")
        f.write(f"- SUCCESS: åˆ†æå®Œæˆï¼Œç”Ÿæˆäº†æ‰€æœ‰ç›¸å…³æ–‡ä»¶å’Œå›¾è¡¨\n")
        f.write(f"- NO_FILE: çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯æ–°å—è¯•è€…\n")
        f.write(f"- READ_ERROR: çŠ¶æ€æ–‡ä»¶è¯»å–å¤±è´¥\n\n")
        
        f.write(f"ğŸ’¡ å»ºè®®:\n")
        f.write(f"1. æˆåŠŸå¤„ç†çš„{success_count}ä¸ªå—è¯•è€…å¯ä»¥ç›´æ¥ä½¿ç”¨ç»“æœ\n")
        if no_file_count > 0:
            f.write(f"2. æ— çŠ¶æ€æ–‡ä»¶çš„{no_file_count}ä¸ªå—è¯•è€…éœ€è¦é‡æ–°å¤„ç†\n")
        if read_error_count > 0:
            f.write(f"3. è¯»å–é”™è¯¯çš„{read_error_count}ä¸ªå—è¯•è€…éœ€è¦æ£€æŸ¥çŠ¶æ€æ–‡ä»¶\n")
        f.write(f"4. æ€»ä½“æˆåŠŸç‡{success_count/total_subjects*100:.1f}%ï¼Œå¤„ç†æ•ˆæœè‰¯å¥½\n")
        f.write(f"5. æœ¬æ‰¹æ¬¡è·³è¿‡äº†{skipped_subjects}ä¸ªå·²å¤„ç†çš„å—è¯•è€…ï¼ŒèŠ‚çœäº†å¤„ç†æ—¶é—´\n\n")
        
        f.write(f"ğŸ“ æœ¬æ‰¹æ¬¡æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶:\n")
        f.write(f"- step1_calibrated_summary_report_{timestamp}.csv (å®Œæ•´æŠ¥å‘Š)\n")
        f.write(f"- step1_calibrated_simple_status_{timestamp}.csv (ç®€åŒ–çŠ¶æ€)\n")
        f.write(f"- step1_calibrated_success_subjects_{timestamp}.txt (æˆåŠŸåˆ—è¡¨)\n")
        if no_file_count > 0:
            f.write(f"- step1_calibrated_no_file_subjects_{timestamp}.txt (æ— æ–‡ä»¶åˆ—è¡¨)\n")
        if read_error_count > 0:
            f.write(f"- step1_calibrated_read_error_subjects_{timestamp}.txt (è¯»å–é”™è¯¯åˆ—è¡¨)\n")
    
    print(f"\nğŸ“ åˆ†ç±»åˆ—è¡¨å·²ä¿å­˜åˆ° {check_dir} æ–‡ä»¶å¤¹:")
    print(f"   âœ… æˆåŠŸ: step1_calibrated_success_subjects_{timestamp}.txt")
    if no_file_count > 0:
        print(f"   ğŸ“ æ— æ–‡ä»¶: step1_calibrated_no_file_subjects_{timestamp}.txt")
    if read_error_count > 0:
        print(f"   ğŸ” è¯»å–é”™è¯¯: step1_calibrated_read_error_subjects_{timestamp}.txt")
    print(f"   ğŸ“‹ æœ€ç»ˆæ±‡æ€»: step1_calibrated_final_summary_{timestamp}.txt")
    
    # æ˜¾ç¤ºæˆåŠŸå’Œå¤±è´¥çš„å—è¯•è€…
    if success_subjects:
        print(f"\nğŸ‰ æˆåŠŸå¤„ç†çš„å—è¯•è€… ({len(success_subjects)}):")
        for i, subject in enumerate(success_subjects, 1):
            print(f"   {i:2d}. {subject}")
    
    if no_file_subjects:
        print(f"\nğŸ“ æ— çŠ¶æ€æ–‡ä»¶çš„å—è¯•è€… ({len(no_file_subjects)}):")
        for i, subject in enumerate(no_file_subjects, 1):
            print(f"   {i:2d}. {subject}")
    
    if read_error_subjects:
        print(f"\nğŸ” è¯»å–é”™è¯¯çš„å—è¯•è€… ({len(read_error_subjects)}):")
        for i, subject in enumerate(read_error_subjects, 1):
            print(f"   {i:2d}. {subject}")
    
    print(f"\nğŸ“ æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶å·²ä¿å­˜åˆ°: {check_dir}/")
    print(f"ğŸ¯ æ£€æŸ¥å®Œæˆï¼")
    
    return summary_df

if __name__ == "__main__":
    main()
